{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ELSER Model\n",
    "\n",
    "In the previous chapter you have seen how to expand an Elasticsearch index with a dense_vector field that is populated with embeddings generated by a Machine Learning model. The model was installed locally on your computer, and the embeddings were generated from the Python code and added to the documents before they were inserted into the index.\n",
    "\n",
    "In this chapter you are going to learn about another vector type, the sparse_vector, which is designed to store inferences from the Elastic Learned Sparse EncodeR model (ELSER). Embeddings returned by this model are a collection of tags (more appropriately called features), each with an assigned weight.\n",
    "\n",
    "In this chapter you will also use a different method for working with Machine Learning models, in which the Elasticsearch service itself runs the model and adds the resulting embeddings to the index through a pipeline.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sparse_vector Field\n",
    "\n",
    "Like the dense_vector field type you used in the previous chapter, the sparse_vector type can store inferences returned by Machine Learning models. While dense vectors hold a fixed-length array of numbers that describe the source text, a sparse vector stores a mapping of features to weights.\n",
    "\n",
    "Let's add a sparse_vector field to the index. This is a type that needs to be defined explicitly in the index mapping. Below you can see an updated version of the create_index() method with a new field called elser_embedding with this type.\n",
    "\n",
    "```python\n",
    "class Search:\n",
    "    # ...\n",
    "\n",
    "    def create_index(self):\n",
    "        self.es.indices.delete(index='my_documents', ignore_unavailable=True)\n",
    "        self.es.indices.create(index='my_documents', mappings={\n",
    "            'properties': {\n",
    "                'embedding': {\n",
    "                    'type': 'dense_vector',\n",
    "                },\n",
    "                'elser_embedding': {\n",
    "                    'type': 'sparse_vector',\n",
    "                },\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # ...\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the ELSER Model\n",
    "\n",
    "As mentioned above, in this example Elasticsearch will take ownership of the model and automatically execute it to generate embeddings, both when inserting documents and when searching.\n",
    "\n",
    "The Elasticsearch client exposes a set of API endpoints to manage Machine Learning models and their pipelines. The following deploy_elser() method in search.py follows a few steps to download and install the ELSER v2 model, and to create a pipeline that uses it to populate the elser_embedding field defined above.\n",
    "\n",
    "```python\n",
    "class Search:\n",
    "    # ...\n",
    "\n",
    "    def deploy_elser(self):\n",
    "        # download ELSER v2\n",
    "        self.es.ml.put_trained_model(model_id='.elser_model_2',\n",
    "                                     input={'field_names': ['text_field']})\n",
    "        \n",
    "        # wait until ready\n",
    "        while True:\n",
    "            status = self.es.ml.get_trained_models(model_id='.elser_model_2',\n",
    "                                                   include='definition_status')\n",
    "            if status['trained_model_configs'][0]['fully_defined']:\n",
    "                # model is ready\n",
    "                break\n",
    "            time.sleep(1)\n",
    "\n",
    "        # deploy the model\n",
    "        self.es.ml.start_trained_model_deployment(model_id='.elser_model_2')\n",
    "\n",
    "        # define a pipeline\n",
    "        self.es.ingest.put_pipeline(\n",
    "            id='elser-ingest-pipeline',\n",
    "            processors=[\n",
    "                {\n",
    "                    'inference': {\n",
    "                        'model_id': '.elser_model_2',\n",
    "                        'input_output': [\n",
    "                            {\n",
    "                                'input_field': 'summary',\n",
    "                                'output_field': 'elser_embedding',\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "```\n",
    "\n",
    "Configuring ELSER for us requires a several steps. First, the ml.put_trained_model() method of the Elasticsearch is used to download ELSER. The model_id argument identifies the model and version to download (ELSER v2 is available for Elasticsearch 8.11 and up). The input field is the configuration required by this model.\n",
    "\n",
    "Once the model is downloaded it needs to be deployed. For this, the ml.start_trained_model_deployment() method is used, just with the identifier of the model to deploy. Note that this is an asynchronous operation, so the model is going to be available for use after a short amount of time.\n",
    "\n",
    "The final step to configure the use of ELSER is to define a pipeline for it. A pipeline is used to tell Elasticsearch how the model has to be used. A pipeline is given an identifier and one or more processing tasks to perform. The pipeline created above is called elser-ingest-pipeline and has a single inference task, which means that each time a document is added, the model is going to run with on the input_field, and the output will be added to the document on the output_field. For this example the summary field is used to generate the embeddings, as with the dense vector embeddings in the previous chapter. The resulting embeddings are going to be written to the elser_embedding sparse vector field created in the previous section.\n",
    "\n",
    "To make it easy to invoke this method, add a deploy-elser command to the Flask application in app.py:\n",
    "\n",
    "```python\n",
    "@app.cli.command()\n",
    "def deploy_elser():\n",
    "    \"\"\"Deploy the ELSER v2 model to Elasticsearch.\"\"\"\n",
    "    try:\n",
    "        es.deploy_elser()\n",
    "    except Exception as exc:\n",
    "        print(f'Error: {exc}')\n",
    "    else:\n",
    "        print(f'ELSER model deployed.')\n",
    "```\n",
    "\n",
    "\n",
    "You can now deploy ELSER on your Elasticsearch service with the following command:\n",
    "\n",
    "\n",
    "The last configuration task involves linking the index with the pipeline, so that the model is automatically executed when documents are inserted on this index. This is done on the index configuration with a settings option. Here is one more update to the create_index() method to create this link:\n",
    "\n",
    "\n",
    "```python\n",
    "class Search:\n",
    "    # ...\n",
    "\n",
    "    def create_index(self):\n",
    "        self.es.indices.delete(index='my_documents', ignore_unavailable=True)\n",
    "        self.es.indices.create(\n",
    "            index='my_documents',\n",
    "            mappings={\n",
    "                'properties': {\n",
    "                    'embedding': {\n",
    "                        'type': 'dense_vector',\n",
    "                    },\n",
    "                    'elser_embedding': {\n",
    "                        'type': 'sparse_vector',\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            settings={\n",
    "                'index': {\n",
    "                    'default_pipeline': 'elser-ingest-pipeline'\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "```\n",
    "\n",
    "With this change, you can now regenerate the index with full support for ELSER inferences:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Queries\n",
    "\n",
    "With the index now equipped with ELSER embeddings, the handle_search() function in app.py can be changed to search these embeddings. For now, you'll see how to search only through ELSER, later the previous search methods will be incorporated back to create a combined solution.\n",
    "\n",
    "To use ELSER inferences when searching, the text_expansion query type is used. Below you can see an updated handle_search() function with this query:\n",
    "\n",
    "\n",
    "```python\n",
    "@app.post('/')\n",
    "def handle_search():\n",
    "    query = request.form.get('query', '')\n",
    "    filters, parsed_query = extract_filters(query)\n",
    "    from_ = request.form.get('from_', type=int, default=0)\n",
    "\n",
    "    results = es.search(\n",
    "        query={\n",
    "            'text_expansion': {\n",
    "                'elser_embedding': {\n",
    "                    'model_id': '.elser_model_2',\n",
    "                    'model_text': parsed_query,\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        size=5,\n",
    "        from_=from_,\n",
    "    )\n",
    "    return render_template('index.html', results=results['hits']['hits'],\n",
    "                           query=query, from_=from_,\n",
    "                           total=results['hits']['total']['value'])\n",
    "\n",
    "```\n",
    "\n",
    "The text_expansion query receives a key with the name of the field to be searched. Under this key, model_id configures which model to use in the search, and model_text defines what to search for. Note how in this case there is no need to generate an embedding for the search text, as Elasticsearch manages the model and can take care of that.\n",
    "\n",
    "In the above version of handle_search() the filters have been left unused, and the aggregations have been omitted. These can be added back in the same way they were incorporated into the full-text search solution. Below is an updated handle_search() function that moves the text_expansion query inside a bool.must section, with filters included in bool.filter and aggregations added as before.\n",
    "\n",
    "```python\n",
    "@app.post('/')\n",
    "def handle_search():\n",
    "    query = request.form.get('query', '')\n",
    "    filters, parsed_query = extract_filters(query)\n",
    "    from_ = request.form.get('from_', type=int, default=0)\n",
    "\n",
    "    results = es.search(\n",
    "        query={\n",
    "            'bool': {\n",
    "                'must': [\n",
    "                    {\n",
    "                        'text_expansion': {\n",
    "                            'elser_embedding': {\n",
    "                                'model_id': '.elser_model_2',\n",
    "                                'model_text': parsed_query,\n",
    "                            }\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "                **filters,\n",
    "            }\n",
    "        },\n",
    "        aggs={\n",
    "            'category-agg': {\n",
    "                'terms': {\n",
    "                    'field': 'category.keyword',\n",
    "                }\n",
    "            },\n",
    "            'year-agg': {\n",
    "                'date_histogram': {\n",
    "                    'field': 'updated_at',\n",
    "                    'calendar_interval': 'year',\n",
    "                    'format': 'yyyy',\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        size=5,\n",
    "        from_=from_,\n",
    "    )\n",
    "    aggs = {\n",
    "        'Category': {\n",
    "            bucket['key']: bucket['doc_count']\n",
    "            for bucket in results['aggregations']['category-agg']['buckets']\n",
    "        },\n",
    "        'Year': {\n",
    "            bucket['key_as_string']: bucket['doc_count']\n",
    "            for bucket in results['aggregations']['year-agg']['buckets']\n",
    "            if bucket['doc_count'] > 0\n",
    "        },\n",
    "    }\n",
    "    return render_template('index.html', results=results['hits']['hits'],\n",
    "                           query=query, from_=from_,\n",
    "                           total=results['hits']['total']['value'], aggs=aggs)\n",
    "\n",
    "```\n",
    "\n",
    "Spend some time experimenting with different searches. You will notice that as with dense vector embeddings, searches driven by the ELSER model work better than full-text search when the exact words do not appear in the indexed documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q) I want to know the difference between vector search and semantic search. In my opinion, both methods are used to search for documents based on similarity.\n",
    "\n",
    "Answer: \n",
    "- In practice, both methods aim for similarity-based search, but they approach it differently. Vector search is more about overall similarity in a continuous space, while semantic search (especially with ELSER) is about matching on specific, contextually important concepts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search: Combined Full-Text and ELSER Results\n",
    "\n",
    "As with vector search in the previous section, in this section you will learn how to combine the best search results from full-text and semantic queries using the Reciprocal Rank Fusion algorithm.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Sub-Searches\n",
    "\n",
    "The solution to implementing a hybrid full-text and dense vector search was to send a search request that included the query, knn arguments to request the two searches, and the rrf argument to combine them into a single results list.\n",
    "\n",
    "The complication that is presented when trying to do the same to combine full-text and sparse vector search requests is that both use the query argument. To be able to provide the two queries that need to be combined with the RRF algorithm, it is necessary to include two query arguments, and the solution to do this is to do it with Sub-Searches.\n",
    "\n",
    "Sub-searches is a feature that is currently in technical preview. For this reason the Python Elasticsearch client does not natively support it. To work around this limitation, the search() method of the Search class can be changed to send the search request using the body argument. Below you can see a new, yet similar implementation that uses the body argument of the client to send a search request:\n",
    "\n",
    "```python\n",
    "class Search:\n",
    "    # ...\n",
    "\n",
    "    def search(self, **query_args):\n",
    "        # sub_searches is not currently supported in the client, so we send\n",
    "        # search requests using the body argument\n",
    "        if 'from_' in query_args:\n",
    "            query_args['from'] = query_args['from_']\n",
    "            del query_args['from_']\n",
    "        return self.es.search(\n",
    "            index='my_documents',\n",
    "            body=json.dumps(query_args),\n",
    "        )\n",
    "\n",
    "```\n",
    "\n",
    "This implementation does not require any changes to the application, as it is functionally equivalent. The only difference is that the search() method validates all arguments before sending the request, with body being the only exception. The server always validates requests regardless of how the client sends them.\n",
    "\n",
    "With this version, the sub_searches argument can be used in Search.search() to send multiple search queries as follows:\n",
    "\n",
    "\n",
    "```python\n",
    "results = es.search(\n",
    "    sub_searches=[\n",
    "        {\n",
    "            'query': { ... },  # full-text search\n",
    "        },\n",
    "        {\n",
    "            'query': { ... },  # semantic search\n",
    "        },\n",
    "    ],\n",
    "    rank={\n",
    "        'rrf': {},  # combine sub-search results\n",
    "    },\n",
    "    aggs={ ... },\n",
    "    size=5,\n",
    "    from_=from_,\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search Implementation\n",
    "To complete this section, let's bring back the full-text logic and combine it with the semantic search query presented earlier in this chapter.\n",
    "\n",
    "Below you can see the updated handle_search() endpoint:\n",
    "\n",
    "```python\n",
    "@app.post('/')\n",
    "def handle_search():\n",
    "    query = request.form.get('query', '')\n",
    "    filters, parsed_query = extract_filters(query)\n",
    "    from_ = request.form.get('from_', type=int, default=0)\n",
    "\n",
    "    if parsed_query:\n",
    "        search_query = {\n",
    "            'sub_searches': [\n",
    "                {\n",
    "                    'query': {\n",
    "                        'bool': {\n",
    "                            'must': {\n",
    "                                'multi_match': {\n",
    "                                    'query': parsed_query,\n",
    "                                    'fields': ['name', 'summary', 'content'],\n",
    "                                }\n",
    "                            },\n",
    "                            **filters\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'query': {\n",
    "                        'bool': {\n",
    "                            'must': [\n",
    "                                {\n",
    "                                    'text_expansion': {\n",
    "                                        'elser_embedding': {\n",
    "                                            'model_id': '.elser_model_2',\n",
    "                                            'model_text': parsed_query,\n",
    "                                        }\n",
    "                                    },\n",
    "                                }\n",
    "                            ],\n",
    "                            **filters,\n",
    "                        }\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "            'rank': {\n",
    "                'rrf': {}\n",
    "            },\n",
    "        }\n",
    "    else:\n",
    "        search_query = {\n",
    "            'query': {\n",
    "                'bool': {\n",
    "                    'must': {\n",
    "                        'match_all': {}\n",
    "                    },\n",
    "                    **filters\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    results = es.search(\n",
    "        **search_query,\n",
    "        aggs={\n",
    "            'category-agg': {\n",
    "                'terms': {\n",
    "                    'field': 'category.keyword',\n",
    "                }\n",
    "            },\n",
    "            'year-agg': {\n",
    "                'date_histogram': {\n",
    "                    'field': 'updated_at',\n",
    "                    'calendar_interval': 'year',\n",
    "                    'format': 'yyyy',\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        size=5,\n",
    "        from_=from_,\n",
    "    )\n",
    "    aggs = {\n",
    "        'Category': {\n",
    "            bucket['key']: bucket['doc_count']\n",
    "            for bucket in results['aggregations']['category-agg']['buckets']\n",
    "        },\n",
    "        'Year': {\n",
    "            bucket['key_as_string']: bucket['doc_count']\n",
    "            for bucket in results['aggregations']['year-agg']['buckets']\n",
    "            if bucket['doc_count'] > 0\n",
    "        },\n",
    "    }\n",
    "    return render_template('index.html', results=results['hits']['hits'],\n",
    "                           query=query, from_=from_,\n",
    "                           total=results['hits']['total']['value'], aggs=aggs)\n",
    "\n",
    "```\n",
    "\n",
    "As you recall, the extract_filters() function looked for category filters entered by the user on the search prompt, and returned the left over portion as parsed_query. If parsed_query is empty, it means that the user only enter a category filter, and in that case the query should be a simple match_all with the selected category as a filter. This is implemented in the else portion of the big conditional.\n",
    "\n",
    "When there is a search query, the sub_searches option is used as shown in the previous section to include the multi_match and text_expansion queries, with the rank option requesting that the results from the two sub-searches are combined into a single list of ranked results. To complete the query, the size and from_ argument are provided to maintain the support for pagination.\n",
    "\n",
    "Click here to review this version of the application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
