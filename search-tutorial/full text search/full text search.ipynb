{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Text Search \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the Elasticsearch Python Client\n",
    "In this section you will install the Elasticsearch client library for Python and use it to connect to the Elasticsearch service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "The Elasticsearch client library is a Python package that is installed with pip. Make sure the virtual environment you created earlier is activated, and then run the following command to install the client:\n",
    "\n",
    "\n",
    "```bash \n",
    "pip install elasticsearch\n",
    "```\n",
    "\n",
    "To avoid any potential incompatibilities, make sure the version of the Elasticsearch client library you install matches the version of the Elasticsearch stack that you are using.\n",
    "\n",
    "It is always recommended to keep a requirements.txt file updated with all your dependencies, so this is a good time to update this file to include the newly installed package. Run the following command from your terminal:\n",
    "\n",
    "\n",
    "```bash \n",
    "pip freeze > requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (8.14.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.13 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from elasticsearch) (8.13.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2.2.2)\n",
      "Requirement already satisfied: certifi in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2024.6.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Elasticsearch\n",
    "To create a connection to your Elasticsearch service, an Elasticsearch object must be created with the appropriate connection options.\n",
    "\n",
    "Create a new search.py file in your code editor, located in the search-tutorial directory. The search.py file is going to be where all the search functions will be defined. The idea of having a separate file for the search functionality is that this will make it easy for you to extract this file and add it into your own projects later on.\n",
    "\n",
    "Enter the following code in search.py to add a Search class:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import os\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class Search:\n",
    "    def __init__(self):\n",
    "        self.es = Elasticsearch('http://localhost:9200')\n",
    "        client_info = self.es.info()\n",
    "        print('Connected to Elasticsearch!')\n",
    "        pprint(client_info.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot to unpack here. The load_dotenv() function that is called right after the imports comes from the python-dotenv package. This package knows how to work with .env files, which are used to store configuration variables such as passwords and keys. The load_dotenv() function reads the variables that are stored in the .env file and imports them into the Python process as environment variables.\n",
    "\n",
    "The Search class has a constructor that creates an instance of the Elasticsearch client class. This is where all the client logic to communicate with the Elasticsearch service lives. Note that this line is currently incomplete, as connection options appropriate to your service need to be included. You will learn what options apply in your case below. Once created, the Elasticsearch object is then stored in an instance variable named self.es.\n",
    "\n",
    "To ensure that the client object can communicate with your Elastic Cloud deployment, the info() method is invoked. This method makes a call to the service requesting basic information. If this call succeeds, then you can assume that you have a valid connection to the service.\n",
    "\n",
    "The method then prints a status message indicating that the connection has been established, and then uses the pprint function from Python to display the information that the service returned in an easy to read format.\n",
    "\n",
    "NOTE:: You may have noticed that the json package from the Python standard library is imported in this file, but not used. Do not remove this import, as this package will be used later.\n",
    "\n",
    "\n",
    "To complete the constructor of the Search class, the Elasticsearch object needs to be given appropriate connection options. The following sub-sections will tell you what options you need for the Elastic Cloud and Docker methods of installation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Connection\n",
    "At this point you are ready to make a connection to your Elasticsearch service. To do this, make sure that your Python virtual environment is activated, and then type python to start a Python interactive session. You should see the familiar >>> prompt, in which you can enter Python statements.\n",
    "\n",
    "Import the Search class as follows:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch!\n",
      "{'cluster_name': 'docker-cluster',\n",
      " 'cluster_uuid': 'KiZ5r8-vR7GZf9YFdRuehg',\n",
      " 'name': '39f73e6b6ad4',\n",
      " 'tagline': 'You Know, for Search',\n",
      " 'version': {'build_date': '2024-03-22T03:35:46.757803203Z',\n",
      "             'build_flavor': 'default',\n",
      "             'build_hash': '09df99393193b2c53d92899662a8b8b3c55b45cd',\n",
      "             'build_snapshot': False,\n",
      "             'build_type': 'docker',\n",
      "             'lucene_version': '9.10.0',\n",
      "             'minimum_index_compatibility_version': '7.0.0',\n",
      "             'minimum_wire_compatibility_version': '7.17.0',\n",
      "             'number': '8.13.0'}}\n"
     ]
    }
   ],
   "source": [
    "es = Search()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Elasticsearch Index\n",
    "\n",
    "Two very important concepts in Elasticsearch are documents and indexes.\n",
    "\n",
    "A document is collection of fields with their associated values. To work with Elasticsearch you have to organize your data into documents, and then add all your documents to an index. You can think of an index as a collection of documents that is stored in a highly optimized format designed to perform efficient searches.\n",
    "\n",
    "If you have worked with other databases, you may know that many databases require a schema definition, which is essentially a description of all the fields that you want to store and their types. An Elasticsearch index can be configured with a schema if desired, but it can also automatically derive the schema from the data itself. In this section you are going to let Elasticsearch figure out the schema on its own, which works quite well for simple data types such as text, numbers and dates. Later, after you are introduced to more complex data types, you will learn how to provide explicit schema definitions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Index\n",
    "This is how you create an Elasticsearch index using the Python client library:\n",
    "\n",
    "\n",
    "```python\n",
    "self.es.indices.create(index='my_documents')\n",
    "```\n",
    "\n",
    "In this example, self.es is an instance of the Elasticsearch class, which in this tutorial is stored in the Search class in search.py. An Elasticsearch deployment can be used to store multiple indexes, each identified by a name such as my_documents in the example above.\n",
    "\n",
    "Indexes can also be deleted:\n",
    "\n",
    "```python\n",
    "self.es.indices.delete(index='my_documents')\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "If you attempt to create an index with a name that is already assigned to an existing index, you will get an error. Sometimes it is useful to create an index automatically deleting a previous instance of the index if it exists. This is especially useful while developing an application, because you will likely need to regenerate an index several times.\n",
    "\n",
    "Let's add a create_index() helper method in search.py. Open this file in your code editor, and add the following code at the bottom, leaving the existing contents as they are:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Search:\n",
    "    def __init__(self):\n",
    "        self.es = Elasticsearch('http://localhost:9200')\n",
    "        client_info = self.es.info()\n",
    "        print('Connected to Elasticsearch!')\n",
    "        pprint(client_info.body)\n",
    "\n",
    "    def create_index(self):\n",
    "        self.es.indices.delete(index='my_documents', ignore_unavailable=True)\n",
    "        self.es.indices.create(index='my_documents')\n",
    "\n",
    "    def insert_document(self, document):\n",
    "        return self.es.index(index='my_documents', body=document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_index() method first deletes an index with the name my_documents. The ignore_unavailable=True option prevents this call from failing when the index name isn't found. The following line in the method creates a brand new index with that same name.\n",
    "\n",
    "The example application featured in this tutorial needs a single Elasticsearch index, and for that reason it hardcodes the index name as my_documents. For more complex applications that use multiple indexes, you may consider accepting the index name as an argument.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Documents to the Index\n",
    "In the Elasticsearch client library for Python, a document is represented as a dictionary of key/value fields. Fields that have a string value are automatically indexed for full-text and keyword search, but in addition to strings you can use other field types such as numbers, dates and booleans, which are also indexed for efficient operations such as filtering. You can also build complex data structures in which a field is set to a list or a dictionary with sub-items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch!\n",
      "{'cluster_name': 'docker-cluster',\n",
      " 'cluster_uuid': 'KiZ5r8-vR7GZf9YFdRuehg',\n",
      " 'name': '39f73e6b6ad4',\n",
      " 'tagline': 'You Know, for Search',\n",
      " 'version': {'build_date': '2024-03-22T03:35:46.757803203Z',\n",
      "             'build_flavor': 'default',\n",
      "             'build_hash': '09df99393193b2c53d92899662a8b8b3c55b45cd',\n",
      "             'build_snapshot': False,\n",
      "             'build_type': 'docker',\n",
      "             'lucene_version': '9.10.0',\n",
      "             'minimum_index_compatibility_version': '7.0.0',\n",
      "             'minimum_wire_compatibility_version': '7.17.0',\n",
      "             'number': '8.13.0'}}\n",
      "dBOhn5ABMnum9rmL7gdm\n"
     ]
    }
   ],
   "source": [
    "es = Search()\n",
    "\n",
    "document = {\n",
    "    'title': 'Work From Home Policy',\n",
    "    'contents': 'The purpose of this full-time work-from-home policy is...',\n",
    "    'created_on': '2023-11-02',\n",
    "}\n",
    "response = es.insert_document(document)\n",
    "print(response['_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting Documents from a JSON File\n",
    "When setting up a new Elasticsearch index, you are likely going to need to import a large number of documents. For this tutorial, the starter project includes a data.json file with some data in JSON format. In this section you will learn how to import all the documents contained in this file into the index.\n",
    "\n",
    "The structure of the documents that are included in the data.json is as follows:\n",
    "- name: the document title\n",
    "- url: a URL to the document hosted on an external site\n",
    "- summary: a short summary of the contents of the document\n",
    "- content: the body of the document\n",
    "- created_on: creation date\n",
    "- updated_at: update date (could be missing if the document was never updated)\n",
    "- category: the document's category, which can be github, sharepoint or teams\n",
    "- rolePermissions: a list of role permissions\n",
    "\n",
    "At this point you are encouraged to open data.json in your editor to familiarize yourself with the data that you are going to work with.\n",
    "\n",
    "In essence, importing a large number of documents is no different than importing one document inside a for-loop. To import the entire contents of the data.json file, you could do something like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "  Downloading Faker-26.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from faker) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "Downloading Faker-26.0.0-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faker\n",
      "Successfully installed faker-26.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000개의 레코드가 data.json 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "def generate_data(num_records=1000):\n",
    "    data = []\n",
    "    categories = ['github', 'sharepoint', 'teams']\n",
    "    roles = ['admin', 'user', 'guest', 'manager']\n",
    "\n",
    "    for _ in range(num_records):\n",
    "        created_on = fake.date_time_between(start_date='-2y', end_date='now')\n",
    "        \n",
    "        record = {\n",
    "            'name': fake.sentence(nb_words=4),\n",
    "            'url': fake.url(),\n",
    "            'summary': fake.paragraph(nb_sentences=2),\n",
    "            'content': fake.text(max_nb_chars=1000),\n",
    "            'created_on': created_on.isoformat(),\n",
    "            'category': random.choice(categories),\n",
    "            'rolePermissions': random.sample(roles, k=random.randint(1, len(roles)))\n",
    "        }\n",
    "\n",
    "        # 50% 확률로 updated_at 필드 추가\n",
    "        if random.random() > 0.5:\n",
    "            updated_at = fake.date_time_between(start_date=created_on, end_date='now')\n",
    "            record['updated_at'] = updated_at.isoformat()\n",
    "\n",
    "        data.append(record)\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename='data.json'):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "data = generate_data()\n",
    "save_to_json(data)\n",
    "print(f\"1000개의 레코드가 data.json 파일에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch!\n",
      "{'cluster_name': 'docker-cluster',\n",
      " 'cluster_uuid': 'KiZ5r8-vR7GZf9YFdRuehg',\n",
      " 'name': '39f73e6b6ad4',\n",
      " 'tagline': 'You Know, for Search',\n",
      " 'version': {'build_date': '2024-03-22T03:35:46.757803203Z',\n",
      "             'build_flavor': 'default',\n",
      "             'build_hash': '09df99393193b2c53d92899662a8b8b3c55b45cd',\n",
      "             'build_snapshot': False,\n",
      "             'build_type': 'docker',\n",
      "             'lucene_version': '9.10.0',\n",
      "             'minimum_index_compatibility_version': '7.0.0',\n",
      "             'minimum_wire_compatibility_version': '7.17.0',\n",
      "             'number': '8.13.0'}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "es = Search()\n",
    "\n",
    "with open('data.json', 'rt') as f:\n",
    "    documents = json.loads(f.read())\n",
    "\n",
    "for document in documents:\n",
    "    es.insert_document(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UI: \n",
    "- http://localhost:9200/_cat/indices?v\n",
    "- http://localhost:9200/my_documents/_search\n",
    "- or Kibana "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch!\n",
      "{'cluster_name': 'docker-cluster',\n",
      " 'cluster_uuid': 'KiZ5r8-vR7GZf9YFdRuehg',\n",
      " 'name': '39f73e6b6ad4',\n",
      " 'tagline': 'You Know, for Search',\n",
      " 'version': {'build_date': '2024-03-22T03:35:46.757803203Z',\n",
      "             'build_flavor': 'default',\n",
      "             'build_hash': '09df99393193b2c53d92899662a8b8b3c55b45cd',\n",
      "             'build_snapshot': False,\n",
      "             'build_type': 'docker',\n",
      "             'lucene_version': '9.10.0',\n",
      "             'minimum_index_compatibility_version': '7.0.0',\n",
      "             'minimum_wire_compatibility_version': '7.17.0',\n",
      "             'number': '8.13.0'}}\n",
      "총 1001개의 문서가 있습니다.\n",
      "{'title': 'Work From Home Policy', 'contents': 'The purpose of this full-time work-from-home policy is...', 'created_on': '2023-11-02'}\n",
      "{'name': 'Sense day.', 'url': 'http://www.day.com/', 'summary': 'Guess improve just born.', 'content': 'Natural second style service she read. Child plan response head.\\nRoad space military weight responsibility true figure develop. Manager parent reveal scene every let these. Family pass energy create already clear.\\nMeasure blood report break among. Kitchen front story own measure. Age head million peace.\\nFour social box family growth article. If if list majority understand that. More talk cause maintain return pay.\\nOk season participant. Loss feeling away forward use attention detail manage. Us why almost military.\\nChallenge need stock look technology. Partner admit us change.\\nChair which reality event miss. Explain whole challenge everyone very father quickly. Tell late finish.\\nCup several month around. Hospital modern serious care thought it better. Green attention arm table sound.\\nMan right yeah enter weight. Unit ten campaign collection key place.\\nPiece between not method describe Mr. Attack order home reach art. Leg pull recently surface wait let likely enough.', 'created_on': '2023-11-22T16:49:22.438959', 'category': 'github', 'rolePermissions': ['guest']}\n",
      "{'name': 'Real continue old region.', 'url': 'https://hendricks-lewis.com/', 'summary': 'East stuff character bed.', 'content': 'Program team floor pull. Require late song institution avoid.\\nUnit value certainly section where. Save their movie treat politics condition claim. Owner better make rule.\\nSuffer kitchen major billion show wife. Successful name poor move full.\\nPage former establish sport. Want these assume no full man.\\nRemember season provide possible suggest service. Court prevent center event third research reflect. Thing information simply moment indicate language.\\nWish social word travel. Right relate although fact power argue.\\nThan want pay company feeling finish occur woman. Today understand college plan Democrat gun a. Institution source religious economy determine weight.\\nSkill coach fish hot bag green stop. Couple direction best final.\\nRepresent audience during rather role crime difference. Wall wide energy beautiful.\\nInclude in common professor method car serve. Building prove listen shoulder concern.\\nDinner although reduce party write adult. Us decide maybe house nothing.', 'created_on': '2023-09-30T08:28:25.225997', 'category': 'teams', 'rolePermissions': ['admin', 'guest']}\n",
      "{'name': 'Maybe play affect.', 'url': 'http://bauer.com/', 'summary': 'Technology change shake card arm.', 'content': 'Bag white level. Enter our wife president along. Near cause set special she style kind activity. Police way many relate consumer truth.\\nIt decade thousand mouth risk area woman. Character action commercial reality former question start animal.\\nDemocratic strong other.\\nSense else term order. Second table in hope director under. Move Mrs player fund more.\\nSave trip guess less happen indicate. Truth do effort less establish.\\nReach type choice. Medical address remember federal drug performance. Raise there air likely wind gas bad.\\nMission doctor claim evidence. Opportunity surface common method likely easy.\\nRather material home. Something however window industry fish service mission.\\nOffice fish themselves if. Century start half keep mention. Language may from cultural. Especially figure relate brother program audience fire quite.\\nScore before seek democratic fill.\\nCall official nature too forward. None spend any find major likely billion. Place ask car suggest represent wonder attorney.', 'created_on': '2024-05-20T10:35:47.679639', 'category': 'teams', 'rolePermissions': ['guest', 'admin', 'manager'], 'updated_at': '2024-07-05T18:44:08.349676'}\n",
      "{'name': 'Election sometimes.', 'url': 'http://goodwin.net/', 'summary': 'Eight generation enter three. Practice beautiful run run.', 'content': 'Indeed any leg land chair. Issue onto employee left. Difficult lay kid run citizen sea drive.\\nAlmost material four drop end money. Structure reduce above answer throughout.\\nSupport perform under air look. To low theory. Respond minute good though light project.\\nBeautiful store shoulder manage me. Recognize good none play strategy Democrat me.\\nCourt too crime almost than forget interest.\\nSouthern from game production mention effect. Record purpose black end least pay right while.\\nPut everyone use party herself anyone poor. Back pay none or.\\nLocal camera both kind central apply vote after. Above less employee risk.\\nManager find certain performance song part have. Probably rest near hope decide tell.\\nThink hard human shake hard everyone.\\nSea read dream take writer. Protect personal price toward prepare response meeting. Attorney company increase how sell.\\nIn energy use old much television return. Local student adult seem then part. Yourself past author. And several throw option.', 'created_on': '2023-06-22T21:24:34.201218', 'category': 'sharepoint', 'rolePermissions': ['manager']}\n",
      "{'name': 'Draw give mind discover purpose.', 'url': 'http://www.perkins-ray.com/', 'summary': 'Still seat hit house animal describe street. Appear his treatment eye according traditional.', 'content': 'Help answer least. Impact of go him religious him mind. Low since general alone.\\nBody father air total. Leader evening material alone.\\nThird country large sell two late notice. Eat main mother through young her police. Require piece more guess eye degree.\\nAlso wonder thought late thousand. As forget game throw any.\\nRather there reveal economy answer society.\\nReality environmental industry part. Fact station bring music board. School indicate allow car meet another executive.\\nRock low environmental although without yes. Tell life anyone whether. Forward worker special probably thing including suggest.\\nAny care safe trouble consumer window shoulder. Bring major work nation home turn rich.\\nLine half choose three seek yet name you. Should without race situation young land win.\\nWhose system although plant tonight. Home various because subject board able. Ground phone apply hit.\\nFormer then yet cost indeed something size. Head position hour happen. Ten actually hair official go debate dog.', 'created_on': '2024-02-09T16:28:48.077094', 'category': 'teams', 'rolePermissions': ['admin'], 'updated_at': '2024-05-24T17:02:40.757099'}\n",
      "{'name': 'Place nature market.', 'url': 'http://newman.org/', 'summary': 'Opportunity carry culture reason up all. Cut unit feel.', 'content': 'Hear customer action hot institution life central. Culture network allow scene message each. Discussion nation house tend you bit scientist.\\nDark system contain again wall heart senior wind. Even side by pressure style option.\\nThan rate Democrat life. High hit happy area. Idea talk region science thought.\\nLanguage outside professional best sign. Population poor natural goal arm region.\\nDifficult training tax professional price join result. Person during table method. Foot great long pick political.\\nDetail organization continue list difference land simple operation. Down system save hour.\\nType pull million here specific already allow. Century student color give. Matter treat bit save senior real eat.\\nAccept travel present. I right rule support. Several set here.\\nSort time under mean dinner economic risk read. Art class past hand.\\nProgram allow dark pay hundred because green. Dinner measure sort play couple.\\nGo around modern himself happen significant. Pay while specific scene.', 'created_on': '2023-04-11T11:29:45.577462', 'category': 'sharepoint', 'rolePermissions': ['admin', 'user'], 'updated_at': '2024-04-27T13:09:30.222358'}\n",
      "{'name': 'Learn reveal usually administration.', 'url': 'http://www.brown-white.com/', 'summary': 'Smile send common smile Republican.', 'content': 'Action response think member most adult. Identify grow thought agent begin in. Deep night support national most positive.\\nReduce child condition face people oil question school. Tree art data shake. Step read paper white development product training.\\nTask billion fear consider. Audience scene wear task fear total process old.\\nProfessional before into list reflect perhaps someone. Marriage vote feeling minute available. Enjoy almost lead on answer.\\nPerform strong network already value finally.\\nSecond benefit bed society. Art value staff now popular fast.\\nRoom guess usually travel. Wonder job cause keep. Gas how market sure save pattern save.\\nLast specific sport themselves believe foreign.\\nEnjoy include mouth use cup.\\nSummer stop station outside. Herself fund list save local. Play director late miss exist. Knowledge threat feel stock go wide sure.\\nSide catch next body win film. Color seem age low. Wish continue cut.', 'created_on': '2024-06-24T02:58:11.266764', 'category': 'github', 'rolePermissions': ['guest', 'admin', 'manager'], 'updated_at': '2024-06-28T05:32:50.273921'}\n",
      "{'name': 'Eat live red ago American.', 'url': 'http://schmidt-carter.com/', 'summary': 'Range fill sense later. Dream action argue letter.', 'content': 'Dark think race around behind. Attorney show almost worker tend.\\nRun fly notice cup test oil administration. Act almost early agency low machine any run.\\nPositive teach ball change. Side must discuss door floor.\\nThreat simply development nothing we girl suffer agree. Perform member trade trade age side. Use hold knowledge probably bank later author.\\nGrowth relationship seven recent. Artist half address name in truth enough.\\nForget painting day maintain. Cold election within administration protect sit full.\\nBeyond daughter local before every. Radio popular upon organization believe approach market give. Present else dog heavy else.\\nThemselves early increase physical girl prepare. Forward camera leader from.\\nTechnology window kid senior thousand threat. Decide which career degree after mean. Under partner risk positive.\\nGeneral class democratic law responsibility open. Long to book measure white. Process evening determine night trip.', 'created_on': '2023-08-15T17:25:43.143523', 'category': 'teams', 'rolePermissions': ['user'], 'updated_at': '2024-01-13T14:40:38.774395'}\n",
      "{'name': 'Growth lose store.', 'url': 'https://vasquez.com/', 'summary': 'Class glass beautiful. Five under able certain only television watch fill.', 'content': 'Network house research agree air. Color from dinner hour. Artist might defense although sound soldier.\\nHave improve girl account conference physical. Seat husband investment.\\nEspecially personal situation talk capital boy. Player herself hold too message.\\nDrop any particularly. Or face high foot technology trial. Job month want nothing debate.\\nTest defense instead nation. Herself same general around. Kid fill think realize sign.\\nAccording gas free soldier bill task let hand. Pay station significant not sea concern so bed. Chair either job space smile anyone society.\\nMy old body house value beyond. Clear growth above lose tell cell man growth.\\nTeach anything health laugh necessary. Chair claim by language. Camera summer ten fire recognize friend.\\nRemember center today international as region rather. Politics information general allow whom sell who.\\nWish let add herself. Structure send push eight social play. Air exist major board local. Seem bar nice.', 'created_on': '2024-02-20T22:21:48.244324', 'category': 'teams', 'rolePermissions': ['guest', 'manager', 'admin']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r4/w6gk0qbd6bd_sf7xj6nwdnxc0000gn/T/ipykernel_43319/2776051734.py:44: DeprecationWarning: Received 'size' via a specific parameter in the presence of a 'body' parameter, which is deprecated and will be removed in a future version. Instead, use only 'body' or only specific parameters.\n",
      "  result = self.es.search(index=\"my_documents\", body=body, size=size, from_=from_)\n",
      "/var/folders/r4/w6gk0qbd6bd_sf7xj6nwdnxc0000gn/T/ipykernel_43319/2776051734.py:44: DeprecationWarning: Received 'from_' via a specific parameter in the presence of a 'body' parameter, which is deprecated and will be removed in a future version. Instead, use only 'body' or only specific parameters.\n",
      "  result = self.es.search(index=\"my_documents\", body=body, size=size, from_=from_)\n"
     ]
    }
   ],
   "source": [
    "class Search:\n",
    "    def __init__(self):\n",
    "        self.es = Elasticsearch('http://localhost:9200')\n",
    "        client_info = self.es.info()\n",
    "        print('Connected to Elasticsearch!')\n",
    "        pprint(client_info.body)\n",
    "\n",
    "    def create_index(self):\n",
    "        self.es.indices.delete(index='my_documents', ignore_unavailable=True)\n",
    "        self.es.indices.create(index='my_documents')\n",
    "\n",
    "    def insert_document(self, document):\n",
    "        return self.es.index(index='my_documents', body=document)\n",
    "\n",
    "    def search(self, query=None, filters=None, sort=None, size=10, from_=0):\n",
    "        \"\"\"\n",
    "        검색 메소드\n",
    "        :param query: 검색 쿼리 (딕셔너리)\n",
    "        :param filters: 필터 조건 (리스트 또는 딕셔너리)\n",
    "        :param sort: 정렬 조건 (리스트 또는 딕셔너리)\n",
    "        :param size: 반환할 문서 수\n",
    "        :param from_: 검색 시작 오프셋\n",
    "        :return: 검색 결과\n",
    "        \"\"\"\n",
    "        body = {}\n",
    "        \n",
    "        if query:\n",
    "            body[\"query\"] = query\n",
    "        else:\n",
    "            body[\"query\"] = {\"match_all\": {}}\n",
    "\n",
    "        if filters:\n",
    "            if \"query\" not in body:\n",
    "                body[\"query\"] = {\"bool\": {}}\n",
    "            if isinstance(filters, list):\n",
    "                body[\"query\"][\"bool\"][\"filter\"] = filters\n",
    "            else:\n",
    "                body[\"query\"][\"bool\"][\"filter\"] = [filters]\n",
    "\n",
    "        if sort:\n",
    "            body[\"sort\"] = sort\n",
    "\n",
    "        try:\n",
    "            result = self.es.search(index=\"my_documents\", body=body, size=size, from_=from_)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"검색 중 오류 발생: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "es = Search()\n",
    "\n",
    "# 데이터 삽입 후\n",
    "result = es.search()\n",
    "print(f\"총 {result['hits']['total']['value']}개의 문서가 있습니다.\")\n",
    "for hit in result['hits']['hits']:\n",
    "    print(hit['_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Search:\n",
    "    def __init__(self):\n",
    "        self.es = Elasticsearch('http://localhost:9200')\n",
    "        client_info = self.es.info()\n",
    "        print('Connected to Elasticsearch!')\n",
    "        pprint(client_info.body)\n",
    "\n",
    "    def create_index(self):\n",
    "        self.es.indices.delete(index='my_documents', ignore_unavailable=True)\n",
    "        self.es.indices.create(index='my_documents')\n",
    "\n",
    "    def insert_documents(self, documents):\n",
    "        operations = []\n",
    "        for document in documents:\n",
    "            operations.append({'index': {'_index': 'my_documents'}})\n",
    "            operations.append(document)\n",
    "        return self.es.bulk(operations=operations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method accepts a list of documents. Instead of adding each document separately, it assembles a single list called operations, and then passes the list to the bulk() method. For each document, two entries are added to the operations list:\n",
    "\n",
    "A description of what operation to perform, set to index, with the name of the index given as an argument.\n",
    "The actual data of the document\n",
    "When processing a bulk request, the Elasticsearch service walks the operations list from the start and performs the operations that were requested.\n",
    "\n",
    "Learn more about the bulk() method in the documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regenerating the Index\n",
    "While you work on this tutorial you will need to regenerate the index a few times. To streamline this operation, add a reindex() method to search.py:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Search:\n",
    "    def __init__(self):\n",
    "        self.es = Elasticsearch('http://localhost:9200')\n",
    "        client_info = self.es.info()\n",
    "        print('Connected to Elasticsearch!')\n",
    "        pprint(client_info.body)\n",
    "\n",
    "    def create_index(self):\n",
    "        self.es.indices.delete(index='my_documents', ignore_unavailable=True)\n",
    "        self.es.indices.create(index='my_documents')\n",
    "\n",
    "    def insert_documents(self, documents):\n",
    "        operations = []\n",
    "        for document in documents:\n",
    "            operations.append({'index': {'_index': 'my_documents'}})\n",
    "            operations.append(document)\n",
    "        return self.es.bulk(operations=operations)\n",
    "\n",
    "    def reindex(self):\n",
    "        self.create_index()\n",
    "        with open('data.json', 'rt') as f:\n",
    "            documents = json.loads(f.read())\n",
    "        return self.insert_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Basics\n",
    "Now that you have built an Elasticsearch index and loaded some documents into it, you are ready to implement full-text search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch Queries\n",
    "The Elasticsearch services uses a Query DSL (Domain Specific Language) based on the JSON format to define queries.\n",
    "\n",
    "The Elasticsearch client for Python has a search() method that is used to submit a search query. Let's add a search() helper method in search.py that uses this method:\n",
    "\n",
    "\n",
    "```python\n",
    "class Search:\n",
    "    # ...\n",
    "\n",
    "    def search(self, **query_args):\n",
    "        return self.es.search(index='my_documents', **query_args)\n",
    "\n",
    "```\n",
    "\n",
    "This method invokes the search() method of the Elasticsearch client with the index name. The query_args argument captures all the keyword arguments provided to the method, and then passes-them through to the es.search() method. These arguments are going to be how the caller specifies what to search for.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Queries\n",
    "The Elasticsearch Query DSL offers many different ways to query an index. Looking through the sub-sections in the documentation you will familiarize with the different types of queries that are possible. The very common task of searching text is covered in the Full-Text queries section.\n",
    "\n",
    "For the first search implementation, let's use the Match query. Below you can see an example that uses this query:\n",
    "- https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QueryDSL \n",
    "\n",
    "쿼리절의 두 가지 유형: \n",
    "- Leaf 쿼리 절:\n",
    "  - 특정 필드에서 특정 값을 찾는 쿼리 (예: match, term, range)\n",
    "\n",
    "- Compound 쿼리 절:\n",
    "  - 다른 leaf 또는 compound 쿼리를 감싸는 쿼리\n",
    "  - 여러 쿼리를 논리적으로 결합하거나 (예: bool, dis_max) 동작을 변경 (예: constant_score)\n",
    "\n",
    "쿼리 컨텍스트와 필터 컨텍스트:\n",
    "- 쿼리 절은 사용되는 컨텍스트에 따라 다르게 동작합니다.\n",
    "- 쿼리 컨택스트: \n",
    "  - \"이 문서가 이 쿼리 절과 얼마나 잘 일치하는가?\"에 대한 답을 구하는 것.\n",
    "  - 특징:\n",
    "    - 관련성 점수(relevance score)를 계산합니다.\n",
    "    - 기본적으로 쿼리 결과를 점수에 따라 정렬합니다.\n",
    "    - 전체 텍스트 검색에 주로 사용됩니다.\n",
    "    - 예시: match, multi_match, query_string 쿼리 등\n",
    "- 필터 컨택스트: \n",
    "  -  \"이 문서가 이 쿼리 절과 일치하는가?\"에 대한 예/아니오 답변을 구합니다.\n",
    "  - 특징:\n",
    "    - 관련성 점수를 계산하지 않습니다.\n",
    "    - 결과를 캐싱할 수 있어 성능상 이점이 있습니다.\n",
    "    - 정확한 값이나 범위에 대한 필터링에 주로 사용됩니다.\n",
    "  - 예시: term, terms, range, exists 쿼리 등\n",
    "\n",
    "- 사용 방법:\n",
    "  - bool 쿼리 내에서 다음과 같이 사용됩니다:\n",
    "  - must, should, must_not 절: 쿼리 컨텍스트\n",
    "  - filter 절: 필터 컨텍스트\n",
    "  - must vs filter:\n",
    "    - must: 조건을 만족하면서 동시에 관련성 점수를 계산합니다.\n",
    "    - filter: 조건을 만족하는지만 확인하고, 관련성 점수 계산에는 영향을 주지 않습니다.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"bool\": {\n",
    "    \"must\": [\n",
    "      { \"match\": { \"title\": \"search\" } }  // 쿼리 컨텍스트\n",
    "    ],\n",
    "    \"filter\": [\n",
    "      { \"term\": { \"status\": \"published\" } },  // 필터 컨텍스트\n",
    "      { \"range\": { \"publish_date\": { \"gte\": \"2015-01-01\" } } }  // 필터 컨텍스트\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "- 선택 기준:\n",
    "  - 전체 텍스트 검색이나 유사도 기반 검색: 쿼리 컨텍스트\n",
    "  - 정확한 값 매칭, 범위 검색, 존재 여부 확인: 필터 컨텍스트\n",
    "\n",
    "- 성능 고려사항:\n",
    "  - 가능한 경우 필터 컨텍스트를 사용하는 것이 성능상 유리합니다.\n",
    "  - 필터 결과는 캐시되어 재사용될 수 있지만, 쿼리 결과는 매번 새로 계산됩니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "비용이 많이 드는 쿼리:\n",
    "- 일부 쿼리 유형은 구현 방식 때문에 일반적으로 실행 속도가 느리며, 클러스터의 안정성에 영향을 줄 수 있습니다. 이러한 쿼리는 다음과 같이 분류됩니다:\n",
    "  - 선형 스캔이 필요한 쿼리:\n",
    "    - Script Query \n",
    "  - 초기 비용이 높은 쿼리:\n",
    "    - fuzzy query \n",
    "    - regexp query \n",
    "    - prefix query \n",
    "    - wildcard query \n",
    "    - range query on text and keyword fields \n",
    "  - 조인 쿼리\n",
    "  - 문서당 비용이 높을 수 있는 쿼리: \n",
    "    - script_score queries \n",
    "    - percolate query \n",
    "\n",
    "\n",
    "비용이 많이 드는 쿼리 제어:\n",
    "- search.allow_expensive_queries 설정을 false로 지정하여 이러한 쿼리의 실행을 방지할 수 있습니다 (기본값은 true).\n",
    "\n",
    "\n",
    "\n",
    "Script Query: \n",
    "- 사용자가 정의한 스크립트를 기반으로 문서를 검색하거나 점수를 매기는 방법입니다. 이를 통해 복잡한 조건이나 계산을 쿼리에 포함시킬 수 있습니다.\n",
    "- 문서 내에 있는 필드를 이용한 복잡한 조건을 스크립트로 만들면 검색할 수 있는거임. \n",
    "- 기본적으로 Painless 스크립트 언어를 지원하며, 설정에 따라 다른 언어도 사용 가능합니다.\n",
    "- 성능 고려: 스크립트 쿼리는 일반 쿼리보다 실행 속도가 느릴 수 있으므로 주의가 필요합니다.\n",
    "\n",
    "Script Query 예시: \n",
    "- 이 쿼리는 가격이 100 미만이고 재고가 있는 문서를 검색합니다.\n",
    "\n",
    "\n",
    "```json \n",
    "{\n",
    "  \"query\": {\n",
    "    \"script\": {\n",
    "      \"script\": {\n",
    "        \"source\": \"doc['price'].value < 100 && doc['in_stock'].value == true\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "매개변수를 사용한 스크립트 쿼리:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"script\": {\n",
    "      \"script\": {\n",
    "        \"source\": \"doc['price'].value < params.max_price\",\n",
    "        \"params\": {\n",
    "          \"max_price\": 50\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "스크립트 점수 쿼리:\n",
    "- 이 쿼리는 모든 문서를 대상으로 하되, 인기도와 평점을 조합하여 점수를 계산합니다.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"script_score\": {\n",
    "      \"query\": { \"match_all\": {} },\n",
    "      \"script\": {\n",
    "        \"source\": \"doc['popularity'].value / 10 + doc['rating'].value\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Individual Results\n",
    "You may have noticed that the index.html template renders the title of each search result as a link. The link points to the third and last endpoint that came implemented in the starter Flask application, called get_document. The implementation that is provided returns a \"Document not found\" hardcoded text, so this is what you will see if you click on any of the results while playing with the application.\n",
    "\n",
    "To correctly render individual documents, let's add a retrieve_document() helper method in search.py, using the get() method of the Elasticsearch client:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Search:\n",
    "    def __init__(self):\n",
    "        self.es = Elasticsearch('http://localhost:9200')\n",
    "        client_info = self.es.info()\n",
    "        print('Connected to Elasticsearch!')\n",
    "        pprint(client_info.body)\n",
    "\n",
    "    def create_index(self):\n",
    "        self.es.indices.delete(index='my_documents', ignore_unavailable=True)\n",
    "        self.es.indices.create(index='my_documents')\n",
    "\n",
    "    def insert_documents(self, documents):\n",
    "        operations = []\n",
    "        for document in documents:\n",
    "            operations.append({'index': {'_index': 'my_documents'}})\n",
    "            operations.append(document)\n",
    "        return self.es.bulk(operations=operations)\n",
    "\n",
    "    def reindex(self):\n",
    "        self.create_index()\n",
    "        with open('data.json', 'rt') as f:\n",
    "            documents = json.loads(f.read())\n",
    "        return self.insert_documents(documents)\n",
    "    \n",
    "    def retrieve_document(self, id):\n",
    "        return self.es.get(index='my_documents', id=id)\n",
    "\n",
    "\n",
    "# document = es.retrieve_document(id)\n",
    "# title = document['_source']['name']\n",
    "#  paragraphs = document['_source']['content'].split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching Multiple Fields\n",
    "\n",
    "After you played with the application for a while you may have noticed that a lot of queries return no results. As you recall, the search is currently implemented on the name field of each document, which is where the document titles are stored. Documents also have summary and content fields, which have longer texts that are apt to be searched as well, but right now these are ignored.\n",
    "\n",
    "In this section you are going to learn about another common full-text search query, the Multi-match, which requests a search to be carried out across multiple fields of an index.\n",
    "\n",
    "Here is the example multi-match query from the documentation:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-match query\n",
    " \n",
    "multi-match query는 여러 필드에 대해 동시에 검색을 수행할 수 있는 유연한 쿼리 타입입니다. 이 쿼리는 같은 검색어를 여러 필드에 적용하고자 할 때 특히 유용합니다.\n",
    "\n",
    "주요 특징:\n",
    "- 여러 필드 검색: 하나의 쿼리로 여러 필드를 동시에 검색할 수 있습니다.\n",
    "- 다양한 타입: 여러 가지 매치 전략을 제공합니다 (best_fields, most_fields, cross_fields 등).\n",
    "- 필드 가중치: 특정 필드에 더 높은 중요도를 부여할 수 있습니다.\n",
    "- 유연성: 와일드카드를 사용하여 유사한 이름의 여러 필드를 한 번에 지정할 수 있습니다.\n",
    "\n",
    "기본 구문:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"multi_match\": {\n",
    "      \"query\": \"검색어\",\n",
    "      \"fields\": [\"필드1\", \"필드2\", \"필드3\"]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "주요 매치 타입:\n",
    "- best_fields (기본값):\n",
    "  - 가장 높은 점수를 받은 필드의 점수를 사용합니다.\n",
    "  - 하나의 필드에서 많은 검색어가 나타나는 문서를 선호합니다.\n",
    "\n",
    "- most_fields:\n",
    "  - 모든 매칭 필드의 점수를 합산합니다.\n",
    "  - 각 필드에 대해 독립적으로 쿼리를 실행한 후, 결과 점수를 합산합니다.\n",
    "  - 각 필드는 개별적으로 분석되고 점수가 매겨집니다.\n",
    "  - most_fields: 각 필드별로 독립적인 IDF를 사용\n",
    "  - 여러 필드에 걸쳐 검색어가 나타나는 문서를 선호합니다\n",
    "\n",
    "- cross_fields:\n",
    "  - 모든 필드를 하나의 큰 필드처럼 취급합니다.\n",
    "  - cross_fields: 모든 필드에 걸쳐 통합된 IDF를 사용 \n",
    "  - 검색어가 여러 필드에 걸쳐 나타나는 경우에 유용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagination\n",
    "\n",
    "It is often impractical for an application to deal with a very large number of results. For this reason, APIs and web services use pagination controls to allow applications to request the results in small chunks or pages.\n",
    "\n",
    "You may have noticed that Elasticsearch by default does not return more than 10 results. The optional size parameter can be given in a search request to change this maximum. The following example asks for up to 5 search results to be returned:\n",
    "\n",
    "\n",
    "```python\n",
    "results = es.search(\n",
    "    query={\n",
    "        'multi_match': {\n",
    "            'query': query,\n",
    "            'fields': ['name', 'summary', 'content'],\n",
    "        }\n",
    "    }, size=5\n",
    ")\n",
    "```\n",
    "\n",
    "To access additional pages of results, the from_ parameter is used, which indicates from where in the complete list of results to start (since from is a reserved keyword in Python, from_ is used).\n",
    "\n",
    "The next example retrieves a second page of 5 results:\n",
    "\n",
    "\n",
    "```python\n",
    "results = es.search(\n",
    "    query={\n",
    "        'multi_match': {\n",
    "            'query': query,\n",
    "            'fields': ['name', 'summary', 'content'],\n",
    "        }\n",
    "    }, size=5, from_=5\n",
    ")\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters\n",
    "\n",
    "Many applications need to give users the power to customize queries in ways that complement what search queries alone can do. In this chapter you are going to learn about filtering, a technique that makes it possible to specify that a search query is executed only on the subset of the documents contained in an index that satisfy a given condition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Boolean Queries\n",
    "Before you can implement filters you have to understand how compound queries are implemented in Elasticsearch.\n",
    "\n",
    "A compound query allows an application to combine two or more individual queries, so that they execute together, and if appropriate, return a combined set of results. The standard way to create compound queries in Elasticsearch is to use a Boolean query.\n",
    "\n",
    "A boolean query acts as a wrapper for two or more individual queries or clauses. There are four different ways to combine queries:\n",
    "\n",
    "- bool.must: the clause must match. If multiple clauses are given, all must match (similar to an AND logical operation).\n",
    "- bool.should: when used without must, at least one clause should match (similar to an OR logical operation). When combined with must each matching clause boosts the relevance score of the document.\n",
    "- bool.filter: only documents that match the clause(s) are considered search result candidates.\n",
    "- bool.must_not: only documents that do not match the clause(s) are considered search result candidates.\n",
    "\n",
    "As you can probably guess from the above, boolean queries involve a fair amount of complexity and can be used in a variety of ways. In this chapter you are going to learn how to combine the multi-match full-text search clause implemented in the previous chapters with a filter that restricts results to one category of documents. Recall that the dataset used with this tutorial includes a category field that can be set to sharepoint, teams or github.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Filter to a Query\n",
    "\n",
    "The multi-match query that is currently implemented in the tutorial application uses the following structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "    'multi_match': {\n",
    "        'query': \"query text here\",\n",
    "        'fields': ['name', 'summary', 'content'],\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "To add a filter that restricts this search to a specific category, the query must be expanded as follows:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    'bool': {\n",
    "        'must': [{\n",
    "            'multi_match': {\n",
    "                'query': \"query text here\",\n",
    "                'fields': ['name', 'summary', 'content'],\n",
    "            }\n",
    "        }],\n",
    "        'filter': [{\n",
    "            'term': {\n",
    "                'category.keyword': {\n",
    "                    'value': \"category to filter\"\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Let's look at the new components in this query in detail.\n",
    "\n",
    "First of all, the multi_match query has been moved inside a bool.must clause. The bool.must clause is usually the place where the base query is defined. Note that must accepts a list of queries to search for, so this allows multiple base-level queries to be combined when desired.\n",
    "\n",
    "The filtering is implemented in a bool.filter section, using a new query type, the term query. Using a match or multi_match query for a filter is not a good idea, because these are full-text search queries. For the purpose of filtering, the query must return an absolute true or false answer for each document and not a relevance score like the match queries do.\n",
    "\n",
    "The term query performs an exact search for the a value in a given field. This type of query is useful to search for identifiers, labels, tags, or as in this case, categories.\n",
    "\n",
    "This query does not work well with fields that are indexed for full-text search. String fields are assigned a default type of text, and have their contents analyzed and separated into individual words before they are indexed. Elasticsearch assigns string fields a secondary type of keyword, which indexes the field contents as a whole, making them more appropriate for filtering with the term query. By using a field name of category.keyword in the filter portion of the query, the keyword typed variant of the field is used instead of the default text one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Query \n",
    "\n",
    "Term Query는 정확한 값 매칭을 위해 사용되는 쿼리 타입임. \n",
    "\n",
    "기본 개념:\n",
    "- Term Query는 지정된 필드에서 정확히 일치하는 용어를 찾습니다.\n",
    "- 대소문자를 구분하며, 분석되지 않은 정확한 값을 검색합니다.\n",
    "\n",
    "주요 특징:\n",
    "- 정확한 매칭: 부분 일치나 유사 매칭이 아닌 정확한 일치만을 찾습니다.\n",
    "- 분석 없음: 쿼리 문자열이 분석되지 않고 그대로 사용됩니다.\n",
    "- 빠른 성능: 정확한 매칭으로 인해 매우 빠른 검색이 가능합니다.\n",
    "\n",
    "기본 구문: \n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"term\": {\n",
    "      \"field_name\": \"exact_value\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "주의사항:\n",
    "- text 필드에 사용 시 주의: text 필드는 기본적으로 분석되므로, term query와 함께 사용할 때 예상치 못한 결과가 나올 수 있습니다.\n",
    "- keyword 필드 사용 권장: 정확한 매칭을 위해서는 keyword 타입 필드를 사용하는 것이 좋습니다.\n",
    "- 대소문자 구분: \"Active\"와 \"active\"는 다른 값으로 취급됩니다.\n",
    "\n",
    "text 필드와 함께 사용할 때의 팁:\n",
    "- 필드명에 '.keyword'를 추가하여 keyword 버전의 필드를 사용합니다.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"term\": {\n",
    "      \"status.keyword\": \"Active\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "다중 값 검색 (terms query):\n",
    "- 여러 값 중 하나와 일치하는 문서를 찾고 싶을 때 사용합니다.\n",
    "\n",
    "```json \n",
    "{\n",
    "  \"query\": {\n",
    "    \"terms\": {\n",
    "      \"status\": [\"active\", \"pending\"]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "``` \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Type Query \n",
    "\n",
    "text 타입의 필드 검색을 위해 Elasticsearch는 여러 가지 쿼리 타입을 제공합니다. 이들은 전문 검색(full-text search)에 적합하며, 각각 다른 특성과 사용 사례를 가지고 있습니다. 주요 쿼리 타입들은 다음과 같습니다:\n",
    "\n",
    "Match Query:\n",
    "- 가장 기본적이고 널리 사용되는 전문 검색 쿼리입니다.\n",
    "- 제공된 텍스트를 분석하고 개별 용어로 분리한 후 검색합니다.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"description\": \"quick brown fox\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Multi-Match Query:\n",
    "- 여러 필드에서 동일한 검색어를 찾을 때 사용합니다.\n",
    "\n",
    "```json\n",
    "text 타입의 필드 검색을 위해 Elasticsearch는 여러 가지 쿼리 타입을 제공합니다. 이들은 전문 검색(full-text search)에 적합하며, 각각 다른 특성과 사용 사례를 가지고 있습니다. 주요 쿼리 타입들은 다음과 같습니다:\n",
    "\n",
    "Match Query:\n",
    "\n",
    "가장 기본적이고 널리 사용되는 전문 검색 쿼리입니다.\n",
    "제공된 텍스트를 분석하고 개별 용어로 분리한 후 검색합니다.\n",
    "\n",
    "jsonCopy{\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"description\": \"quick brown fox\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "Multi-Match Query:\n",
    "- 여러 필드에서 동일한 검색어를 찾을 때 사용합니다.\n",
    "\n",
    "jsonCopy{\n",
    "  \"query\": {\n",
    "    \"multi_match\": {\n",
    "      \"query\": \"quick brown fox\",\n",
    "      \"fields\": [\"title\", \"description\"]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "Match Phrase Query:\n",
    "- 정확한 구문을 검색할 때 사용합니다.\n",
    "- 단어의 순서와 근접성을 고려합니다.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"match_phrase\": {\n",
    "      \"description\": \"quick brown fox\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "``` \n",
    "\n",
    "Query String Query:\n",
    "- 복잡한 검색 구문을 지원합니다 (AND, OR, NOT 등).\n",
    "- 사용자가 직접 검색 쿼리를 입력할 때 유용합니다.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"query_string\": {\n",
    "      \"default_field\": \"description\",\n",
    "      \"query\": \"quick AND fox OR (brown AND dog)\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "``` \n",
    "\n",
    "Prefix Query:\n",
    "- 특정 접두사로 시작하는 단어를 검색합니다.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"prefix\": {\n",
    "      \"title\": \"qu\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Wildcard Query:\n",
    "- 와일드카드 문자 (*,?)를 사용한 패턴 매칭을 지원합니다.\n",
    "\n",
    "```json\n",
    "text 타입의 필드 검색을 위해 Elasticsearch는 여러 가지 쿼리 타입을 제공합니다. 이들은 전문 검색(full-text search)에 적합하며, 각각 다른 특성과 사용 사례를 가지고 있습니다. 주요 쿼리 타입들은 다음과 같습니다:\n",
    "\n",
    "Match Query:\n",
    "\n",
    "가장 기본적이고 널리 사용되는 전문 검색 쿼리입니다.\n",
    "제공된 텍스트를 분석하고 개별 용어로 분리한 후 검색합니다.\n",
    "\n",
    "jsonCopy{\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"description\": \"quick brown fox\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "Multi-Match Query:\n",
    "\n",
    "여러 필드에서 동일한 검색어를 찾을 때 사용합니다.\n",
    "\n",
    "jsonCopy{\n",
    "  \"query\": {\n",
    "    \"multi_match\": {\n",
    "      \"query\": \"quick brown fox\",\n",
    "      \"fields\": [\"title\", \"description\"]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "Match Phrase Query:\n",
    "\n",
    "정확한 구문을 검색할 때 사용합니다.\n",
    "단어의 순서와 근접성을 고려합니다.\n",
    "\n",
    "jsonCopy{\n",
    "  \"query\": {\n",
    "    \"match_phrase\": {\n",
    "      \"description\": \"quick brown fox\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "Query String Query:\n",
    "\n",
    "복잡한 검색 구문을 지원합니다 (AND, OR, NOT 등).\n",
    "사용자가 직접 검색 쿼리를 입력할 때 유용합니다.\n",
    "\n",
    "jsonCopy{\n",
    "  \"query\": {\n",
    "    \"query_string\": {\n",
    "      \"default_field\": \"description\",\n",
    "      \"query\": \"quick AND fox OR (brown AND dog)\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "Simple Query String Query:\n",
    "\n",
    "Query String Query의 간소화된 버전입니다.\n",
    "구문 오류에 더 관대합니다.\n",
    "\n",
    "jsonCopy{\n",
    "  \"query\": {\n",
    "    \"simple_query_string\": {\n",
    "      \"fields\": [\"title\", \"description\"],\n",
    "      \"query\": \"quick brown +fox -dog\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "Prefix Query:\n",
    "\n",
    "특정 접두사로 시작하는 단어를 검색합니다.\n",
    "\n",
    "jsonCopy{\n",
    "  \"query\": {\n",
    "    \"prefix\": {\n",
    "      \"title\": \"qu\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "Wildcard Query:\n",
    "\n",
    "와일드카드 문자 (*,?)를 사용한 패턴 매칭을 지원합니다.\n",
    "\n",
    "jsonCopy{\n",
    "  \"query\": {\n",
    "    \"wildcard\": {\n",
    "      \"description\": \"qu*k\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "Fuzzy Query:\n",
    "- 철자 오류를 허용하는 유사 검색을 수행합니다.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"fuzzy\": {\n",
    "      \"description\": {\n",
    "        \"value\": \"quik\",\n",
    "        \"fuzziness\": \"AUTO\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying a Filter\n",
    "Before the filtered query can be implemented, it is necessary to add a way for end users to enter a desired filter. The solution implemented in this tutorial will look for a category:<category-name> pattern in the text of the search query. Let's add a function called extract_filters() to app.py to look for filter expressions:\n",
    "\n",
    "\n",
    "```python\n",
    "def extract_filters(query):\n",
    "    filters = []\n",
    "\n",
    "    filter_regex = r'category:([^\\s]+)\\s*'\n",
    "    m = re.search(filter_regex, query)\n",
    "    if m:\n",
    "        filters.append({\n",
    "            'term': {\n",
    "                'category.keyword': {\n",
    "                    'value': m.group(1)\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        query = re.sub(filter_regex, '', query).strip()\n",
    "\n",
    "    return {'filter': filters}, query\n",
    "```\n",
    "\n",
    "The function accepts the query entered by the user and returns a tuple with the filters that were found in the query, and the modified query after the filters were removed. To look for the filter pattern it uses a regular expression. The function is designed to be expanded with additional filters.\n",
    "\n",
    "When a filter is found, the filters list is extended with a corresponding filter expression, which in this case is based on the term query, as discussed above.\n",
    "\n",
    "To better understand how this function works, start a Python session (make sure the virtual environment is activated first) and run the following code:\n",
    "\n",
    "```python\n",
    "from app import extract_filters\n",
    "extract_filters('this is the search text category:sharepoint')\n",
    "\n",
    "```\n",
    "\n",
    "The returned tuple from the function should be:\n",
    "\n",
    "{'filter': [{'term': 'category.keyword': {'value': 'sharepoint'}}]}, 'this is the search text'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Filtered Search\n",
    "\n",
    "What remains to do is to change the handle_search() function to send an updated query that combines the full-text search expression with a filter, if one is given by the user. Below is the new version of this function:\n",
    "\n",
    "```python\n",
    "@app.post('/')\n",
    "def handle_search():\n",
    "    query = request.form.get('query', '')\n",
    "    filters, parsed_query = extract_filters(query)\n",
    "    from_ = request.form.get('from_', type=int, default=0)\n",
    "\n",
    "    results = es.search(\n",
    "        query={\n",
    "            'bool': {\n",
    "                'must': {\n",
    "                    'multi_match': {\n",
    "                        'query': parsed_query,\n",
    "                        'fields': ['name', 'summary', 'content'],\n",
    "                    }\n",
    "                },\n",
    "                **filters\n",
    "            }\n",
    "        },\n",
    "        size=5,\n",
    "        from_=from_\n",
    "    )\n",
    "    return render_template('index.html', results=results['hits']['hits'],\n",
    "                           query=query, from_=from_,\n",
    "                           total=results['hits']['total']['value'])\n",
    "\n",
    "```\n",
    "\n",
    "The query has now been changed to send a bool expression, and the search expression was moved inside a must section under it. The extract_filters() function returns the filter portion of the query in the form it needs to be sent to Elasticsearch, so it is inserted in the query dictionary also under the top-level bool key.\n",
    "\n",
    "Try a search query such as work from home category:sharepoint to see how only documents from the given category are returned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range Filters\n",
    "Elasticsearch supports a variety of filters besides the term filter. Another one that is commonly used is the range filter, which works with numbers and dates. Let's add a year filter that can be used to restrict results based on the year they were last updated, which is given in the updated_at field.\n",
    "\n",
    "Below is an updated version of the extract_filters() function that looks for both category:<category> and year:<yyyy> as filters:\n",
    "\n",
    "```python\n",
    "def extract_filters(query):\n",
    "    filters = []\n",
    "\n",
    "    filter_regex = r'category:([^\\s]+)\\s*'\n",
    "    m = re.search(filter_regex, query)\n",
    "    if m:\n",
    "        filters.append({\n",
    "            'term': {\n",
    "                'category.keyword': {\n",
    "                    'value': m.group(1)\n",
    "                }\n",
    "            },\n",
    "        })\n",
    "        query = re.sub(filter_regex, '', query).strip()\n",
    "\n",
    "    filter_regex = r'year:([^\\s]+)\\s*'\n",
    "    m = re.search(filter_regex, query)\n",
    "    if m:\n",
    "        filters.append({\n",
    "            'range': {\n",
    "                'updated_at': {\n",
    "                    'gte': f'{m.group(1)}||/y',\n",
    "                    'lte': f'{m.group(1)}||/y',\n",
    "                }\n",
    "            },\n",
    "        })\n",
    "        query = re.sub(filter_regex, '', query).strip()\n",
    "\n",
    "    return {'filter': filters}, query\n",
    "\n",
    "```\n",
    "\n",
    "This version adds a second regular expression to find year:yyyy in the query string. It creates a range filter for the updated_at field, and sets the low and high bounds of the range to the year that is given after the colon, which is captured in the regular expression match as m.group(1).\n",
    "\n",
    "There is a small complication, because the updated_at field contains full dates, and in this filter only needs to look at the year. Luckily, when the range filter is used with date field the bounds of the range can be enhanced with date math. The ||/y suffix that is added to the gte (lower bound) and lte (upper bound) parameters of the range indicates that the given value is a year that must be completed to form a full date that can be compared against the field.\n",
    "\n",
    "With this change, you can include a query such as year:2020 work from home to see results from the requested year only. The query can include the two filters as well, for example year:2020 category:teams work from home.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The match-all query\n",
    "Before moving on to a new topic, try entering only a filter in the search query text field, for example category:github. Unfortunately this does not return any results, but the expected behavior in this case would be to receive all the results that match the requested category.\n",
    "\n",
    "What happens is that the extract_filters() function returns a tuple with the filter(s) in the first element and an empty query string in the second element. The multi_match query receives the empty string, and returns an empty list of results, because nothing matches an empty string.\n",
    "\n",
    "To address this special case, the multi_match query can be replaced with match_all when the search text is empty. The version of the handle_search() function below adds logic to do this. Update the function in app.py.\n",
    "\n",
    "\n",
    "```python\n",
    "@app.post('/')\n",
    "def handle_search():\n",
    "    query = request.form.get('query', '')\n",
    "    filters, parsed_query = extract_filters(query)\n",
    "    from_ = request.form.get('from_', type=int, default=0)\n",
    "\n",
    "    if parsed_query:\n",
    "        search_query = {\n",
    "            'must': {\n",
    "                'multi_match': {\n",
    "                    'query': parsed_query,\n",
    "                    'fields': ['name', 'summary', 'content'],\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        search_query = {\n",
    "            'must': {\n",
    "                'match_all': {}\n",
    "            }\n",
    "        }\n",
    "\n",
    "    results = es.search(\n",
    "        query={\n",
    "            'bool': {\n",
    "                **search_query,\n",
    "                **filters\n",
    "            }\n",
    "        },\n",
    "        size=5,\n",
    "        from_=from_\n",
    "    )\n",
    "    return render_template('index.html', results=results['hits']['hits'],\n",
    "                           query=query, from_=from_,\n",
    "                           total=results['hits']['total']['value'])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this version, you can ask for all the documents that match a category. Note how all the results that are returned come back with the same score of 1.0, because there are no search terms to compute scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faceted Search\n",
    "\n",
    "Faceted Search(패싯 검색)는 Elasticsearch에서 제공하는 강력한 검색 기능 중 하나입니다. 이 기능을 통해 사용자는 검색 결과를 다양한 카테고리나 속성(패싯)으로 필터링하고 정리할 수 있습니다. \n",
    "\n",
    "기본 개념:\n",
    "- Faceted Search는 검색 결과를 여러 차원으로 분류하여 제시합니다.\n",
    "- 사용자가 검색 결과를 쉽게 탐색하고 필터링할 수 있게 해줍니다.\n",
    "\n",
    "\n",
    "주요 특징:\n",
    "- 동적 필터링: 사용자가 실시간으로 검색 결과를 필터링할 수 있습니다.\n",
    "- 다차원 분류: 여러 속성을 기준으로 결과를 분류합니다.\n",
    "- 결과 요약: 각 패싯에 대한 문서 수를 제공합니다.\n",
    "\n",
    "일반적인 패싯 유형:\n",
    "- 카테고리 패싯: 제품 카테고리, 브랜드 등\n",
    "- 숫자 범위 패싯: 가격 범위, 날짜 범위 등\n",
    "- 태그 패싯: 키워드, 태그 등\n",
    "\n",
    "사용 예시:\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"description\": \"laptop\"\n",
    "    }\n",
    "  },\n",
    "  \"aggs\": {\n",
    "    \"brands\": {\n",
    "      \"terms\": {\n",
    "        \"field\": \"brand.keyword\"\n",
    "      }\n",
    "    },\n",
    "    \"price_ranges\": {\n",
    "      \"range\": {\n",
    "        \"field\": \"price\",\n",
    "        \"ranges\": [\n",
    "          { \"to\": 500 },\n",
    "          { \"from\": 500, \"to\": 1000 },\n",
    "          { \"from\": 1000 }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "실제 적용 사례:\n",
    "- 전자상거래: 제품을 브랜드, 가격 범위, 색상 등으로 필터링\n",
    "- 도서 검색: 저자, 출판년도, 장르 등으로 필터링\n",
    "- 호텔 예약: 위치, 가격, 별점 등으로 필터링\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faceted Search에서의 동적 필터링\n",
    "\n",
    "동적 필터링의 핵심 개념:\n",
    "- 실시간 업데이트: 사용자가 필터를 선택할 때마다 검색 결과와 다른 패싯들이 즉시 업데이트됩니다.\n",
    "- 상호 의존적 패싯: 하나의 패싯 선택이 다른 패싯의 가용한 옵션들에 영향을 줍니다.\n",
    "- 필터 조합: 여러 패싯에서 선택한 필터들이 조합되어 적용됩니다.\n",
    "\n",
    "\n",
    "구현 방법:\n",
    "- 1. 필터 적용:\n",
    "  - 사용자가 패싯을 선택할 때마다 해당 필터를 쿼리에 추가합니다.\n",
    "\n",
    "```json\n",
    "GET /products/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": [\n",
    "        { \"match\": { \"description\": \"laptop\" } }\n",
    "      ],\n",
    "      \"filter\": [\n",
    "        { \"term\": { \"brand\": \"BrandA\" } },\n",
    "        { \"range\": { \"price\": { \"gte\": 500, \"lt\": 1000 } } }\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"aggs\": {\n",
    "    // aggregations here\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "- 2. 동적 집계 (Dynamic Aggregations): 선택된 필터를 제외한 나머지 패싯에 대해 정확한 카운트를 유지하기 위해 필터된 집계를 사용합니다.\n",
    "\n",
    "```json\n",
    "GET /products/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": [\n",
    "        { \"match\": { \"description\": \"laptop\" } }\n",
    "      ],\n",
    "      \"filter\": [\n",
    "        { \"term\": { \"brand\": \"BrandA\" } },\n",
    "        { \"range\": { \"price\": { \"gte\": 500, \"lt\": 1000 } } }\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"aggs\": {\n",
    "    \"all_categories\": {\n",
    "      \"global\": {},\n",
    "      \"aggs\": {\n",
    "        \"categories\": {\n",
    "          \"terms\": { \"field\": \"category\" }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"filtered_brands\": {\n",
    "      \"filter\": {\n",
    "        \"range\": { \"price\": { \"gte\": 500, \"lt\": 1000 } }\n",
    "      },\n",
    "      \"aggs\": {\n",
    "        \"brands\": {\n",
    "          \"terms\": { \"field\": \"brand\" }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"price_ranges\": {\n",
    "      \"filter\": {\n",
    "        \"term\": { \"brand\": \"BrandA\" }\n",
    "      },\n",
    "      \"aggs\": {\n",
    "        \"price_ranges\": {\n",
    "          \"range\": {\n",
    "            \"field\": \"price\",\n",
    "            \"ranges\": [\n",
    "              { \"to\": 500 },\n",
    "              { \"from\": 500, \"to\": 1000 },\n",
    "              { \"from\": 1000 }\n",
    "            ]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "- 3. 필터 제거 (Filter Removal): 사용자가 필터를 제거할 수 있도록 하고, 제거 시 해당 필터를 쿼리에서 삭제합니다.\n",
    "\n",
    "- 4. 브라우저 상태 관리: 선택된 필터들을 URL 파라미터나 브라우저 상태로 관리하여 페이지 새로고침 후에도 필터 상태를 유지할 수 있게 합니다.\n",
    "\n",
    "- 5. 캐싱: 자주 사용되는 필터 조합에 대한 결과를 캐싱하여 응답 시간을 개선할 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "정리하자면 동적 필터링 기능을 제공하기 위해선 두 가지 작업이 필요함: \n",
    "- 쿼리의 filter 부분에 사용자가 선택한 필터 조건 추가: 이는 검색 결과를 사용자의 선택에 맞게 필터링합니다.\n",
    "- aggregations (aggs) 부분에 각 패싯에 대한 필터 추가: 이는 다른 패싯의 카운트를 정확하게 유지하기 위함입니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Aggregations\n",
    "\n",
    "In Elasticsearch faceted search is implemented using the aggregations feature. One of the supported aggregations divides the search results in buckets, based on some criteria. The list of buckets, each including the number of documents it contains, is going to be used to render the facets sidebar.\n",
    "\n",
    "The simplest type of bucket aggregation is the one in which buckets are defined for each keyword. This type, which is called terms aggregation is perfect to create the buckets for the category field. Here is the search request from the application, expanded to ask for category aggregations:\n",
    "\n",
    "\n",
    "```python\n",
    "results = es.search(\n",
    "    query={\n",
    "        'bool': {\n",
    "            **search_query,\n",
    "            **filters\n",
    "        }\n",
    "    },\n",
    "    aggs={\n",
    "        'category-agg': {\n",
    "            'terms': {\n",
    "                'field': 'category.keyword',\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    size=5,\n",
    "    from_=from_\n",
    ")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregtaions \n",
    "\n",
    "Elasticsearch의 aggregations(집계) 기능은 데이터를 그룹화하고 통계를 계산하는 강력한 도구입니다. \n",
    "\n",
    "기본 개념:\n",
    "- 데이터를 그룹화하고 메트릭을 계산합니다.\n",
    "- 검색 쿼리와 함께 또는 독립적으로 사용할 수 있습니다.\n",
    "- 복잡한 데이터 분석과 요약을 가능하게 합니다.\n",
    "\n",
    "주요 타입:\n",
    "- Bucket Aggregations:\n",
    "  - 데이터를 그룹(버킷)으로 나눕니다.\n",
    "  - 예: terms, date_histogram, range\n",
    "\n",
    "- Metric Aggregations: \n",
    "  - 숫자 필드에 대한 계산을 수행합니다.\n",
    "  - 예: avg, sum, min, max, cardinality\n",
    "\n",
    "- Pipeline Aggregations:\n",
    "  - 다른 집계의 결과를 입력으로 사용합니다.\n",
    "  - 예: avg_bucket, sum_bucket, cumulative_su\n",
    "\n",
    "주요 Bucket Aggregations:\n",
    "- Terms: 필드의 고유 값을 기준으로 그룹화\n",
    "- Date Histogram: 날짜/시간 필드를 기준으로 그룹화\n",
    "- Range: 숫자나 날짜 범위로 그룹화\n",
    "- Filters: 사전 정의된 필터로 그룹화\n",
    "\n",
    "주요 Metric Aggregations:\n",
    "- Avg: 평균 계산\n",
    "- Sum: 합계 계산\n",
    "- Min/Max: 최소/최대값 찾기\n",
    "- Cardinality: 고유 값의 개수 계산 (근사값)\n",
    "- Percentiles: 백분위수 계산\n",
    "\n",
    "\n",
    "사용 예시: \n",
    "- 이 예시는 가장 인기 있는 5개 색상과 각 색상의 평균 가격을 계산합니다.\n",
    " \n",
    "```json\n",
    "GET /my_index/_search\n",
    "{\n",
    "  \"size\": 0,\n",
    "  \"aggs\": {\n",
    "    \"popular_colors\": {\n",
    "      \"terms\": {\n",
    "        \"field\": \"color\",\n",
    "        \"size\": 5\n",
    "      },\n",
    "      \"aggs\": {\n",
    "        \"avg_price\": {\n",
    "          \"avg\": {\n",
    "            \"field\": \"price\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buckets \n",
    "\n",
    "buckets(버킷)은 aggregations(집계) 기능의 핵심 개념 중 하나입니다. 버킷은 특정 기준에 따라 문서들을 그룹화하는 컨테이너라고 생각할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
