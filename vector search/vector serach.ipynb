{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search\n",
    "\n",
    "This section will introduce you to a different way of searching that leverages Machine Learning (ML) techniques to interpret meaning and context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Embeddings\n",
    "In Machine Learning, an embedding is a vector (an array of numbers) that represents real-world objects such as words, sentences, images or videos. The interesting property that these embeddings have is that two embeddings that represent similar or related real-world entities will share some similarities as well, so embeddings can be compared, and a distance between them can be calculated.\n",
    "\n",
    "When thinking specifically in terms of an application for searching, performing a search of embeddings in the vector space tends to find results that are more related to concepts, instead of to the exact keywords typed in the search prompt.\n",
    "\n",
    "In this section of the tutorial you are going to learn how to generate embeddings using freely available machine learning models, then you will use Elasticsearch's vector database support to store and search these embeddings. And towards the end, you will also combine vector and full-text search results and create a powerful hybrid search solution that offers the best of both approaches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings\n",
    "In this section you are going to learn about one of the most convenient options that are available to generate embeddings for text, which is based on the SentenceTransformers framework.\n",
    "\n",
    "Working with SentenceTransformers is the recommended path while you explore and become familiar with the use of embeddings, as the models that are available under this framework can be installed on your computer, perform reasonably well without a GPU, and are free to use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentenceTransformers\n",
    "\n",
    "SentenceTransformers는 문장이나 텍스트의 의미를 고차원 벡터 공간에 매핑하는 데 사용되는 Python 프레임워크입니다. 이 프레임워크는 주로 자연어 처리(NLP) 작업에 사용되며, 특히 Vector Search와 같은 의미 기반 검색에 매우 유용합니다.\n",
    "\n",
    "기본 개념:\n",
    "- 텍스트를 고정 길이의 dense vector로 변환합니다.\n",
    "- 이 벡터는 텍스트의 의미적 내용을 포착합니다.\n",
    "\n",
    "모델 기반:\n",
    "- 주로 BERT, RoBERTa, XLM-RoBERTa 등의 트랜스포머 모델을 기반으로 합니다.\n",
    "- 다양한 사전 훈련된 모델을 제공합니다.\n",
    "\n",
    "다국어 지원:\n",
    "- 100개 이상의 언어를 지원하는 모델을 포함합니다.\n",
    "\n",
    "사용 예시: \n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install SentenceTransformers\n",
    "The SentenceTransformers framework is installed as a Python package. Make sure that your Python virtual environment is activated, and then run the following command on your terminal to install this framework:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from sentence-transformers) (4.41.2)\n",
      "Requirement already satisfied: tqdm in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: numpy in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from sentence-transformers) (1.26.4)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.5.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (12 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from sentence-transformers) (0.23.4)\n",
      "Requirement already satisfied: Pillow in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Using cached sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "Downloading scikit_learn-1.5.1-cp39-cp39-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sentence-transformers\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.1 scipy-1.13.1 sentence-transformers-3.0.1 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a Model\n",
    "The next task is to decide on a machine learning model to use for embedding generation. There is a list of pretrained models in the documentation. Because SentenceTransformers is a very popular framework, there are also compatible models created by researchers not directly associated with the framework. To see a complete list of models that can be used, you can check the SentenceTransformers tag on HuggingFace.\n",
    "\n",
    "For the purposes of this tutorial there is no need to overthink the model selection, as any model will suffice. The SentenceTransformers documentation includes the following note with regards to their pretrained models:\n",
    "\n",
    "The all-* models where trained on all available training data (more than 1 billion training pairs) and are designed as general purpose models. The all-mpnet-base-v2 model provides the best quality, while all-MiniLM-L6-v2 is 5 times faster and still offers good quality.\n",
    "\n",
    "This seems to suggest that their all-MiniLM-L6-v2 model is a good choice that offers a good compromise between speed and quality, so let's use this model. Locate this model in the table, and click the \"info\" icon to see some information about it.\n",
    "\n",
    "An interesting detail that is good to be aware of about your chosen model is the length the generated embeddings have, or in other words, how many numbers or dimensions the resulting vectors will have. This is important because it directly affects the amount of storage you will need. In the case of all-MiniLM-L6-v2, the generated vectors have 384 dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model\n",
    "The following Python code demonstrates how the model is loaded. You can try this in a Python shell.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time you do this, the model will be downloaded and installed in your virtual environment, so the call may take some time to return. Once the model is installed, instantiating it should not take long.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings\n",
    "With the model instantiated, you are now ready to generate an embedding. To do this, pass the source text to the model.encode() method:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.encode('The quick brown fox jumps over the lazy dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.54968011e-02,  6.12862706e-02,  5.26920557e-02,  7.07050413e-02,\n",
       "        3.31013948e-02, -3.06696109e-02,  6.62061898e-03, -6.11832850e-02,\n",
       "       -1.32602104e-03,  1.06456643e-02,  3.86499353e-02,  3.99531797e-02,\n",
       "       -3.83675545e-02, -1.66687984e-02, -5.61561156e-03, -2.43558995e-02,\n",
       "       -3.59968394e-02, -3.02429274e-02,  5.84700331e-02, -4.94961701e-02,\n",
       "       -7.72954598e-02, -5.23876920e-02,  2.45271791e-02,  2.93105822e-02,\n",
       "       -7.39091635e-02, -2.49592010e-02, -6.53142035e-02, -4.28864807e-02,\n",
       "        7.11656362e-02, -1.13819472e-01, -1.26593364e-02,  3.96260805e-02,\n",
       "       -2.10036244e-02,  1.78064071e-02, -3.18874270e-02, -9.11229402e-02,\n",
       "        5.91224581e-02, -7.30397413e-03,  3.31367701e-02,  2.99061127e-02,\n",
       "        4.21688668e-02, -1.69129800e-02, -4.50015739e-02,  2.96744388e-02,\n",
       "       -9.92584750e-02,  5.32891788e-02, -7.64784738e-02, -1.48679931e-02,\n",
       "        1.52494945e-02,  1.37893632e-02, -4.41923775e-02, -2.78393030e-02,\n",
       "        6.73079677e-03,  5.64969927e-02,  7.21781850e-02, -4.12064092e-03,\n",
       "       -3.77659616e-03, -3.55087370e-02,  4.90683950e-02, -1.03429845e-02,\n",
       "        2.36084294e-02,  3.63824107e-02,  1.80067439e-02, -9.42732149e-04,\n",
       "        3.87706645e-02,  2.31451318e-02, -2.71658991e-02, -8.00189674e-02,\n",
       "       -9.76722687e-02,  3.99069861e-03,  1.36213414e-02, -4.74255942e-02,\n",
       "       -1.67798586e-02, -9.50407144e-03,  4.89121722e-03, -2.80310176e-02,\n",
       "        5.52376173e-02, -5.92487194e-02,  6.14453927e-02,  3.54701141e-03,\n",
       "       -2.98315510e-02, -5.49717359e-02, -5.29682450e-02,  4.70336266e-02,\n",
       "        3.43414098e-02,  5.52391959e-03,  2.80624460e-02,  3.03138662e-02,\n",
       "       -1.43292733e-02, -3.52453962e-02, -2.86585912e-02, -6.23138398e-02,\n",
       "       -4.20150645e-02,  2.44777668e-02,  5.53563982e-03,  8.14049505e-03,\n",
       "        1.53732244e-02, -4.85228077e-02, -6.48295283e-02,  2.46882383e-02,\n",
       "        1.49867628e-02,  1.80066098e-02,  1.23578578e-01,  2.14027166e-02,\n",
       "       -1.67560428e-02, -4.69397977e-02,  5.99938957e-03,  8.19533411e-03,\n",
       "        9.56789255e-02,  2.58209594e-02, -1.20125134e-02, -5.72569715e-03,\n",
       "       -8.57408531e-03,  1.05052933e-01,  2.76331566e-02,  8.67517665e-03,\n",
       "       -6.76575303e-02, -2.67715380e-02, -4.06814069e-02, -1.03794850e-01,\n",
       "        7.66287893e-02,  1.26355812e-01, -8.59397203e-02,  1.20138340e-02,\n",
       "       -2.57108063e-02, -5.09863645e-02, -3.28317918e-02, -2.02578727e-33,\n",
       "        7.33251795e-02, -2.40866486e-02, -8.00556242e-02, -6.78968057e-02,\n",
       "       -5.16041443e-02, -7.83167183e-02, -1.33349467e-02, -2.67809909e-02,\n",
       "       -2.50325222e-02,  4.69421595e-02, -7.37413913e-02, -2.62129324e-04,\n",
       "        1.30887171e-02, -3.09572518e-02, -2.00150553e-02, -1.16042152e-01,\n",
       "        2.14620028e-03, -1.27644120e-02,  2.96524931e-02,  5.50505035e-02,\n",
       "        3.08322776e-02,  1.05993316e-01, -3.80328223e-02, -2.74119228e-02,\n",
       "        5.24591953e-02, -2.05130354e-02, -7.18794763e-02, -3.37742902e-02,\n",
       "       -1.51276672e-02,  4.96650226e-02, -4.12642807e-02, -4.23065946e-02,\n",
       "       -4.00017314e-02,  9.03002471e-02, -2.37208940e-02, -1.30580321e-01,\n",
       "        6.22314066e-02, -5.70305772e-02, -3.23460624e-02,  6.05499744e-02,\n",
       "       -6.04436081e-03,  1.40458988e-02,  3.24462727e-02,  2.66418327e-02,\n",
       "       -6.91086650e-02, -1.00011355e-03,  2.81587839e-02,  1.46810878e-02,\n",
       "       -1.64453013e-04,  2.58034449e-02, -2.66302042e-02,  1.57559905e-02,\n",
       "        5.38726561e-02, -5.33531122e-02, -5.54367937e-02,  9.00570676e-02,\n",
       "        7.70272985e-02, -2.44424567e-02, -3.47368196e-02,  9.82798636e-02,\n",
       "        3.04498225e-02, -2.00087689e-02,  4.53980733e-03, -4.74198498e-02,\n",
       "        1.42640695e-01, -6.86056241e-02, -8.13757703e-02,  1.05308800e-03,\n",
       "       -1.78350881e-02,  7.29835406e-02,  1.64816715e-02,  3.98400053e-02,\n",
       "        4.68362644e-02, -1.44534960e-01,  4.02424373e-02, -3.14182155e-02,\n",
       "        1.53545868e-02, -3.36852893e-02,  3.83163020e-02, -2.92718969e-02,\n",
       "        1.20120727e-01, -8.05278793e-02, -4.77892756e-02,  4.57337126e-02,\n",
       "       -2.07783915e-02,  6.10832982e-02,  7.39166327e-03,  1.99949238e-02,\n",
       "       -1.49614522e-02, -3.89547087e-02, -4.93791997e-02, -8.07103608e-03,\n",
       "        4.91255037e-02, -4.90615815e-02,  6.87639490e-02,  1.05193116e-33,\n",
       "        9.64529663e-02, -4.49997336e-02,  7.08592236e-02,  7.01550096e-02,\n",
       "       -3.07358764e-02,  5.32288365e-02, -7.13494280e-03,  4.52578254e-02,\n",
       "       -7.71549195e-02,  6.13044575e-02, -2.57572345e-02,  8.33091699e-03,\n",
       "       -1.65870227e-03, -4.16173687e-04,  1.13713190e-01, -2.59553781e-04,\n",
       "        6.54771924e-02, -6.39371714e-03,  2.79586669e-02,  1.51046263e-02,\n",
       "       -4.68892045e-02,  3.95999067e-02, -1.86009500e-02,  6.94512874e-02,\n",
       "        3.29815932e-02,  5.68617955e-02,  8.76662508e-02, -2.53197793e-02,\n",
       "       -4.36838344e-02, -1.03877187e-01, -5.24876602e-02, -5.71490116e-02,\n",
       "       -1.11060664e-02, -4.67860252e-02,  1.87631398e-02,  4.78794649e-02,\n",
       "       -4.17945981e-02, -6.59293821e-03, -2.18464565e-02, -8.24298114e-02,\n",
       "        3.08676343e-02, -1.24090922e-03,  2.34952662e-02,  7.13226423e-02,\n",
       "        2.72879377e-02,  3.08869872e-03, -5.66032231e-02,  4.98435088e-02,\n",
       "       -3.76918651e-02,  6.29746541e-02, -3.45266517e-03,  3.84236537e-02,\n",
       "        3.93796787e-02,  2.76155323e-02, -4.96771075e-02, -5.40543720e-02,\n",
       "        4.65717539e-03, -4.01742086e-02,  3.90536711e-02, -1.10563887e-02,\n",
       "        8.10949598e-03,  2.47772057e-02, -1.24726193e-02, -3.20834829e-03,\n",
       "       -6.75000669e-03, -8.95393342e-02, -7.46335313e-02, -5.39297946e-02,\n",
       "        7.71142319e-02, -7.48031959e-02, -5.91213070e-03,  3.00307330e-02,\n",
       "        9.53919627e-03, -7.08926022e-02,  9.31638945e-03,  7.84344673e-02,\n",
       "        1.10272020e-01,  4.93478356e-03,  7.26145059e-02, -3.91793624e-02,\n",
       "        1.15645919e-02, -1.69644225e-02, -1.54852110e-03,  1.13657378e-02,\n",
       "       -6.91898316e-02,  3.62798236e-02, -1.15797043e-01,  7.05056340e-02,\n",
       "        4.28795479e-02, -6.56523332e-02,  2.57525016e-02,  9.05412138e-02,\n",
       "        5.89157790e-02,  8.48691687e-02, -1.29126860e-02, -1.76108035e-08,\n",
       "       -5.10395542e-02,  1.34697203e-02, -9.77618247e-02,  4.43888083e-02,\n",
       "        8.00856501e-02,  2.05735713e-02, -3.20180431e-02,  1.20612327e-02,\n",
       "        8.37345198e-02, -3.04363370e-02,  3.55386697e-02,  2.50448193e-02,\n",
       "        5.86509667e-02,  4.10656035e-02, -2.28329021e-02,  1.78446341e-02,\n",
       "       -3.64020541e-02,  1.02113262e-02,  2.88050883e-02,  1.61286622e-01,\n",
       "       -4.23883880e-03, -5.56772612e-02, -1.09122684e-02, -2.70623937e-02,\n",
       "       -5.23658805e-02, -3.65737788e-02, -8.47721621e-02,  5.52399177e-03,\n",
       "       -3.13450210e-02,  1.30554419e-02, -5.08419797e-02,  9.68923792e-02,\n",
       "       -8.70660767e-02,  8.64324858e-04,  3.43630686e-02,  3.16394307e-02,\n",
       "        1.01861604e-01, -9.79756238e-04,  2.66416874e-02,  8.03417061e-03,\n",
       "        8.95350333e-03,  3.50238048e-02, -2.04781555e-02, -7.34798471e-03,\n",
       "       -7.61493742e-02, -6.33014878e-03, -3.11222579e-02, -1.02514505e-01,\n",
       "        7.49528930e-02, -5.15709221e-02, -4.73833345e-02, -4.23636027e-02,\n",
       "        4.27946597e-02,  6.56187385e-02, -4.99790646e-02,  1.02580467e-03,\n",
       "       -5.40309353e-03, -6.54073581e-02, -4.58586924e-02,  3.61347534e-02,\n",
       "        6.25733212e-02,  5.46831712e-02,  5.38233444e-02,  8.67674053e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an array with all the numbers that make up the embedding. As you recall, the embeddings generated by the chosen model have 384 dimensions, so this is the length of the embedding array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Embeddings in Elasticsearch\n",
    "Elasticsearch provides full support for storing and retrieving vectors, which makes it an ideal database for working with embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field Types\n",
    "In the Full-Text Search chapter of this tutorial you have learned how to create an index with several fields. At that time it was mentioned that Elasticsearch can, for the most part, automatically determine the best type to use for each field based on the data itself. Even though Elasticsearch 8.11 is able to automatically map some vector types, in this chapter you will define this type explicitly as an opportunity to learn more about type mappings in Elasticsearch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Type Mappings\n",
    "The types associated with each field in an index are determined in a process called mapping, which can be dynamic or explicit. The mappings that were created in the Full-Text Search portion of this tutorial were all dynamically generated by Elasticsearch.\n",
    "\n",
    "The Elasticsearch client offers a get_mapping method, which returns the type mappings that are in effect for a given index. If you want to explore these mappings on your own, start a Python shell and enter the following code:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "class Search:\n",
    "    def __init__(self):\n",
    "        self.es = Elasticsearch('http://localhost:9200')\n",
    "        client_info = self.es.info()\n",
    "        print('Connected to Elasticsearch!')\n",
    "        pprint(client_info.body)\n",
    "\n",
    "    def create_index(self):\n",
    "        self.es.indices.delete(index='my_documents', ignore_unavailable=True)\n",
    "        self.es.indices.create(index='my_documents', mappings={\n",
    "            'properties': {\n",
    "                'embedding': {\n",
    "                    'type': 'dense_vector',\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    def insert_documents(self, documents):\n",
    "        operations = []\n",
    "        for document in documents:\n",
    "            operations.append({'index': {'_index': 'my_documents'}})\n",
    "            operations.append(document)\n",
    "        return self.es.bulk(operations=operations)\n",
    "\n",
    "    def reindex(self):\n",
    "        self.create_index()\n",
    "        with open('data.json', 'rt') as f:\n",
    "            documents = json.loads(f.read())\n",
    "        return self.insert_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense vector field type\n",
    "\n",
    "정의:\n",
    "- dense_vector 필드 타입은 숫자 값의 밀집 벡터를 저장합니다.\n",
    "- 주로 k-최근접 이웃(k-nearest neighbor, kNN) 검색에 사용됩니다.\n",
    "\n",
    "\n",
    "제한사항:\n",
    "- 이 필드 타입은 집계(aggregations)나 정렬(sorting)을 지원하지 않습니다.\n",
    "\n",
    "\n",
    "구조:\n",
    "- 숫자 값의 배열로 저장됩니다.\n",
    "- 기본적으로 float 타입의 element_type을 사용합니다.\n",
    "\n",
    "매핑 예시:\n",
    "```json\n",
    "\"my_vector\": {\n",
    "  \"type\": \"dense_vector\",\n",
    "  \"dims\": 3\n",
    "}\n",
    "```\n",
    "\n",
    "- \"dims\": 3은 이 벡터가 3차원임을 나타냅니다.\n",
    "\n",
    "사용 예시:\n",
    "- 인덱스 생성 및 매핑 정의:\n",
    "\n",
    "```json\n",
    "PUT my-index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"my_vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 3\n",
    "      },\n",
    "      \"my_text\" : {\n",
    "        \"type\" : \"keyword\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "문서 추가:\n",
    "```json\n",
    "PUT my-index/_doc/1\n",
    "{\n",
    "  \"my_text\" : \"text1\",\n",
    "  \"my_vector\" : [0.5, 10, 6]\n",
    "}\n",
    "\n",
    "PUT my-index/_doc/2\n",
    "{\n",
    "  \"my_text\" : \"text2\",\n",
    "  \"my_vector\" : [-0.5, 10, 10]\n",
    "}\n",
    "```\n",
    "\n",
    "주의사항:\n",
    "- 벡터의 차원 수(dims)는 매핑 시 명시적으로 정의해야 하며, 문서마다 일관된 차원 수를 유지해야 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index vectors for kNN search\n",
    "\n",
    "kNN 검색 정의:\n",
    "- 쿼리 벡터와 가장 유사한 k개의 벡터를 찾는 검색 방법입니다.\n",
    "\n",
    "Dense vector 필드의 사용:\n",
    "- script_score 쿼리에서 문서 랭킹에 사용될 수 있습니다.\n",
    "- 모든 문서를 스캔하는 브루트 포스 kNN 검색이 가능합니다.\n",
    "\n",
    "효율적인 kNN 검색:\n",
    "- 브루트 포스 방식은 비효율적일 수 있어, 특수한 데이터 구조로 벡터를 인덱싱합니다.\n",
    "- search API의 knn 옵션을 통해 빠른 kNN 검색을 지원합니다.\n",
    "\n",
    "동적 매핑:\n",
    "- 128에서 4096 사이의 크기를 가진 float 배열은 자동으로 dense_vector로 매핑됩니다.\n",
    "- 기본 유사도 메트릭은 코사인 유사도입니다\n",
    "\n",
    "인덱싱 비용:\n",
    "- 벡터 인덱싱은 비용이 많이 드는 작업입니다.\n",
    "- 인덱싱이 활성화된 벡터 필드가 있는 문서의 색인 생성에 상당한 시간이 걸릴 수 있습니다.\n",
    "\n",
    "인덱싱 설정:\n",
    "- 기본적으로 int8_hnsw로 인덱싱됩니다.\n",
    "- 유사도 메트릭을 지정할 수 있습니다 (예: dot_product).\n",
    "\n",
    "```json \n",
    "PUT my-index-2\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"my_vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 3,\n",
    "        \"similarity\": \"dot_product\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "인덱싱 비활성화:\n",
    "- index 파라미터를 false로 설정하여 인덱싱을 비활성화할 수 있습니다.\n",
    "\n",
    "```json\n",
    "PUT my-index-2\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"my_vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 3,\n",
    "        \"index\": false\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "HNSW 알고리즘:\n",
    "- Elasticsearch는 효율적인 kNN 검색을 위해 HNSW(Hierarchical Navigable Small World) 알고리즘을 사용합니다.\n",
    "- 이는 근사 방법으로, 결과의 정확성을 일부 희생하여 검색 속도를 향상시킵니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically quantize vectors for kNN search\n",
    "\n",
    "자동 벡터 양자화의 목적:\n",
    "- float 벡터를 검색할 때 필요한 메모리 사용량을 줄이기 위해 사용됩니다.\n",
    "- 원래는 dense vector 에서 각 벡터당 float 이니까 4바이트 씀. 여기서는 int8_hnsw 를 써서 1바이트로 줄인거고\n",
    "\n",
    "\n",
    "\n",
    "지원되는 양자화 방법:\n",
    "- 현재는 int8 양자화만 지원됩니다.\n",
    "- 입력 벡터의 element_type은 반드시 float이어야 합니다.\n",
    "\n",
    "int8_hnsw 인덱스의 특징:\n",
    "- 각 float 벡터의 차원을 1바이트 정수로 양자화합니다.\n",
    "- 메모리 사용량을 최대 75%까지 줄일 수 있습니다.\n",
    "- 정확도가 약간 감소할 수 있습니다.\n",
    "- 디스크 사용량은 양자화된 벡터와 원본 벡터를 모두 저장하기 때문에 25% 정도 증가할 수 있습니다.\n",
    "\n",
    "```json\n",
    "PUT my-byte-quantized-index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"my_vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 3,\n",
    "        \"index\": true,\n",
    "        \"index_options\": {\n",
    "          \"type\": \"int8_hnsw\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Embeddings to Documents\n",
    "In the previous section you have learned how to generate embeddings using the SentenceTransformers framework and the all-MiniLM-L6-v2 model. Now it is time to integrate the model into the application.\n",
    "\n",
    "First of all, the model can be instantiated in the Search class constructor:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "class Search:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.es = Elasticsearch('http://localhost:9200')\n",
    "        client_info = self.es.info()\n",
    "        print('Connected to Elasticsearch!')\n",
    "        pprint(client_info.body)\n",
    "\n",
    "    def create_index(self):\n",
    "        self.es.indices.delete(index='my_documents', ignore_unavailable=True)\n",
    "        self.es.indices.create(index='my_documents', mappings={\n",
    "            'properties': {\n",
    "                'embedding': {\n",
    "                    'type': 'dense_vector',\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    def insert_documents(self, documents):\n",
    "        operations = []\n",
    "        for document in documents:\n",
    "            operations.append({'index': {'_index': 'my_documents'}})\n",
    "            operations.append(document)\n",
    "        return self.es.bulk(operations=operations)\n",
    "\n",
    "    def reindex(self):\n",
    "        self.create_index()\n",
    "        with open('data.json', 'rt') as f:\n",
    "            documents = json.loads(f.read())\n",
    "        return self.insert_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you recall from the full-text search portion of this tutorial, the Search class has insert_document() and insert_documents() methods, to insert single and multiple documents into the index respectively. These two methods now need to generate the corresponding embeddings that go with each document.\n",
    "\n",
    "The next code block shows new versions of these two methods, along with a new get_embedding() helper method that returns an embedding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "class Search:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.es = Elasticsearch('http://localhost:9200')\n",
    "        client_info = self.es.info()\n",
    "        print('Connected to Elasticsearch!')\n",
    "        pprint(client_info.body)\n",
    "\n",
    "    def create_index(self):\n",
    "        self.es.indices.delete(index='my_documents', ignore_unavailable=True)\n",
    "        self.es.indices.create(index='my_documents', mappings={\n",
    "            'properties': {\n",
    "                'embedding': {\n",
    "                    'type': 'dense_vector',\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    def insert_document(self, document):\n",
    "        return self.es.index(index='my_documents', document={\n",
    "            **document,\n",
    "            'embedding': self.get_embedding(document['summary']),\n",
    "        })\n",
    "    \n",
    "    def insert_documents(self, documents):\n",
    "        operations = []\n",
    "        for document in documents:\n",
    "            operations.append({'index': {'_index': 'my_documents'}})\n",
    "            operations.append({\n",
    "                **document,\n",
    "                'embedding': self.get_embedding(document['summary']),\n",
    "            })\n",
    "        return self.es.bulk(operations=operations)\n",
    "\n",
    "\n",
    "    def reindex(self):\n",
    "        self.create_index()\n",
    "        with open('data.json', 'rt') as f:\n",
    "            documents = json.loads(f.read())\n",
    "        return self.insert_documents(documents)\n",
    "    \n",
    "    def get_embedding(self, text):\n",
    "        return self.model.encode(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modified methods add the new embedding field to the document to be inserted. The embedding is generated from the summary field of each document. In general, embeddings are generated from sentences or short paragraphs, so in this case the summary is an ideal field to use. Other options would have been the name field, which contains the title of the document, or maybe the first few sentences from the document's body.\n",
    "\n",
    "With these changes in place the index can be rebuilt, so that it stores an embedding for each document. To rebuilt the index, use this command:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbor (kNN) Search\n",
    "\n",
    "The k-nearest neighbor (kNN) algorithm performs a similarity search on fields of dense_vector type. This type of search, which is more appropriately called \"approximate kNN\", accepts a vector or embedding as a search term, and finds entries in the index that are close.\n",
    "\n",
    "In this section you are going to learn how to run a kNN search using the document embeddings created in the previous section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The knn Query\n",
    "In the full-text search section of the tutorial you learned about the query option passed to the search() method of the Elasticsearch client. When searching vectors, the knn option is used instead.\n",
    "\n",
    "Below you can see a new version of the handle_search() function in app.py that runs a kNN search for the query entered by the user in the search form.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest neighbor (kNN) search\n",
    "\n",
    "kNN 검색 정의:\n",
    "- 쿼리 벡터와 가장 유사한 k개의 벡터를 찾는 검색 방법입니다.\n",
    "- 유사도 메트릭을 사용하여 측정합니다.\n",
    "\n",
    "\n",
    "kNN의 일반적인 사용 사례:\n",
    "- 자연어 처리(NLP) 알고리즘 기반의 관련성 랭킹\n",
    "- 제품 추천 및 추천 엔진\n",
    "- 이미지나 비디오의 유사도 검색\n",
    "\n",
    "\n",
    "전제 조건:\n",
    "- 데이터를 의미 있는 벡터 값으로 변환할 수 있어야 합니다.\n",
    "- 벡터는 Elasticsearch 내부의 NLP 모델을 사용하거나 외부에서 생성할 수 있습니다.\n",
    "- 문서에 dense_vector 필드 값으로 벡터를 추가합니다.\n",
    "- 쿼리도 동일한 차원의 벡터로 표현됩니다.\n",
    "\n",
    "\n",
    "벡터 설계 원칙:\n",
    "- 유사도 메트릭 기준으로 문서의 벡터가 쿼리 벡터에 가까울수록 더 좋은 매치가 되도록 설계해야 합니다.\n",
    "\n",
    "\n",
    "필요한 인덱스 권한:\n",
    "- create_index 또는 manage: dense_vector 필드를 가진 인덱스 생성\n",
    "- create, index, 또는 write: 생성한 인덱스에 데이터 추가\n",
    "- read: 인덱스 검색\n",
    "\n",
    "\n",
    "중요한 포인트:\n",
    "- kNN 검색은 벡터 공간에서의 유사도를 기반으로 합니다.\n",
    "- 효과적인 kNN 검색을 위해서는 데이터를 적절한 벡터 표현으로 변환하는 것이 중요합니다.\n",
    "- 벡터의 차원과 유사도 메트릭은 검색 성능과 정확도에 큰 영향을 미칩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벡터 유사도 메트릭(similarity metric)\n",
    "\n",
    "Dense Vector Field 에서 지정할 수 있는 유사도 검색 파라미터에 대해 알아보자. \n",
    "\n",
    "similarity 파라미터:\n",
    "- kNN 검색에서 사용할 벡터 유사도 메트릭을 지정합니다.\n",
    "- 옵션이지만, index가 true일 때만 지정할 수 있습니다.\n",
    "- 기본값은 cosine입니다\n",
    "\n",
    "l2_norm (L2 거리, 유클리드 거리):\n",
    "- 벡터 간의 절대적인 거리가 중요할 때\n",
    "- 데이터 포인트의 실제 위치나 크기가 의미가 있을 때 \n",
    "- 고차원 데이터에는 유용하지 않음. \n",
    "- 이상치 탐지나 클러스터링 작업에서 유용 (데이터 포인트간의 거리 차이로 이상치를 탐색함.)\n",
    "\n",
    "dot_product: \n",
    "- 모든 벡터가 이미 정규화되어 있을 때 (cosine 유사도의 최적화 버전)\n",
    "- 대규모 데이터셋에서는 계산 효율성이 중요할 수 있으며, 이 경우 dot_product가 유리할 수 있습니다.\n",
    "\n",
    "dot_product (내적): 사용을 피해야 할 경우:\n",
    "- 벡터가 정규화되어 있지 않을 때 (길이가 1이 아닌 경우)\n",
    "- 벡터의 크기 차이가 결과에 영향을 주면 안 될 때\n",
    "- 음수 값을 포함한 데이터에서 유사도의 해석이 중요할 때\n",
    "- 데이터의 스케일이 일정하지 않은 경우\n",
    "- 예: 대규모 텍스트 코퍼스에서의 빠른 유사도 검색, 임베딩 벡터를 사용한 단어 유사성 비교\n",
    "\n",
    "\n",
    "cosine (코사인 유사도):\n",
    "- 벡터의 방향이 중요하고 크기는 덜 중요할 때\n",
    "- 텍스트 문서의 유사성을 비교할 때\n",
    "- 고차원 데이터에서 작동이 잘 될 때\n",
    "- 예시: 문서 유사도 검색, 추천 시스템 자연어 처리 작업\n",
    "\n",
    "cosine (코사인 유사도): 사용을 피해야 할 경우:\n",
    "- 벡터의 크기(magnitude)가 중요한 의미를 가질 때\n",
    "- 음수 값을 포함한 데이터의 유사도를 비교할 때 (코사인 유사도는 양수 공간에서만 의미가 있음)\n",
    "- 벡터의 절대적 위치가 중요한 경우 (예: 지리적 위치 데이터)\n",
    "- 영벡터(모든 요소가 0인 벡터)를 포함한 데이터셋\n",
    "\n",
    "max_inner_product:\n",
    "- 벡터의 방향과 크기가 중요한 의미를 가질 때\n",
    "- 정규화되지 않은 벡터를 다룰 때\n",
    "- 양수와 음수 값을 모두 고려해야 할 때 \n",
    "- 예: 추천 시스템에서 사용자-아이템 상호작용 강도를 고려할 때, 특성의 중요도가 서로 다른 기계학습 모델의 결과를 비교할 때\n",
    "\n",
    "\n",
    "데이터의 특성: 데이터가 방향만 중요한지, 절대적 크기도 중요한지 고려합니다.\n",
    "정규화 여부: 데이터가 이미 정규화되어 있다면 dot_product나 cosine이 효율적일 수 있습니다.\n",
    "계산 효율성: 대규모 데이터셋에서는 계산 효율성이 중요할 수 있으며, 이 경우 dot_product가 유리할 수 있습니다.\n",
    "벡터 값 범위: 음수 지원하는지 \n",
    "\n",
    "dot_product vs cosine \n",
    "- 둘 다 벡터의 방향이 더 중요\n",
    "- cosine 은 -1 ~ 1 까지 지원, dot_product 는 양수만 지원. 그래서 cosine 이 더 넓은 유사도 범위를 할 수 있음. \n",
    "- dot_product 는 cosine 보다 계산이 간단해서 대규모 데이터에서 더 빠름. \n",
    "- dot_product 는 정규화 벡터에서만 동작. cosine 은 알아서 정규화해줌. \n",
    "\n",
    "\n",
    "max_inner_product vs l2_norm \n",
    "- max_inner_product 는 벡터의 방향과 크기도 고려하는 반면에, l2_norm 은 순수 거리만을 측정한다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Methods \n",
    "\n",
    "Elasticsearch의 두 가지 kNN 검색 방법: \n",
    "- Approximate kNN: \n",
    "  - 낮은 지연 시간을 제공하지만, 인덱싱이 느리고 완벽한 정확도를 보장하지 않습니다.\n",
    "\n",
    "- brute-force kNN:  \n",
    "  - 정확한 결과를 보장하지만, 대규모 데이터셋에서는 확장성이 떨어집니다.\n",
    "\n",
    "사용 시 고려사항:\n",
    "- 데이터셋의 크기\n",
    "- 요구되는 정확도 수준\n",
    "- 허용 가능한 검색 지연 시간\n",
    "- 인덱싱 속도의 중요성\n",
    "\n",
    "브루트 포스 kNN의 활용:\n",
    "- 데이터를 작은 부분집합으로 필터링할 수 있는 경우, 이 방법으로도 좋은 검색 성능을 얻을 수 있습니다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate kNN\n",
    "\n",
    "리소스 요구사항: \n",
    "- 근사 kNN 검색은 특별한 리소스 요구사항이 있습니다.\n",
    "- 모든 벡터 데이터가 노드의 페이지 캐시에 맞아야 효율적입니다.\n",
    "- 구성 및 크기 조정에 대한 튜닝 가이드를 참조하는 것이 중요합니다.\n",
    "\n",
    "kNN 쿼리 실행: \n",
    "- 'knn' 옵션을 사용하여 검색을 실행합니다.\n",
    "- 검색 파라미터로는 field, query_vector, k, num_candidates 등이 있습니다.\n",
    "- 문서의 _score는 쿼리 벡터와 문서 벡터 간의 유사도에 의해 결정됩니다.\n",
    "\n",
    "```json\n",
    "POST image-index/_search\n",
    "{\n",
    "  \"knn\": {\n",
    "    \"field\": \"image-vector\",\n",
    "    \"query_vector\": [-5, 9, -12],\n",
    "    \"k\": 10,\n",
    "    \"num_candidates\": 100\n",
    "  },\n",
    "  \"fields\": [ \"title\", \"file-type\" ]\n",
    "}\n",
    "\n",
    "``` \n",
    "\n",
    "매핑 요구사항:\n",
    "- dense_vector 필드를 명시적으로 매핑해야 합니다.\n",
    "- similarity 값을 지정해야 합니다 (기본값은 cosine).\n",
    "- 예시에서는 'l2_norm' 유사도 메트릭을 사용합니다.\n",
    "\n",
    "```json\n",
    "PUT image-index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"image-vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 3,\n",
    "        \"similarity\": \"l2_norm\"\n",
    "      },\n",
    "      \"title-vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 5,\n",
    "        \"similarity\": \"l2_norm\"\n",
    "      },\n",
    "      \"title\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"file-type\": {\n",
    "        \"type\": \"keyword\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "버전 호환성:\n",
    "- 근사 kNN 검색 지원은 Elasticsearch 8.0 버전에서 추가되었습니다.\n",
    "- 8.0 이전 버전에서 생성된 인덱스의 경우, 근사 kNN 검색을 지원하려면 index: true 옵션으로 데이터를 재인덱싱해야 합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune approximate kNN for speed or accuracy\n",
    "\n",
    "kNN 검색 과정:\n",
    "- 각 샤드에서 'num_candidates' 수만큼의 근사 최근접 이웃 후보를 찾습니다.\n",
    "- 이 후보 벡터들과 쿼리 벡터 간의 유사도를 계산합니다.\n",
    "- 각 샤드에서 가장 유사한 k개의 결과를 선택합니다.\n",
    "- 모든 샤드의 결과를 병합하여 전체 상위 k개의 최근접 이웃을 반환합니다.\n",
    "\n",
    "num_candidates 파라미터의 역할:\n",
    "- 이 파라미터는 정확도와 검색 속도 사이의 균형을 조절합니다.\n",
    "\n",
    "\n",
    "num_candidates 값 증가:\n",
    "- 장점: 더 정확한 결과를 얻을 수 있습니다.\n",
    "- 단점: 검색 속도가 느려집니다.\n",
    "- 이유: 각 샤드에서 더 많은 후보를 고려하므로, 진정한 상위 k개 최근접 이웃을 찾을 확률이 높아집니다.\n",
    "\n",
    "\n",
    "num_candidates 값 감소:\n",
    "- 장점: 검색 속도가 빨라집니다.\n",
    "- 단점: 결과의 정확도가 떨어질 수 있습니다.\n",
    "- 이유: 각 샤드에서 고려하는 후보 수가 줄어들어 처리 시간이 단축됩니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate kNN using byte vectors\n",
    "\n",
    "바이트 벡터 지원:\n",
    "- 근사 kNN 검색 API는 float 값 벡터 외에도 바이트 값 벡터를 지원합니다.\n",
    "- dense_vector 필드에 element_type을 'byte'로 설정하고 인덱싱을 활성화해야 합니다.\n",
    "\n",
    "```json \n",
    "PUT byte-image-index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"byte-image-vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"element_type\": \"byte\",\n",
    "        \"dims\": 2\n",
    "      },\n",
    "      \"title\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "데이터 인덱싱:\n",
    "- 모든 벡터 값은 -128에서 127 사이의 정수여야 합니다.\n",
    "\n",
    "주요 포인트:\n",
    "- 바이트 벡터는 float 벡터에 비해 메모리 사용량을 줄일 수 있습니다.\n",
    "- 값의 범위가 제한되어 있으므로(-128에서 127), 이 범위 내에서 데이터를 적절히 정규화해야 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte quantized kNN search \n",
    "\n",
    "바이트 양자화의 목적:\n",
    "- float 벡터를 제공하면서도 바이트 벡터의 메모리 절약 효과를 얻을 수 있습니다.\n",
    "- float 벡터를 내부적으로 바이트 벡터로 인덱싱하지만, 원본 float 벡터도 인덱스에 유지합니다.\n",
    "\n",
    "기본 인덱스 타입:\n",
    "- dense_vector의 기본 인덱스 타입은 'int8_hnsw'입니다.\n",
    "\n",
    "인덱스 매핑:\n",
    "- dense_vector 필드에 'int8_hnsw' 인덱스 타입을 지정합니다.\n",
    "- element_type은 'float'으로 설정합니다.\n",
    "\n",
    "```json \n",
    "PUT quantized-image-index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"image-vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"element_type\": \"float\",\n",
    "        \"dims\": 2,\n",
    "        \"index\": true,\n",
    "        \"index_options\": {\n",
    "          \"type\": \"int8_hnsw\"\n",
    "        }\n",
    "      },\n",
    "      \"title\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "'knn' 옵션을 사용하여 검색을 실행합니다.\n",
    "- 검색 시 float 벡터가 자동으로 바이트 벡터로 양자화됩니다.\n",
    "\n",
    "\n",
    "```json\n",
    "POST quantized-image-index/_search\n",
    "{\n",
    "  \"knn\": {\n",
    "    \"field\": \"image-vector\",\n",
    "    \"query_vector\": [0.1, -2],\n",
    "    \"k\": 10,\n",
    "    \"num_candidates\": 100\n",
    "  },\n",
    "  \"fields\": [ \"title\" ]\n",
    "}\n",
    "```\n",
    "\n",
    "재점수화(Rescoring):\n",
    "- 원본 float 벡터를 사용하여 상위 결과의 점수를 재계산할 수 있습니다.\n",
    "- 상위 k개 결과에 대해서만 원본 float 벡터를 사용하여 재점수화합니다.\n",
    "- 이를 통해 빠른 검색과 정확한 점수 계산의 장점을 모두 얻을 수 있습니다.\n",
    "\n",
    "\n",
    "```json\n",
    "POST quantized-image-index/_search\n",
    "{\n",
    "  \"knn\": {\n",
    "    \"field\": \"image-vector\",\n",
    "    \"query_vector\": [0.1, -2],\n",
    "    \"k\": 15,\n",
    "    \"num_candidates\": 100\n",
    "  },\n",
    "  \"fields\": [ \"title\" ],\n",
    "  \"rescore\": {\n",
    "    \"window_size\": 10,\n",
    "    \"query\": {\n",
    "      \"rescore_query\": {\n",
    "        \"script_score\": {\n",
    "          \"query\": {\n",
    "            \"match_all\": {}\n",
    "          },\n",
    "          \"script\": {\n",
    "            \"source\": \"cosineSimilarity(params.query_vector, 'image-vector') + 1.0\",\n",
    "            \"params\": {\n",
    "              \"query_vector\": [0.1, -2]\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered kNN search\n",
    "\n",
    "필터링된 kNN 검색:\n",
    "- kNN 검색 API는 필터를 사용하여 검색 범위를 제한할 수 있습니다.\n",
    "- 검색은 필터 쿼리와 일치하는 문서 중에서 상위 k개의 문서를 반환합니다.\n",
    "\n",
    "검색 요청 예시:\n",
    "- 'image-vector' 필드에 대해 근사 kNN 검색을 수행합니다.\n",
    "- 'file-type' 필드를 기준으로 필터링합니다 (이 예에서는 'png' 파일만 검색).\n",
    "\n",
    "```json\n",
    "POST image-index/_search\n",
    "{\n",
    "  \"knn\": {\n",
    "    \"field\": \"image-vector\",\n",
    "    \"query_vector\": [54, 10, -2],\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 50,\n",
    "    \"filter\": {\n",
    "      \"term\": {\n",
    "        \"file-type\": \"png\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"fields\": [\"title\"],\n",
    "  \"_source\": false\n",
    "}\n",
    "```\n",
    "\n",
    "필터 적용 방식:\n",
    "- 필터는 근사 kNN 검색 중에 적용됩니다.\n",
    "- 이는 k개의 일치하는 문서를 확실히 반환하기 위함입니다.\n",
    "\n",
    "후처리 필터링과의 차이:\n",
    "- 후처리 필터링은 kNN 검색 완료 후 필터를 적용합니다.\n",
    "- 후처리 필터링의 단점: 충분한 일치 문서가 있어도 k개 미만의 결과를 반환할 수 있습니다.\n",
    "\n",
    "주의사항:\n",
    "- 필터가 너무 제한적이면 k개의 결과를 찾지 못할 수 있습니다.\n",
    "- num_candidates 값을 적절히 설정하여 검색의 정확성과 속도를 조절해야 합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate kNN search and filtering\n",
    "\n",
    "일반적인 쿼리와 필터링 쿼리와의 차이:\n",
    "- ES 에서는 일반적인 쿼리에서는 더 제한적인 필터가 보통 더 빠른 쿼리 실행을 의미합니다.\n",
    "- 그러나 HNSW 인덱스를 사용한 근사 kNN 검색에서는 필터 적용이 오히려 성능을 저하시킬 수 있습니다.\n",
    "\n",
    "\n",
    "성능 저하의 이유:\n",
    "- HNSW 그래프 검색 시 필터 기준을 만족하는 num_candidates를 얻기 위해 추가적인 탐색이 필요합니다.\n",
    "- 추가적인 오버헤드가 있는거지. 일반적인 필터는 탐색 범위를 줄이고 가는 반면에 여기서는 HNSW 탐색 하면서 필터를 적용하다보니까. \n",
    "\n",
    "\n",
    "\n",
    "Lucene의 성능 최적화 전략:\n",
    "- Lucene은 세그먼트 별로 다음 전략을 구현하여 성능 저하를 방지합니다:\n",
    "  - 필터링된 문서 수가 num_candidates 이하인 경우:\n",
    "    - HNSW 그래프 검색을 우회해서, 필터링된 문서에 대해 브루트 포스 검색을 수행합니다.\n",
    "  - HNSW 그래프 탐색 중 특정 조건 만족 시: \n",
    "    - 탐색된 노드 수가 필터를 만족하는 문서 수를 초과하면, 그래프 탐색을 중단하고 필터링된 문서에 대해 브루트 포스 검색으로 전환합니다.\n",
    "\n",
    "필터링 + HNSW 그래프 검색 매커니즘: \n",
    "- 필터링으로 통과한 문서의 집합을 구함. \n",
    "- num_candidated 수만큼을 구하기 위해서 HNSW 인덱스를 타서 검색을 함. 여기서 마지막 노드가 필터링을 통과하지 못한다면 추가 탐색이 계속적으로 발생할 수 있음. 그래서 필터링이 성능 저하를 일으킴. \n",
    "\n",
    "\n",
    "Lucene 성능 최적화 해석: \n",
    "- num_candidated 만큼 최소 HNSW 탐색이 이뤄지기 때문에, 필터링 된 문서가 num_candidated 보다 이하라면 브루트 포스 접근을 하는것. \n",
    "- 필터링된 문서의 수만큼 브루트 포스를 하게 되면 최대 시간 복잡도는 계산할 수 있음. 근데 num_candidate 가 필터링 문서보다 작다면 HNSW 탐색으로 더 빠르게 찾아낼 수도 있는 가능성이 있음. 근데 이게 필터링된 문서 수보다 많이 탐색을 해야한다면 그냥 브루트 포스 접근이 나은 것이었으니 이 방식으로 하는 것. \n",
    "\n",
    "사용 시 고려사항:\n",
    "- num_candidates 값을 적절히 설정하는 것이 중요합니다.\n",
    "- 필터의 선택성을 고려하여 쿼리를 설계해야 합니다.\n",
    "- 대규모 데이터셋에서는 필터링과 kNN 검색의 균형을 잘 맞추어야 합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine approximate kNN with other features\n",
    "\n",
    "하이브리드 검색:\n",
    "- kNN 옵션과 일반 쿼리를 함께 사용하여 하이브리드 검색을 수행할 수 있습니다.\n",
    "\n",
    "예시 쿼리 \n",
    "- 텍스트 매치 쿼리(\"mountain lake\")와 벡터 kNN 검색을 결합합니다.\n",
    "- 각 부분에 boost 값을 적용하여 가중치를 조절합니다.\n",
    "\n",
    "```json \n",
    "POST image-index/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"title\": {\n",
    "        \"query\": \"mountain lake\",\n",
    "        \"boost\": 0.9\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"knn\": {\n",
    "    \"field\": \"image-vector\",\n",
    "    \"query_vector\": [54, 10, -2],\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 50,\n",
    "    \"boost\": 0.1\n",
    "  },\n",
    "  \"size\": 10\n",
    "}\n",
    "```\n",
    "\n",
    "결과 결합 방식:\n",
    "- kNN 결과와 쿼리 결과는 disjunction(OR) 방식으로 결합됩니다.\n",
    "\n",
    "점수 계산:\n",
    "- 각 히트의 점수는 kNN 점수와 쿼리 점수의 가중 합으로 계산됩니다.\n",
    "- 예: score = 0.9 * match_score + 0.1 * knn_score\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform semantic search\n",
    "\n",
    "의미론적 검색의 개념:\n",
    "- 검색어의 문자 그대로의 일치가 아닌, 검색 쿼리의 의도와 문맥적 의미에 기반하여 결과를 검색합니다.\n",
    "\n",
    "\n",
    "작동 원리:\n",
    "- 사전에 배포된 텍스트 임베딩 모델을 사용합니다.\n",
    "- 입력 쿼리 문자열을 밀집 벡터(dense vector)로 변환합니다.\n",
    "- 이 벡터를 동일한 모델로 생성된 밀집 벡터가 저장된 인덱스에 대해 검색합니다.\n",
    "\n",
    "\n",
    "의미론적 검색 수행을 위한 요구사항:\n",
    "- 검색 대상 데이터의 밀집 벡터 표현이 포함된 인덱스가 필요합니다.\n",
    "- 검색에 사용하는 텍스트 임베딩 모델은 입력 데이터의 벡터 생성에 사용한 모델과 동일해야 합니다.\n",
    "- 텍스트 임베딩 NLP 모델 배포가 시작되어 있어야 합니다.\n",
    "\n",
    "쿼리 구조:\n",
    "- query_vector_builder 객체를 사용하여 배포된 텍스트 임베딩 모델을 참조합니다.\n",
    "- model_text 파라미터에 검색 쿼리를 제공합니다.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"knn\": {\n",
    "    \"field\": \"dense-vector-field\",\n",
    "    \"k\": 10,\n",
    "    \"num_candidates\": 100,\n",
    "    \"query_vector_builder\": {\n",
    "      \"text_embedding\": { \n",
    "        \"model_id\": \"my-text-embedding-model\", \n",
    "        \"model_text\": \"The opposite of blue\" \n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "추가 정보:\n",
    "- 훈련된 모델을 배포하고 텍스트 임베딩을 생성하는 방법에 대한 자세한 정보는 별도의 예제를 참조\n",
    "- https://www.elastic.co/guide/en/machine-learning/8.14/ml-nlp-text-emb-vector-search-example.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search multiple kNN fields\n",
    "\n",
    "여러 kNN(k-Nearest Neighbors) 벡터 필드를 동시에 검색하는 방법 \n",
    "\n",
    "다중 kNN 필드 검색:\n",
    "- 하나 이상의 kNN 벡터 필드를 동시에 검색할 수 있습니다.\n",
    "- 이는 하이브리드 검색의 확장된 형태입니다.\n",
    "\n",
    "예시 쿼리 구조:\n",
    "- 텍스트 매치 쿼리(\"mountain lake\")\n",
    "- 두 개의 kNN 검색 (image-vector와 title-vector)\n",
    "- 각 부분에 대한 boost 값 지정\n",
    "\n",
    "\n",
    "결과 결합 방식:\n",
    "- 여러 kNN 엔트리와 쿼리 매치는 disjunction(OR) 방식으로 결합됩니다.\n",
    "- 각 벡터 필드의 상위 k 결과는 모든 인덱스 샤드에 걸친 전역 최근접 이웃을 나타냅니다.\n",
    "\n",
    "\n",
    "점수 계산:\n",
    "- 각 문서의 점수는 텍스트 매치 점수와 각 kNN 검색의 점수를 가중 합산하여 계산됩니다.\n",
    "- 예시: score = 0.9 * match_score + 0.1 * knn_score_image-vector + 0.5 * knn_score_title-vector\n",
    "\n",
    "\n",
    "```json \n",
    "POST image-index/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"title\": {\n",
    "        \"query\": \"mountain lake\",\n",
    "        \"boost\": 0.9\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"knn\": [ {\n",
    "    \"field\": \"image-vector\",\n",
    "    \"query_vector\": [54, 10, -2],\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 50,\n",
    "    \"boost\": 0.1\n",
    "  },\n",
    "  {\n",
    "    \"field\": \"title-vector\",\n",
    "    \"query_vector\": [1, 20, -52, 23, 10],\n",
    "    \"k\": 10,\n",
    "    \"num_candidates\": 10,\n",
    "    \"boost\": 0.5\n",
    "  }],\n",
    "  \"size\": 10\n",
    "}\n",
    "```\n",
    "\n",
    "주요 이점:\n",
    "- 다양한 유형의 벡터 데이터를 동시에 고려할 수 있습니다.\n",
    "- 텍스트 기반 검색과 여러 벡터 기반 검색을 결합하여 더 풍부한 검색 결과를 제공합니다.\n",
    "- 각 검색 구성요소에 대한 가중치(boost)를 조정하여 검색 결과의 우선순위를 세밀하게 제어할 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search kNN with expected similarity\n",
    "\n",
    "Elasticsearch의 kNN(k-Nearest Neighbors) 검색에서 기대 유사도(expected similarity)를 사용하는 방법에 대해 설명\n",
    "\n",
    "kNN의 한계점:\n",
    "- kNN은 항상 k개의 최근접 이웃을 반환하려고 합니다.\n",
    "- 필터와 함께 사용할 때, 관련 없는 문서만 남을 수 있습니다.\n",
    "\n",
    "\n",
    "similarity 파라미터:\n",
    "- kNN 절에서 사용 가능한 새로운 파라미터입니다.\n",
    "- 벡터가 매치로 간주되기 위한 최소 유사도를 지정합니다.\n",
    "\n",
    "similarity를 사용한 kNN 검색 흐름:\n",
    "- 사용자가 제공한 필터 쿼리 적용\n",
    "- 벡터 공간에서 k개의 벡터 탐색\n",
    "- 구성된 similarity보다 멀리 있는 벡터는 반환하지 않음\n",
    "\n",
    "similarity와 _score의 관계:\n",
    "- similarity는 _score로 변환되기 전의 실제 유사도입니다.\n",
    "- 각 유사도 메트릭에 대한 _score 변환 공식이 제공됩니다.\n",
    "  - l2_norm: sqrt((1 / _score) - 1)\n",
    "  - cosine: (2 * _score) - 1\n",
    "  - dot_product: (2 * _score) - 1\n",
    "  - max_inner_product:\n",
    "    - _score < 1: 1 - (1 / _score)\n",
    "    - _score >= 1: _score - 1\n",
    "\n",
    "```json\n",
    "\n",
    "POST image-index/_search\n",
    "{\n",
    "  \"knn\": {\n",
    "    \"field\": \"image-vector\",\n",
    "    \"query_vector\": [1, 5, -20],\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 50,\n",
    "    \"similarity\": 36,\n",
    "    \"filter\": {\n",
    "      \"term\": {\n",
    "        \"file-type\": \"png\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"fields\": [\"title\"],\n",
    "  \"_source\": false\n",
    "}\n",
    "```\n",
    "\n",
    "주요 이점:\n",
    "- 관련성 없는 결과를 효과적으로 필터링할 수 있습니다.\n",
    "- 유사도에 기반한 더 정확한 검색 결과를 얻을 수 있습니다.\n",
    "\n",
    "\n",
    "사용 시 고려사항:\n",
    "- similarity 값을 적절히 설정하는 것이 중요합니다. (유사도 메트릭도 추가로 고려해야함.)\n",
    "- 데이터의 특성과 벡터 공간의 분포를 이해해야 합니다.\n",
    "- 필터와 similarity를 함께 사용할 때 검색 결과가 없을 수 있음을 인지해야 합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested kNN Search\n",
    "\n",
    "텍스트 청킹(Chunking)의 필요성:\n",
    "- 텍스트가 특정 모델의 토큰 제한을 초과할 때 청킹이 필요합니다.\n",
    "- 개별 청크에 대한 임베딩을 생성해야 합니다.\n",
    "\n",
    "기존에 텍스트 청킹 방법: \n",
    "- 청킹 텍스트에다가 메타 데이터로 상위 문서를 포함시켰음. 그래서 저장 비용이 두 배로 든다. \n",
    "\n",
    "ES 에서는 Nested 와 Dense Vector 의 결합으로 데이터를 저장할 수 있음. \n",
    "- 문서 내용 전체를 인덱싱하고, 청킹 부분 (Nested, paragraph) 부분만 각각 저장하면 됨. \n",
    "\n",
    "다음과 같이 매핑을 만들어야한다. \n",
    "- 'passage_vectors' 인덱스를 생성합니다.\n",
    "- 'full_text', 'creation_time'과 같은 상위 레벨 필드를 포함합니다.\n",
    "- 'paragraph'라는 nested 타입 필드를 정의합니다.\n",
    "- 'vector': dense_vector 타입, HNSW 인덱스 옵션 사용\n",
    "- 'text': 텍스트 필드, 인덱싱되지 않음\n",
    "\n",
    "\n",
    "```json\n",
    "PUT passage_vectors\n",
    "{\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"full_text\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"creation_time\": {\n",
    "                \"type\": \"date\"\n",
    "            },\n",
    "            \"paragraph\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"vector\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": 2,\n",
    "                        \"index_options\": {\n",
    "                            \"type\": \"hnsw\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"text\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"index\": false\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "``` \n",
    "\n",
    "색인도 다음 방법처럼 벌크로 하면 됨. \n",
    "\n",
    "```json\n",
    "POST passage_vectors/_bulk?refresh=true\n",
    "{ \"index\": { \"_id\": \"1\" } }\n",
    "{ \"full_text\": \"first paragraph another paragraph\", \"creation_time\": \"2019-05-04\", \"paragraph\": [ { \"vector\": [ 0.45, 45 ], \"text\": \"first paragraph\", \"paragraph_id\": \"1\" }, { \"vector\": [ 0.8, 0.6 ], \"text\": \"another paragraph\", \"paragraph_id\": \"2\" } ] }\n",
    "{ \"index\": { \"_id\": \"2\" } }\n",
    "{ \"full_text\": \"number one paragraph number two paragraph\", \"creation_time\": \"2020-05-04\", \"paragraph\": [ { \"vector\": [ 1.2, 4.5 ], \"text\": \"number one paragraph\", \"paragraph_id\": \"1\" }, { \"vector\": [ -1, 42 ], \"text\": \"number two paragraph\", \"paragraph_id\": \"2\" } ] }\n",
    "```\n",
    "\n",
    "\n",
    "검색 방법: \n",
    "\n",
    "```json\n",
    "POST passage_vectors/_search\n",
    "{\n",
    "    \"fields\": [\"full_text\", \"creation_time\"],\n",
    "    \"_source\": false,\n",
    "    \"knn\": {\n",
    "        \"query_vector\": [\n",
    "            0.45,\n",
    "            45\n",
    "        ],\n",
    "        \"field\": \"paragraph.vector\",\n",
    "        \"k\": 2,\n",
    "        \"num_candidates\": 2\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "검색 결과의 특징:\n",
    "- 총 4개의 벡터가 있지만, 2개의 문서만 반환됩니다.\n",
    "- Nested dense_vectors에 대한 kNN 검색은 항상 상위 레벨 문서에 대해 결과를 다양화합니다.\n",
    "- 'k'개의 상위 레벨 문서가 반환되며, 각 문서는 가장 가까운 패시지 벡터에 의해 점수가 매겨집니다.\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"took\": 4,\n",
    "    \"timed_out\": false,\n",
    "    \"_shards\": {\n",
    "        \"total\": 1,\n",
    "        \"successful\": 1,\n",
    "        \"skipped\": 0,\n",
    "        \"failed\": 0\n",
    "    },\n",
    "    \"hits\": {\n",
    "        \"total\": {\n",
    "            \"value\": 2,\n",
    "            \"relation\": \"eq\"\n",
    "        },\n",
    "        \"max_score\": 1.0,\n",
    "        \"hits\": [\n",
    "            {\n",
    "                \"_index\": \"passage_vectors\",\n",
    "                \"_id\": \"1\",\n",
    "                \"_score\": 1.0,\n",
    "                \"fields\": {\n",
    "                    \"creation_time\": [\n",
    "                        \"2019-05-04T00:00:00.000Z\"\n",
    "                    ],\n",
    "                    \"full_text\": [\n",
    "                        \"first paragraph another paragraph\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"_index\": \"passage_vectors\",\n",
    "                \"_id\": \"2\",\n",
    "                \"_score\": 0.9997144,\n",
    "                \"fields\": {\n",
    "                    \"creation_time\": [\n",
    "                        \"2020-05-04T00:00:00.000Z\"\n",
    "                    ],\n",
    "                    \"full_text\": [\n",
    "                        \"number one paragraph number two paragraph\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested kNN Search with Inner hits\n",
    "\n",
    "Nested Search 에서 Inner Hits 를 이용하는 법을 다룸. \n",
    "\n",
    "Inner Hits: \n",
    "- 매치된 문서에서 가장 가까운 패시지(passage)를 추출하기 위해 사용됩니다.\n",
    "\n",
    "Inner Hits 사용 방법:\n",
    "- kNN 절에 inner_hits를 추가하여 사용합니다.\n",
    "\n",
    "장점:\n",
    "- 문서 전체가 아닌 가장 관련 있는 패시지를 정확히 식별할 수 있습니다.\n",
    "- 긴 문서에서 특정 부분을 효과적으로 검색하고 추출할 수 있습니다.\n",
    "- 상위 레벨 문서 컨텍스트와 함께 세부적인 매치 정보를 얻을 수 있습니다.\n",
    "\n",
    "\n",
    "검색 예시 inner_hits:\n",
    "- _source: false로 설정하여 nested 객체의 원본을 반환하지 않습니다.\n",
    "- fields: 반환할 nested 필드를 지정합니다 (\"paragraph.text\").\n",
    "- size: 각 문서당 반환할 inner hit의 수를 지정합니다 (여기서는 1).\n",
    "\n",
    "\n",
    "```json\n",
    "POST passage_vectors/_search\n",
    "{\n",
    "    \"fields\": [\n",
    "        \"creation_time\",\n",
    "        \"full_text\"\n",
    "    ],\n",
    "    \"_source\": false,\n",
    "    \"knn\": {\n",
    "        \"query_vector\": [\n",
    "            0.45,\n",
    "            45\n",
    "        ],\n",
    "        \"field\": \"paragraph.vector\",\n",
    "        \"k\": 2,\n",
    "        \"num_candidates\": 2,\n",
    "        \"inner_hits\": {\n",
    "            \"_source\": false,\n",
    "            \"fields\": [\n",
    "                \"paragraph.text\"\n",
    "            ],\n",
    "            \"size\": 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Inner hits 검색 결과: \n",
    "```json \n",
    "{\n",
    "    \"took\": 4,\n",
    "    \"timed_out\": false,\n",
    "    \"_shards\": {\n",
    "        \"total\": 1,\n",
    "        \"successful\": 1,\n",
    "        \"skipped\": 0,\n",
    "        \"failed\": 0\n",
    "    },\n",
    "    \"hits\": {\n",
    "        \"total\": {\n",
    "            \"value\": 2,\n",
    "            \"relation\": \"eq\"\n",
    "        },\n",
    "        \"max_score\": 1.0,\n",
    "        \"hits\": [\n",
    "            {\n",
    "                \"_index\": \"passage_vectors\",\n",
    "                \"_id\": \"1\",\n",
    "                \"_score\": 1.0,\n",
    "                \"fields\": {\n",
    "                    \"creation_time\": [\n",
    "                        \"2019-05-04T00:00:00.000Z\"\n",
    "                    ],\n",
    "                    \"full_text\": [\n",
    "                        \"first paragraph another paragraph\"\n",
    "                    ]\n",
    "                },\n",
    "                \"inner_hits\": {\n",
    "                    \"paragraph\": {\n",
    "                        \"hits\": {\n",
    "                            \"total\": {\n",
    "                                \"value\": 2,\n",
    "                                \"relation\": \"eq\"\n",
    "                            },\n",
    "                            \"max_score\": 1.0,\n",
    "                            \"hits\": [\n",
    "                                {\n",
    "                                    \"_index\": \"passage_vectors\",\n",
    "                                    \"_id\": \"1\",\n",
    "                                    \"_nested\": {\n",
    "                                        \"field\": \"paragraph\",\n",
    "                                        \"offset\": 0\n",
    "                                    },\n",
    "                                    \"_score\": 1.0,\n",
    "                                    \"fields\": {\n",
    "                                        \"paragraph\": [\n",
    "                                            {\n",
    "                                                \"text\": [\n",
    "                                                    \"first paragraph\"\n",
    "                                                ]\n",
    "                                            }\n",
    "                                        ]\n",
    "                                    }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"_index\": \"passage_vectors\",\n",
    "                \"_id\": \"2\",\n",
    "                \"_score\": 0.9997144,\n",
    "                \"fields\": {\n",
    "                    \"creation_time\": [\n",
    "                        \"2020-05-04T00:00:00.000Z\"\n",
    "                    ],\n",
    "                    \"full_text\": [\n",
    "                        \"number one paragraph number two paragraph\"\n",
    "                    ]\n",
    "                },\n",
    "                \"inner_hits\": {\n",
    "                    \"paragraph\": {\n",
    "                        \"hits\": {\n",
    "                            \"total\": {\n",
    "                                \"value\": 2,\n",
    "                                \"relation\": \"eq\"\n",
    "                            },\n",
    "                            \"max_score\": 0.9997144,\n",
    "                            \"hits\": [\n",
    "                                {\n",
    "                                    \"_index\": \"passage_vectors\",\n",
    "                                    \"_id\": \"2\",\n",
    "                                    \"_nested\": {\n",
    "                                        \"field\": \"paragraph\",\n",
    "                                        \"offset\": 1\n",
    "                                    },\n",
    "                                    \"_score\": 0.9997144,\n",
    "                                    \"fields\": {\n",
    "                                        \"paragraph\": [\n",
    "                                            {\n",
    "                                                \"text\": [\n",
    "                                                    \"number two paragraph\"\n",
    "                                                ]\n",
    "                                            }\n",
    "                                        ]\n",
    "                                    }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing considerations\n",
    "\n",
    "근사 kNN(k-Nearest Neighbors) 검색을 위한 인덱싱 고려사항에 대해 다룸. \n",
    "\n",
    "HNSW 그래프 저장:\n",
    "- Elasticsearch는 각 세그먼트의 dense vector 값을 HNSW(Hierarchical Navigable Small World) 그래프로 저장합니다.\n",
    "\n",
    "인덱싱 시간:\n",
    "- 근사 kNN 검색을 위한 벡터 인덱싱은 상당한 시간이 소요될 수 있습니다.\n",
    "- HNSW 그래프 구축이 계산 비용이 높기 때문입니다.\n",
    "\n",
    "\n",
    "클라이언트 요청 타임아웃:\n",
    "- 인덱스 및 벌크 요청에 대한 클라이언트 요청 타임아웃을 증가시켜야 할 수 있습니다.\n",
    "\n",
    "성능 튜닝 가이드:\n",
    "- 인덱싱 성능과 검색 성능에 영향을 미치는 인덱스 구성에 대한 중요한 지침이 있습니다.\n",
    "- 참고: https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-knn-search.html\n",
    "\n",
    "\n",
    "HNSW 알고리즘 파라미터:\n",
    "- 검색 시간 튜닝 파라미터 외에도 인덱스 시간 파라미터가 있습니다.\n",
    "- 이 파라미터들은 그래프 구축 비용, 검색 속도, 정확도 사이의 균형을 조절합니다.\n",
    "\n",
    "\n",
    "index_options 설정:\n",
    "- dense_vector 매핑 설정 시 index_options 인자를 사용하여 이러한 파라미터를 조정할 수 있습니다.\n",
    "- type: HNSW 알고리즘 사용을 지정\n",
    "- m: 그래프의 각 노드가 가질 수 있는 최대 연결 수 (높을수록 더 정확하지만 인덱싱이 느려짐) (기본값은 16)\n",
    "- ef_construction: 그래프에 새로운 노드를 추가할 때 고려해야하는 후보 노드의 수임. (높을수록 더 정확하지만 인덱싱이 느려짐) (기본값은 100)\n",
    "\n",
    "\n",
    "```json \n",
    "PUT image-index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"image-vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 3,\n",
    "        \"similarity\": \"l2_norm\",\n",
    "        \"index_options\": {\n",
    "          \"type\": \"hnsw\",\n",
    "          \"m\": 32,\n",
    "          \"ef_construction\": 100\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations for approximate kNN search\n",
    "\n",
    "근사 kNN(k-Nearest Neighbors) 검색의 제한사항에 대해 다룸. \n",
    "\n",
    "크로스 클러스터 검색 제한:\n",
    "- kNN 검색을 크로스 클러스터 검색에서 사용할 때, 'ccs_minimize_roundtrips' 옵션은 지원되지 않습니다.\n",
    "- 이는 크로스 클러스터 검색 최적화의 일부 기능이 kNN 검색과 호환되지 않음을 의미합니다.\n",
    "\n",
    "HNSW 알고리즘의 특성:\n",
    "- Elasticsearch는 효율적인 kNN 검색을 위해 HNSW(Hierarchical Navigable Small World) 알고리즘을 사용합니다.\n",
    "- HNSW는 근사 방법으로, 대부분의 kNN 알고리즘과 마찬가지로 결과의 정확성을 일부 희생하여 검색 속도를 향상시킵니다.\n",
    "- 이는 반환된 결과가 항상 진정한 k개의 가장 가까운 이웃이 아닐 수 있음을 의미합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact kNN \n",
    "\n",
    "여기서는 Elasticsearch에서 정확한 kNN(k-Nearest Neighbors) 검색을 수행하는 방법을 다룸. 이전까지는 근사 kNN 에 대해 다루었다면. \n",
    "\n",
    "정확한 kNN 검색 방법:\n",
    "- script_score 쿼리와 벡터 함수를 사용합니다.\n",
    "\n",
    "인덱스 매핑:\n",
    "- dense_vector 필드를 명시적으로 매핑합니다.\n",
    "- 근사 kNN을 사용하지 않을 경우, index 옵션을 false로 설정하여 인덱싱 속도를 향상시킵니다.\n",
    "\n",
    "```json\n",
    "PUT product-index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"product-vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 5,\n",
    "        \"index\": false\n",
    "      },\n",
    "      \"price\": {\n",
    "        \"type\": \"long\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "검색 쿼리 구조:\n",
    "- script_score 쿼리를 사용합니다.\n",
    "- 벡터 함수(여기서는 cosineSimilarity)를 스크립트에 포함시킵니다.\n",
    "\n",
    "\n",
    "성능 최적화:\n",
    "- filter 쿼리를 사용하여 벡터 함수에 전달되는 문서 수를 제한합니다.\n",
    "- 모든 문서를 매치하는 것은 검색 지연 시간을 크게 증가시킬 수 있습니다.\n",
    "\n",
    "\n",
    "```json\n",
    "POST product-index/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"script_score\": {\n",
    "      \"query\" : {\n",
    "        \"bool\" : {\n",
    "          \"filter\" : {\n",
    "            \"range\" : {\n",
    "              \"price\" : {\n",
    "                \"gte\": 1000\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"script\": {\n",
    "        \"source\": \"cosineSimilarity(params.queryVector, 'product-vector') + 1.0\",\n",
    "        \"params\": {\n",
    "          \"queryVector\": [-0.5, 90.0, -10, 14.8, -156.0]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "주요 특징:\n",
    "- 정확성: 근사 방법과 달리 정확한 kNN 결과를 제공합니다.\n",
    "- 유연성: 다양한 벡터 함수와 필터링 조건을 조합할 수 있습니다.\n",
    "- 성능 고려: 대규모 데이터셋에서는 느릴 수 있으므로 적절한 필터링이 중요합니다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search: Combined Full-Text and kNN Results\n",
    "You have now seen two different approaches to search a collection of documents, each with its own particular benefits. If one of these methods matches your needs then you don't need anything else, but in many cases each method of searching returns valuable results that the other method would miss, so the best option is to offer a combined result set.\n",
    "\n",
    "For these cases, Elasticsearch offers Reciprocal Rank Fusion, an algorithm that combines results from two or more lists into a single list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How RRF Works\n",
    "Elasticsearch integrates the RRF algorithm into the search query. Consider the following example, which has query and knn sections to request full-text and vector searches respectively, and a rrf section that combines them into a single result list.\n",
    "\n",
    "\n",
    "```python\n",
    "self.es.search(\n",
    "    query={\n",
    "        # full-text search query here\n",
    "    },\n",
    "    knn={\n",
    "        # vector search query here\n",
    "    },\n",
    "    rank={\n",
    "        \"rrf\": {}\n",
    "    }\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "While RRF works fairly well for short lists of results without any configuration, there are some parameters that can be tuned to provide the best results. Consult the documentation to learn about these in detail.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reciprocal rank fusion\n",
    "\n",
    "참고: https://youngerjesus.tistory.com/148\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "\n",
    "class Search:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.es = Elasticsearch('http://localhost:9200')\n",
    "        client_info = self.es.info()\n",
    "        print('Connected to Elasticsearch!')\n",
    "        # pprint(client_info.body)\n",
    "\n",
    "    def create_index(self):\n",
    "        self.es.indices.delete(index='my_documents', ignore_unavailable=True)\n",
    "        self.es.indices.create(index='my_documents', mappings={\n",
    "            'properties': {\n",
    "                'embedding': {\n",
    "                    'type': 'dense_vector',\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    def insert_document(self, document):\n",
    "        return self.es.index(index='my_documents', document={\n",
    "            **document,\n",
    "            'embedding': self.get_embedding(document['summary']),\n",
    "        })\n",
    "    \n",
    "    def insert_documents(self, documents):\n",
    "        operations = []\n",
    "        for document in documents:\n",
    "            operations.append({'index': {'_index': 'my_documents'}})\n",
    "            operations.append({\n",
    "                **document,\n",
    "                'embedding': self.get_embedding(document['summary']),\n",
    "            })\n",
    "        return self.es.bulk(operations=operations)\n",
    "\n",
    "\n",
    "    def reindex(self):\n",
    "        self.create_index()\n",
    "        with open('data.json', 'rt') as f:\n",
    "            documents = json.loads(f.read())\n",
    "        return self.insert_documents(documents)\n",
    "    \n",
    "    \n",
    "    def get_embedding(self, text):\n",
    "        return self.model.encode(text)\n",
    "    \n",
    "    def extract_filters(self, query):\n",
    "        # This is a placeholder. Implement your filter extraction logic here.\n",
    "        return {}, query\n",
    "\n",
    "    def hybrid_search(self, query, from_=0, size=5):\n",
    "        filters, parsed_query = self.extract_filters(query)\n",
    "\n",
    "        if parsed_query:\n",
    "            search_query = {\n",
    "                'must': {\n",
    "                    'multi_match': {\n",
    "                        'query': parsed_query,\n",
    "                        'fields': ['name', 'summary', 'content'],\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            search_query = {\n",
    "                'must': {\n",
    "                    'match_all': {}\n",
    "                }\n",
    "            }\n",
    "\n",
    "        results = self.es.search(\n",
    "            index='my_documents',\n",
    "            query={\n",
    "                'bool': {\n",
    "                    **search_query,\n",
    "                }\n",
    "            },\n",
    "            knn={\n",
    "                'field': 'embedding',\n",
    "                'query_vector': self.get_embedding(parsed_query),\n",
    "                'k': 10,\n",
    "                'num_candidates': 50,\n",
    "            },\n",
    "            rank={\n",
    "                'rrf': {}\n",
    "            },\n",
    "            aggs={\n",
    "                'year-agg': {\n",
    "                    'date_histogram': {\n",
    "                        'field': 'updated_at',\n",
    "                        'calendar_interval': 'year',\n",
    "                        'format': 'yyyy',\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            size=size,\n",
    "            from_=from_,\n",
    "        )\n",
    "\n",
    "        aggs = {\n",
    "            'Year': {\n",
    "                bucket['key_as_string']: bucket['doc_count']\n",
    "                for bucket in results['aggregations']['year-agg']['buckets']\n",
    "                if bucket['doc_count'] > 0\n",
    "            },\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            'hits': results['hits']['hits'],\n",
    "            'total': results['hits']['total']['value'],\n",
    "            'aggs': aggs\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch!\n"
     ]
    }
   ],
   "source": [
    "search = Search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hits': [{'_index': 'my_documents', '_id': '_hPcpZABMnum9rmLig26', '_score': None, '_rank': 1, '_ignored': ['content.keyword'], '_source': {'name': 'Huge process.', 'url': 'http://www.fitzpatrick.net/', 'summary': 'Lawyer pay Congress indeed owner. Easy face boy feel performance history car.', 'content': 'Whose front on well gun happy. Blood other hear need write turn prepare huge.\\nAs positive own production. Heart too government woman draw how particular. Team us act seek.\\nWord wind what management. Live officer over radio. Traditional player subject religious turn.\\nOut bank car shoulder will available. Upon mention report major. West toward oil shake help operation.\\nNor reveal wide and never focus share.\\nVisit street ask gas show single pay matter.\\nLeg strong name chance travel skill. Game president type would speech road north moment. Cut beyond early say.\\nBehind together series suggest state campaign. Able two begin rest instead teacher.\\nToo phone save yeah threat read southern. Guess prepare stuff everybody degree. Agreement industry every his feeling claim enter.\\nTotal financial within research. Care situation section forward Congress news whole indicate.\\nHour series many section ground. Energy radio financial.\\nAmount development relate us. Generation piece next return our rock.', 'created_on': '2023-07-16T21:43:50.822067', 'category': 'teams', 'rolePermissions': ['admin', 'manager'], 'embedding': [-0.11676279455423355, 0.10096698254346848, 0.0349574089050293, -0.0028089832048863173, -0.10557562857866287, -0.011354755610227585, 0.022535530850291252, 0.00046278993249870837, -0.03263850137591362, -0.0011303327046334743, -0.025346292182803154, 0.017187360674142838, 0.013700676150619984, 0.02954036183655262, -0.005892168264836073, 0.02480132132768631, 0.054770972579717636, 0.03117644600570202, 0.0494467094540596, 0.04443914815783501, -0.10200051218271255, 0.03331359475851059, -0.0177245382219553, -0.02389734983444214, 0.024150537326931953, 0.04662952944636345, -0.04777764901518822, 0.06674390286207199, 0.07970722019672394, -0.03463710844516754, 0.0475551001727581, -0.01864871382713318, 0.0037227089051157236, 0.046000417321920395, 0.01315954327583313, -0.03733266890048981, 0.015673935413360596, -0.01636389084160328, -0.015026143752038479, 0.05281243845820427, -0.02067854255437851, 0.02737652324140072, -0.027508007362484932, 0.010232734493911266, -0.015752090141177177, 0.02360355667769909, 0.13121484220027924, 0.03302973508834839, 0.022619158029556274, -0.029537878930568695, -0.05770634859800339, -0.019607940688729286, 0.07300803065299988, -0.061581190675497055, -0.05026279017329216, 0.028973594307899475, 0.014418739825487137, 0.016302606090903282, -0.014906972646713257, 0.02574995532631874, -0.06342589110136032, -0.0455540232360363, 0.004067903384566307, -0.04564058408141136, -0.02281777746975422, 0.016600588336586952, 0.04056324064731598, -0.012038820423185825, -0.061481181532144547, 0.01911037415266037, 0.11123830080032349, 0.0233168825507164, -0.019010452553629875, -0.03230518847703934, -0.04913468286395073, -0.06115110218524933, 0.04280262440443039, 0.0659269466996193, 0.04644833877682686, -0.029357364401221275, -0.009241661056876183, -0.07704650610685349, -0.12126842141151428, -0.038791120052337646, -0.07586604356765747, -0.06438648700714111, 0.03537827730178833, 0.003459141356870532, -0.03317636623978615, 0.10112663358449936, -0.024189475923776627, -0.03671515733003616, 0.03896903246641159, -0.06882204115390778, 0.015666259452700615, 0.07757492363452911, 0.03943441063165665, 0.006522929761558771, -0.06164725497364998, 0.060001324862241745, 0.022732727229595184, 0.04104515165090561, 0.010819773189723492, -0.003954360727220774, 0.015963396057486534, 0.012685789726674557, 0.09123768657445908, 0.10073954612016678, 0.02311515621840954, -0.025008993223309517, 0.02898678556084633, 0.024898022413253784, -0.02509511634707451, 0.02293129824101925, 0.03264573588967323, 0.04480404034256935, -0.10273335874080658, 0.04243903607130051, 0.055136024951934814, -0.028271764516830444, -0.0343574583530426, 0.024780625477433205, -0.04394032061100006, 0.027457769960165024, -0.02148626372218132, -0.09016568958759308, 0.0003588194667827338, -1.6501091948454096e-33, 0.010893071070313454, 0.0373753197491169, -0.025432433933019638, 0.027575012296438217, 0.015862707048654556, 0.015799637883901596, 0.00998025480657816, 0.037034131586551666, -0.05555706098675728, -0.0024847211316227913, -0.010413020849227905, -0.08255377411842346, -0.07755858451128006, -0.02059338614344597, -0.07286398857831955, 0.06080395355820656, -0.1700071543455124, -0.027835175395011902, -0.056250542402267456, -0.051242683082818985, -0.03274917975068092, 0.10456807166337967, 0.0683039054274559, 0.07076382637023926, -0.00064377038506791, -0.007072674110531807, 0.03348219022154808, -0.014982238411903381, -0.016095902770757675, -0.009888420812785625, 0.029373809695243835, 0.03873177245259285, 0.03468596190214157, 0.04142850264906883, 0.04027579352259636, 0.06469375640153885, -0.005927777849137783, -0.04069178178906441, 0.0453437939286232, 0.05269535630941391, -0.015199066139757633, -0.03559796139597893, -0.05534297227859497, -0.011548315174877644, -0.10390352457761765, 0.08666127175092697, -0.08565041422843933, -0.0008556541288271546, 0.019859153777360916, 0.018369795754551888, -0.01976476050913334, 0.06262505799531937, 0.028700117021799088, -0.048777464777231216, -0.01699964888393879, 0.007496654521673918, -0.025939226150512695, 0.08104114234447479, -0.01001105085015297, -0.05329037085175514, 0.005181024316698313, 0.08682595193386078, 0.012436880730092525, 0.076275534927845, -0.18967124819755554, 0.04098738357424736, -0.005295057315379381, -0.010361692868173122, 0.01219287607818842, 0.025724176317453384, 0.020753009244799614, 0.022357095032930374, -0.04229604825377464, -0.08362873643636703, 0.03867914155125618, -0.0006408459739759564, 0.011275853030383587, -0.027762362733483315, -0.025975028052926064, 0.03198898211121559, -0.03309520706534386, 0.03342270106077194, 0.04761780425906181, -0.03325682133436203, 0.0941498801112175, -0.020603442564606667, -0.023002129048109055, -0.057961780577898026, 0.09216461330652237, 0.07253912836313248, 0.01194711122661829, -0.004345650319010019, -0.008728296495974064, 0.07545662671327591, -0.007633247412741184, 5.162829504351547e-34, 0.0004525413678493351, -0.13999618589878082, 0.07296593487262726, -0.03247943893074989, -0.01527278870344162, -0.039076656103134155, -0.045361053198575974, -0.08313656598329544, 0.0012242329539731145, 0.06548892706632614, -0.05048571154475212, -0.011598197743296623, -0.03265772759914398, 0.021051011979579926, 0.02489568293094635, -0.007667517755180597, 0.08522225171327591, -0.09776356816291809, -0.02333599142730236, -0.007104475516825914, 0.004554233979433775, 0.05080054700374603, 0.016853276640176773, 0.05986142158508301, -0.022627923637628555, -0.009205393493175507, 0.04067699983716011, -0.01332176011055708, 0.06214987486600876, -0.008508254773914814, -0.027171265333890915, -0.049968380481004715, -0.11981987208127975, 0.032791100442409515, -0.04960362985730171, -0.005906095262616873, -0.05650090053677559, -0.044058993458747864, -0.07499201595783234, -0.05077055096626282, 0.03188053518533707, -0.026166437193751335, 0.03300190344452858, 0.1442955583333969, 0.03550930693745613, -0.07840259373188019, 0.001423580921255052, -0.10554086416959763, -0.06297282874584198, 0.021480927243828773, 0.0013726699398830533, -0.015366329811513424, 0.07218451052904129, 0.06971995532512665, -0.028118764981627464, 0.04083571210503578, 0.11019109189510345, -0.024857012555003166, 0.0417325496673584, 0.06877196580171585, 0.03265099599957466, -0.02670266106724739, -0.06570266932249069, 0.004047283437103033, -0.04806208238005638, -0.017042996361851692, -0.044278375804424286, -0.06111936643719673, 0.008338411338627338, 0.0025348695926368237, 0.0969538614153862, -0.11216311156749725, -0.06282714009284973, 0.0032646774780005217, 0.0013828100636601448, 0.17049525678157806, -0.009873311966657639, 0.05851146951317787, -0.05947301909327507, -0.03779011592268944, 0.03383258357644081, -0.05714932084083557, 0.0731816440820694, 0.014338086359202862, 0.02246619574725628, 0.05554499104619026, 0.027560420334339142, -0.07544992864131927, 0.032295577228069305, 0.061210691928863525, 0.00477942917495966, 0.042389433830976486, 0.004249270539730787, 0.025637803599238396, -0.03748674690723419, -2.0456932148249507e-08, -0.08926117420196533, -0.029214585199952126, 0.009538537822663784, -0.01927860826253891, -0.008974084630608559, -0.008334989659488201, 0.06370705366134644, -0.006774009205400944, -0.08389178663492203, 0.012486235238611698, 0.10302062332630157, -0.03504304587841034, 0.005804459564387798, -0.0005635458510369062, -0.004961575847119093, 0.011534351855516434, -0.04631206393241882, 0.06582554429769516, -0.06487484276294708, 0.026951860636472702, -0.02207372337579727, 0.04281121864914894, 0.009771597571671009, 0.11261194944381714, -0.08722025156021118, 0.0035840191412717104, -0.04851389676332474, 0.05335737764835358, -0.01812826469540596, 0.0042829434387385845, -0.02294101007282734, 0.06987987458705902, -0.0028601123485714197, -0.07357891649007797, 0.011474226601421833, -0.06312062591314316, 0.0029515449423342943, 0.02146286517381668, 0.03194598853588104, -0.024228692054748535, -0.036247916519641876, 0.07962930202484131, -0.009903475642204285, -0.00971634965389967, 0.024297671392560005, 0.04510127380490303, -0.0398113988339901, -0.010360236279666424, 0.032070327550172806, -0.0060086022131145, 0.08567211031913757, -0.060156434774398804, -0.06788524985313416, 0.024005167186260223, 0.02747916430234909, -0.0032594643998891115, -0.010622380301356316, 0.046837229281663895, -0.09733586758375168, -0.039255864918231964, 0.07374803721904755, 0.008334603160619736, 0.06363697350025177, 0.052480630576610565]}}, {'_index': 'my_documents', '_id': '6xPcpZABMnum9rmLig67', '_score': None, '_rank': 2, '_ignored': ['content.keyword'], '_source': {'name': 'Six listen treat.', 'url': 'https://www.parker.com/', 'summary': 'Travel computer little star.', 'content': 'Left light rich forward happy generation thousand. Thus everyone thing oil low model evidence.\\nWhether bed century rise.\\nHowever age leave together. Decade billion mother probably near smile teach. Fast court author one raise between.\\nCompare church product.\\nRather sense always he size. Leader quite probably year.\\nOut suddenly situation without north him. Hour major reduce Republican leg. Air scene nothing hard until baby recently.\\nParticularly about research peace why. I career Congress politics despite leader will. Technology my series more north pick carry.\\nInstitution it between work yeah number already stock. Another answer north four person produce real decade. Else some agree.\\nHer author seat imagine fear. Catch federal among cold blue take. Loss control win gas recently audience.\\nReality economy ever financial less sell.\\nDemocrat note big subject. Remain experience television analysis however party. Material draw two bill state treatment.', 'created_on': '2023-10-01T10:15:09.847013', 'category': 'github', 'rolePermissions': ['admin'], 'embedding': [-0.00044100446393713355, 0.066290482878685, 0.017244497314095497, 0.03076316975057125, -0.0021649571135640144, -0.05330093204975128, 0.10601302236318588, -0.0234660841524601, 0.019172245636582375, 0.0108414301648736, 0.006447210442274809, 0.041326507925987244, 0.023922940716147423, -0.013349803164601326, -0.05285745859146118, -0.005038663279265165, 0.011647532694041729, -0.10356011986732483, 0.02662789635360241, -0.003249281784519553, -0.03856560215353966, -0.033523060381412506, -0.11195803433656693, 0.03433173894882202, -0.0039285654202103615, 0.09422227740287781, -0.003379722125828266, 0.0009255693876184523, -0.03885531798005104, -0.06138117238879204, -0.02075342647731304, 0.06337805837392807, -0.10815954208374023, 0.08034980297088623, -0.040429215878248215, 0.011445782147347927, 0.06674862653017044, -0.07206860184669495, -0.016918523237109184, -0.07885293662548065, -0.019034748896956444, -0.03543110936880112, 0.05245174095034599, 0.08030620962381363, -0.0022088002879172564, -0.0474519319832325, -0.06751900911331177, -0.0740308091044426, 0.06291455030441284, 0.029937462881207466, -0.03200039640069008, -0.047427427023649216, -0.011074925772845745, -0.02108832448720932, 0.0073195104487240314, -0.024744447320699692, 0.07448524981737137, -0.052698224782943726, 0.037449151277542114, 0.01634756661951542, -0.001417580177076161, -0.08411255478858948, -0.030690472573041916, 0.04012654721736908, 0.003921987488865852, -0.08506648987531662, -0.07350319623947144, -0.03241479769349098, -0.04943418875336647, -0.01480143703520298, -0.06001616641879082, 0.018514025956392288, -0.034087132662534714, 0.13322483003139496, 0.016234343871474266, -0.10426148027181625, 0.04790710285305977, -0.03578998148441315, 0.07985415309667587, 0.07462280243635178, -0.008055107668042183, -0.026085544377565384, -0.12296578288078308, 0.06667214632034302, -0.029919102787971497, -0.02031630463898182, 0.0023092469200491905, -0.012636937201023102, -0.00254322518594563, -0.07309219241142273, 0.030074890702962875, -0.021150968968868256, 0.024081405252218246, 0.008943567052483559, -0.06425636261701584, -0.10370342433452606, -0.025217324495315552, -0.1577785164117813, 0.016837408766150475, 0.095250703394413, 0.03284668177366257, 0.07515916228294373, 0.1219119280576706, -0.0038281530141830444, 0.006112338043749332, -0.003312695072963834, 0.10619042813777924, -0.008042499423027039, 0.03589073196053505, -0.010241981595754623, -0.04199525713920593, 0.01809297315776348, -0.05160319060087204, -0.007146842312067747, -0.012191030196845531, 0.0006222723750397563, -0.08058572560548782, 0.05466663837432861, -0.012485207058489323, -0.0025599536020308733, 0.008738504722714424, -0.03395339474081993, 0.039945267140865326, 0.008515716530382633, -0.03187468275427818, -0.029829995706677437, 0.03580378368496895, -4.442938121302258e-33, 0.011974281631410122, 0.08003874123096466, 0.00023498597147408873, 0.060480859130620956, 0.07954467087984085, 0.03257974237203598, -0.04145445674657822, -0.005880529992282391, -0.038487508893013, -0.06563323736190796, -0.015050457790493965, -0.021390380337834358, -0.021779600530862808, 0.03126431256532669, 0.1543627232313156, -0.05556272342801094, 0.04468740522861481, -0.036105863749980927, 0.01667347177863121, -0.05060359835624695, 0.03483354300260544, -0.005606681574136019, 0.03142070770263672, -0.04136035591363907, 0.06916805356740952, 0.019064785912632942, -0.029183901846408844, -0.04762933775782585, 0.10373992472887039, 0.0388571098446846, -0.025550363585352898, 0.027581052854657173, -0.02081739902496338, -0.012361346744000912, -0.014201703481376171, 0.037639692425727844, -0.04300972446799278, -0.05694970488548279, 0.03096761554479599, 0.011189782992005348, -0.05269763991236687, -0.0008198412251658738, -0.05529969930648804, 0.003914910368621349, -0.049667589366436005, -0.0009032118250615895, 0.09816720336675644, -0.007149511016905308, 0.040407806634902954, 0.060908474028110504, -0.06641583144664764, -0.014052131213247776, -0.029605085030198097, -0.08218754827976227, -0.028035160154104233, 0.004719325341284275, 0.04759690538048744, -0.007175099104642868, 0.03525856137275696, 0.0030742313247174025, 0.08660271763801575, 0.07620560377836227, 0.04549749940633774, 0.012149354442954063, 0.051883213222026825, 0.07880987226963043, 0.06946324557065964, 0.0313882902264595, -0.011245056986808777, 0.009115113876760006, -0.04189791902899742, 0.0061129918321967125, 0.06121761351823807, -0.1150735393166542, 0.03395652770996094, 0.015602095052599907, 0.023459937423467636, -0.04840129613876343, -0.12645888328552246, -0.09783728420734406, -0.08341894298791885, -0.056797757744789124, 0.053353674709796906, -0.0004864875809289515, 0.07335160672664642, -0.042962342500686646, -0.049870815128088, -0.006602979730814695, -0.04196448624134064, -0.057220179587602615, 0.007390287704765797, -0.010359042324125767, 0.015004109591245651, 0.024990146979689598, -0.11250827461481094, 2.925458668363181e-33, -0.02929670549929142, -0.06202814355492592, 0.08929987996816635, 0.05861937254667282, 0.04533488675951958, -0.01748194359242916, 0.07375412434339523, 0.008332855999469757, -0.046515967696905136, 0.07131819427013397, -0.08333636075258255, 0.024364104494452477, 0.06770829111337662, -0.10570772737264633, 0.14892640709877014, 0.034093089401721954, 0.01416089665144682, 0.07130317389965057, -0.016308970749378204, 0.050454579293727875, 0.04712175577878952, 0.011116351000964642, -0.0776146724820137, -0.021588144823908806, -0.0035240547731518745, 0.03177646920084953, 0.02656649984419346, -0.06824598461389542, -0.05394277349114418, 0.009871355257928371, -0.012693696655333042, -0.035463474690914154, 0.03441716730594635, 0.03555169701576233, 0.008112620562314987, 0.05409342423081398, 0.016160590574145317, -0.01946248672902584, -0.023508857935667038, -0.013017229735851288, 0.034961555153131485, 0.05271429568529129, 0.010723969899117947, 0.10764237493276596, 0.04082370549440384, -0.013009504415094852, -0.05931352078914642, 0.06945723295211792, -0.009184494614601135, 0.02087302878499031, 0.04449174553155899, -0.06879828870296478, -0.000602955580689013, 0.005619403440505266, -0.020121129229664803, 0.04384034499526024, 0.0002042616397375241, 0.07739632576704025, 0.06969835609197617, -0.0022493363358080387, -0.017476288601756096, -0.10362832993268967, 0.006819861475378275, -0.005740482825785875, -0.044384751468896866, -0.059827689081430435, 0.05520808324217796, 0.09745493531227112, -0.06403754651546478, -0.013431282714009285, 0.06741925328969955, 0.03951788321137428, -0.0172727033495903, -0.007855755276978016, 0.009107159450650215, 0.06130458414554596, 0.012821649201214314, 0.03403650224208832, -0.0354742705821991, 0.0348476879298687, 0.016518523916602135, 0.011868755333125591, -0.001324056414887309, -0.016750328242778778, 0.08963579684495926, -0.00799281895160675, 0.01321929506957531, -0.051459696143865585, -0.03982093930244446, 0.0019115412142127752, -0.043006595224142075, 0.10267451405525208, 0.05845768749713898, -0.04009557142853737, -0.023480935022234917, -1.374867153458581e-08, -0.01970953494310379, -0.01651078462600708, 0.0021084346808493137, 0.009281774051487446, 0.06175043806433678, -0.04447176307439804, -0.007479640655219555, -0.018284063786268234, 0.025174882262945175, 0.02090766839683056, -0.007216005120426416, -0.04113978520035744, 0.01930999755859375, 0.030062170699238777, 0.009696373715996742, 0.04303497448563576, -0.03181200101971626, 0.0019353061215952039, -0.011886623688042164, 0.08224233984947205, 0.04385877028107643, 0.048267900943756104, 0.06949181854724884, 0.01932566426694393, 0.01790127158164978, -0.026257703080773354, -0.01352918054908514, 0.07745656371116638, 0.003190861316397786, -0.10305429995059967, -0.035125818103551865, 0.08903724700212479, -0.09263435751199722, -0.04463640972971916, -0.08266476541757584, 0.042460404336452484, 0.04263097792863846, 0.08919209241867065, 0.05469834804534912, 0.047865986824035645, 0.005467432085424662, 0.020169900730252266, 0.012170468457043171, -0.02267494797706604, -0.08552394062280655, 0.01918662339448929, -0.06401453912258148, -0.1321532130241394, -0.03015850856900215, 0.01953131891787052, -0.03981279581785202, 0.005811604205518961, -0.03423122689127922, 0.03856978192925453, 0.04109321907162666, -0.005792796146124601, -0.05798020958900452, 0.0468258373439312, -0.03637542948126793, 0.06685059517621994, 0.04845144599676132, 0.04916179180145264, -0.034504830837249756, 0.01193761732429266]}}, {'_index': 'my_documents', '_id': 'iBPcpZABMnum9rmLig66', '_score': None, '_rank': 3, '_ignored': ['content.keyword'], '_source': {'name': 'Production success save least reality.', 'url': 'https://www.richardson.com/', 'summary': 'She attack car Congress.', 'content': 'Paper open sure actually. Science allow owner. Wind yourself summer political long feel.\\nStaff region each in paper along let. Force understand occur yourself single economic. Product team society customer. Interest involve discuss environmental reveal.\\nNation outside collection prevent language ground matter. Share spend no black.\\nLose effort need detail. Together development act my current. Hear six set company possible.\\nShould project mention agent boy. Continue end everything assume choice service race. House memory order however.\\nAsk fine recent moment human activity prepare. Authority why others likely consider sing media however.\\nForm expert Mr film wonder area use official. Rest must page subject.\\nFederal always coach strategy feeling relationship. Ground reach ago behavior respond weight. We impact cover door art movement fire. Tough pull side loss purpose personal.\\nParty others he value full. Worker agreement management like product. Talk per pull study.', 'created_on': '2023-08-25T18:56:17.192814', 'category': 'sharepoint', 'rolePermissions': ['guest', 'user'], 'embedding': [-0.02451169490814209, 0.02035452425479889, 0.054557833820581436, 0.09971975535154343, -0.0492747500538826, 0.0762442797422409, -0.008378525264561176, -0.003632820211350918, -0.011192673817276955, 0.022331612184643745, -0.0016964513342827559, 0.01045955065637827, 0.015171945095062256, -0.024072248488664627, -0.04420100152492523, 0.09417293220758438, 0.04971772059798241, 0.001414664788171649, -0.004921460524201393, 0.07360085099935532, -0.011550888419151306, 0.04253489896655083, 0.06867581605911255, 0.07855396717786789, 0.00725832162424922, 0.008025110699236393, -0.06552192568778992, 0.027222367003560066, -0.03668473660945892, -0.03727814927697182, -0.017709705978631973, 0.01830385811626911, -0.06659583747386932, 0.11460477858781815, 0.018853209912776947, -0.04379664734005928, 0.08083051443099976, 0.026163330301642418, 0.057190313935279846, -0.037091389298439026, -0.09121744334697723, -0.11574223637580872, -0.05894666910171509, -0.03823341429233551, -0.09241794794797897, -0.05056927353143692, 0.0594407394528389, 0.08710829168558121, 0.03299380838871002, -0.14598079025745392, -0.01717344857752323, 0.048152051866054535, -0.09134923666715622, -0.09033478051424026, 0.010945875197649002, -0.038494937121868134, 0.018916305154561996, 0.06711990386247635, 0.10144882649183273, 0.02685460075736046, 0.008243104442954063, -0.010877755470573902, -0.017392680048942566, 0.03185325860977173, -0.029892731457948685, -0.03440694883465767, 0.0630766749382019, -0.0006693553295917809, 0.018299371004104614, 0.07437464594841003, 0.11130573600530624, 0.057209886610507965, 0.0020627982448786497, -0.04868071898818016, 0.05746084451675415, -0.02890426479279995, 0.10403679311275482, 0.044067203998565674, 0.07266967743635178, -0.040788684040308, -0.007367642596364021, 0.013412831351161003, 0.031520944088697433, 0.03969059884548187, 0.07137364894151688, -0.043973080813884735, -0.005132417660206556, 0.0010138743091374636, -0.005156003870069981, 0.025352325290441513, -0.08998840302228928, -0.03391153737902641, 0.011024413630366325, 0.030387796461582184, -0.07167036831378937, -0.028421519324183464, -0.008183772675693035, -0.09293066710233688, 0.031577788293361664, 0.07275821268558502, -0.00545039027929306, 0.07898638397455215, -0.03482254222035408, -0.06939095258712769, -0.06128576397895813, 0.019216137006878853, 0.08527307212352753, -0.0009382024290971458, -0.023274725303053856, -0.006535110995173454, 0.057974498718976974, -0.05005665123462677, -0.06503576040267944, -0.06422317028045654, -0.025131847709417343, 0.055236224085092545, 0.02096344344317913, -0.0207686610519886, -0.06563667953014374, 0.013653324916958809, -0.04750863462686539, -0.1334601789712906, -0.05706381797790527, 0.02625560574233532, 0.01765955425798893, -0.08409620076417923, -0.013537740334868431, -5.479384347391731e-33, -0.05687637999653816, -0.02574567310512066, -0.03928305581212044, 0.0783437043428421, 0.06335379183292389, 0.03925631567835808, 0.044202759861946106, 0.007801508065313101, 0.027571149170398712, 0.0020822316873818636, -0.017171213403344154, -0.030915414914488792, -0.024714836850762367, -0.07645612955093384, 0.05598274618387222, 0.0642472580075264, -0.10606683045625687, 0.0350758358836174, -0.05505906790494919, -0.007324256934225559, 0.0615169033408165, 0.0816924199461937, 0.03876349702477455, 0.057975884526968, 0.05768093466758728, 0.002937855664640665, 0.07844588160514832, 0.02933688834309578, -0.08835658431053162, 0.05003291368484497, 0.004822279326617718, 0.0022655227221548557, 0.038899876177310944, 0.0552198700606823, 0.03241796791553497, 0.055313125252723694, -0.02270367555320263, -0.07283295691013336, -0.03223215043544769, 0.011663924902677536, 0.016420388594269753, 0.014233874157071114, 0.022984281182289124, 0.005130063742399216, -0.057386547327041626, 0.009113674983382225, 0.05787895992398262, -0.018033362925052643, 0.03401533514261246, -0.0026891729794442654, -0.016093801707029343, -0.012095356360077858, 0.022369323298335075, 0.05215836688876152, -0.004405447747558355, 0.06206269934773445, -0.043276555836200714, -0.05515255779027939, 0.08543471246957779, 0.008280360139906406, -0.04206021875143051, 0.0393490344285965, -0.04955374449491501, 0.057978738099336624, -0.0834321603178978, 0.04938163980841637, -0.10865873843431473, -0.03172125294804573, 0.016653992235660553, 0.11125091463327408, -0.026069726794958115, 0.009643692523241043, 0.04343116655945778, -0.0241620484739542, -0.1350209265947342, 0.07036753743886948, 0.046322889626026154, -0.06982816010713577, 0.00016931400750763714, -0.029390381649136543, -0.03913656994700432, -0.011344748549163342, 0.10023897141218185, 0.049123819917440414, 0.08424313366413116, -0.0317046083509922, -0.05645411089062691, -0.05636848881840706, 0.021705875173211098, 0.03006603568792343, -0.037387147545814514, 0.007934978231787682, 0.03299908712506294, -0.05424021556973457, -0.05254727602005005, 3.6895483634968995e-33, -0.011861820705235004, -0.023785995319485664, -0.008889895863831043, 0.037646275013685226, 0.01855308562517166, -0.01658784970641136, -0.06750860810279846, -0.013618147931993008, 0.033352240920066833, -0.004355445969849825, -0.0016655500512570143, -0.03676379844546318, 0.037646327167749405, 0.032971467822790146, 0.10096840560436249, 0.010446431115269661, 0.11877171695232391, -0.06168639659881592, -0.05132006108760834, -0.0367896594107151, -0.1285499781370163, 0.01874772273004055, 0.05281387269496918, 0.04414927214384079, 0.04377502202987671, 0.00021706953702960163, 0.05171813443303108, -0.03342003375291824, 0.08191481977701187, 0.006869029253721237, -6.168340769363567e-05, -0.06145947799086571, -0.05295606330037117, 0.030584977939724922, 0.01101115345954895, 0.06277069449424744, 0.011814536526799202, -0.043083757162094116, 0.0032693457324057817, -0.07767027616500854, 0.03550642728805542, -0.010668476112186909, 0.0826025977730751, 0.10794365406036377, 0.005870950873941183, -0.017318226397037506, 0.04289868101477623, 0.04870343953371048, -0.04335685446858406, 0.018045969307422638, -0.016895798966288567, -0.04909810796380043, 0.028349634259939194, 0.06960874050855637, 0.004816030152142048, 0.006244594696909189, 0.08628744632005692, -0.011209012940526009, 0.05450655147433281, 0.02316199243068695, 0.0503353476524353, -0.0016078694025054574, -0.04397645592689514, -0.051963336765766144, -0.0264204703271389, -0.09426945447921753, -0.08311651647090912, -0.042721111327409744, 0.057614438235759735, 0.044515058398246765, 0.08317302167415619, 0.04990500211715698, -0.09502236545085907, 0.028595680370926857, -0.08453530073165894, 0.012127547524869442, -0.04507791996002197, -0.009995194151997566, -0.02779378369450569, 0.008493754081428051, 0.037546221166849136, -0.041293270885944366, -0.006392380688339472, -0.025404948741197586, 0.0633280947804451, 0.0878322497010231, 0.04128191992640495, 0.01718374341726303, 0.05208598077297211, 0.030342167243361473, -0.006278271321207285, -0.033589910715818405, 0.047805145382881165, 0.004168563988059759, -0.07787320017814636, -1.4083986421553618e-08, -0.015781037509441376, 0.027572257444262505, 0.031098119914531708, -0.09159943461418152, -0.022482240572571754, 0.023692864924669266, -0.03945958614349365, 0.0003584474907256663, -0.060256779193878174, -0.021110376343131065, 0.12158365547657013, 0.014505956321954727, 0.01969527266919613, -0.010639731772243977, -0.038110192865133286, -0.024022869765758514, 0.04180264100432396, 0.0006163727957755327, -0.081595279276371, -0.010234115645289421, -0.05283426493406296, -0.038576044142246246, -0.010024971328675747, 0.10467950254678726, -0.009181707166135311, 0.0001898178888950497, -0.04554338380694389, 0.04089292883872986, 0.03150765970349312, -0.021248210221529007, -0.06090877577662468, 0.021698396652936935, -0.03405964374542236, -0.04318621754646301, -0.08198391646146774, 0.0704064816236496, 0.028084002435207367, 0.02564399316906929, -0.07869208604097366, -0.04333790764212608, -0.005447467789053917, 0.06727100908756256, 0.029414622113108635, -0.03953242301940918, -0.012499041855335236, -0.019216278567910194, -0.06246112659573555, -0.07132577896118164, 0.06016435846686363, -0.02588697336614132, 0.0067688580602407455, 0.0013271820498630404, -0.06338172405958176, 0.07074572145938873, 0.009705235250294209, 0.016852254047989845, -0.018546389415860176, -0.0963805690407753, 0.005649234633892775, 0.021397065371274948, -0.007173974998295307, 0.021439364179968834, 0.049116604030132294, -0.041157301515340805]}}, {'_index': 'my_documents', '_id': 'yRPcpZABMnum9rmLig26', '_score': None, '_rank': 4, '_ignored': ['content.keyword'], '_source': {'name': 'Others onto bar.', 'url': 'https://nelson.org/', 'summary': 'Current even throughout car.', 'content': 'Tough describe individual individual discover herself year catch. Issue interesting quite travel Republican seek. Police environment yourself member.\\nSome road subject station dream. Laugh know attack education of chance.\\nNor he worker. Last hard authority visit. None center technology state effect.\\nHealth member up bill put officer purpose. However huge professional note determine protect option future. But make hold skill beautiful fast air test.\\nThree international food current message red number on. Not wrong collection catch property.\\nProduct chair paper professional drug forget nearly. Apply or pay treat everything character.\\nCase those big magazine. Focus stuff could opportunity. These son nice new. Expert leader herself enough value.\\nBring point explain end simple. Responsibility law fall notice foreign.\\nDifferent any event heart board school wrong. Next probably ball control have former build ability. Along must special southern down hair manager.', 'created_on': '2022-11-13T17:14:37.861651', 'category': 'sharepoint', 'rolePermissions': ['admin'], 'updated_at': '2023-08-08T03:29:03.144243', 'embedding': [0.025146594271063805, 0.04967927932739258, -0.0003882748424075544, 0.05833117663860321, -0.024333525449037552, -0.01878233440220356, 0.024585390463471413, 0.014375206083059311, 0.038811277598142624, 0.015053453855216503, 0.05943239480257034, 0.08078127354383469, 0.02556421607732773, 0.055452026426792145, 0.017906606197357178, 0.026062902063131332, 0.039572615176439285, 0.003943100571632385, -0.01404754351824522, -0.025257399305701256, 0.02098309062421322, 0.0055538928136229515, 0.05821177735924721, 0.017033055424690247, 0.027488308027386665, 0.014096437953412533, -0.027226509526371956, 0.002510186517611146, 0.030982695519924164, -0.043478045612573624, -0.05920959264039993, 0.043666772544384, -0.11267215758562088, -0.0007314664544537663, -0.06277294456958771, -0.04476536810398102, -0.03974059224128723, 0.07207194715738297, 0.06341004371643066, 0.06354072690010071, -0.0220540352165699, -0.09225176274776459, 0.07506129145622253, -0.025702474638819695, 0.03179863467812538, 0.03997710347175598, 0.027022311463952065, 0.013354946859180927, 0.06900666654109955, -0.048498060554265976, 0.03613125532865524, -0.020390179008245468, 0.008075769990682602, -0.004363687243312597, -0.04925811290740967, -0.022624388337135315, 0.009450141340494156, 0.08857519179582596, 0.03592931479215622, 0.006177943665534258, 0.049481216818094254, -0.013315396383404732, -0.020767634734511375, 0.014089901931583881, 0.028924448415637016, -0.016153259202837944, 0.018154669553041458, 0.0074274176731705666, 0.026986243203282356, 0.04339899122714996, 0.07294950634241104, 0.007978714071214199, 0.0029103602282702923, -0.1466333568096161, 0.04058268666267395, -0.09908217191696167, 0.01576741226017475, 0.008713667280972004, 0.0315307192504406, -0.003891703439876437, -0.032950349152088165, -0.02525237947702408, -0.06422781944274902, -0.07425632327795029, 0.11250166594982147, 0.005591117776930332, 0.07260694354772568, -0.055579423904418945, -0.11233063787221909, 0.10437118262052536, -0.10246934741735458, -0.0679745078086853, 0.04281134530901909, -0.029115384444594383, 0.009847303852438927, 0.022296329960227013, 0.025624888017773628, -0.006201956421136856, -0.05445762351155281, -0.0077206287533044815, 0.10709953308105469, 0.0747407004237175, -0.0509968064725399, 0.049776043742895126, -0.030787529423832893, 0.06941715627908707, -0.003201513085514307, 0.023515507578849792, -0.010039671324193478, 0.012261522002518177, 0.014483878389000893, -0.027348367497324944, -0.056888189166784286, -0.039541032165288925, -0.02748950384557247, -0.06087738648056984, 0.017341505736112595, 0.07052329927682877, -0.03127843514084816, 0.10062715411186218, -0.05505748465657234, 0.015103896148502827, 0.02182401344180107, 0.031594276428222656, 0.0007154117920435965, -0.0009327033767476678, 0.11840211600065231, -4.080244849485811e-33, -0.006503628101199865, -0.06960759311914444, 0.05476866662502289, 0.06158335879445076, -0.025554433465003967, -0.00017917364311870188, -0.024349695071578026, 0.07851933687925339, 0.07914494723081589, 0.026349222287535667, 0.04508807137608528, 0.13431762158870697, -0.0008066263399086893, -0.0011320393532514572, 0.047684196382761, 0.03231024742126465, -0.01732020452618599, -0.041215699166059494, -0.06082050874829292, -0.07940386235713959, -0.05541415140032768, 0.019520552828907967, 0.004220239818096161, -0.02430875226855278, 0.07601549476385117, 0.032515548169612885, 0.014622640796005726, 0.025119313970208168, -0.09538009017705917, 0.015786070376634598, 0.018495045602321625, 0.023540956899523735, -0.05241696164011955, 0.110904760658741, 0.0012019657297059894, 0.04619625583291054, -0.005213945172727108, -0.0014200325822457671, 0.03433000668883324, -0.049425505101680756, -0.0029931324534118176, -0.02656909078359604, -0.0012767744483426213, -0.001827129628509283, -0.029464034363627434, 0.01069154404103756, -0.016799213364720345, -0.044283054769039154, -0.0026309837121516466, 0.07461749762296677, -0.017907537519931793, -0.09779630601406097, -0.036721061915159225, 0.07270273566246033, 0.025052126497030258, 0.10082334280014038, -0.03785029426217079, -0.009685242548584938, -0.008443936705589294, -0.007226248737424612, -0.04678179696202278, 0.0834796205163002, -0.01962750218808651, -0.03069809265434742, -0.09919450432062149, 0.04513222351670265, -0.026255231350660324, 0.029335906729102135, -0.07414449751377106, 0.0009569989051669836, 0.009636583738029003, -0.040471501648426056, 0.049206458032131195, -0.02371818758547306, -0.013690716587007046, 0.0018275888869538903, 0.018869366496801376, 0.0315963514149189, -0.05509885773062706, -0.002700514392927289, -0.08907397836446762, -0.07397709786891937, -0.00025130363064818084, -0.03967363014817238, 0.06294999271631241, 0.008173694834113121, -0.026233596727252007, -0.032267842441797256, 0.03073565475642681, 0.04682140052318573, -0.005318050272762775, 0.029266448691487312, 0.042566072195768356, -0.06434847414493561, -0.010878894478082657, 2.582257296935275e-33, 0.03547440841794014, -0.02061278745532036, -0.012168840505182743, 0.028633778914809227, 0.03141582012176514, -0.024857094511389732, -0.014382528141140938, -0.01370799820870161, -0.036098603159189224, 0.09595262259244919, 0.0030851480551064014, 0.003115551546216011, -0.02185947448015213, -0.014159337617456913, 0.01820238120853901, -0.03757229819893837, 0.06708486378192902, 0.03085450455546379, -0.018247099593281746, 0.008000051602721214, -0.04567290097475052, -0.003098040819168091, 0.07996239513158798, 0.035824984312057495, -0.019423801451921463, 0.013511867262423038, -0.05970176309347153, -0.009671085514128208, -0.03149140626192093, -0.026880616322159767, 0.03505139425396919, 0.026732169091701508, -0.04100261256098747, 0.0018097622087225318, -0.08326201885938644, -0.052988920360803604, -0.00043958608875982463, -0.01958734355866909, 0.0049975053407251835, 0.042037233710289, -0.00385556323453784, -0.01996207796037197, 0.06933493167161942, 0.10389982908964157, -0.06682796776294708, -0.01396864466369152, 0.013976629823446274, -0.11226502805948257, -0.04650643840432167, 0.039825018495321274, 0.035835739225149155, -0.1294213831424713, -0.017916731536388397, 0.040831513702869415, 0.0008946846355684102, 0.056521426886320114, 0.02661379799246788, 0.0748731791973114, -0.07529415190219879, -0.038954269140958786, 0.17546021938323975, 0.028635986149311066, -0.049395669251680374, -0.04669573903083801, -0.07040853053331375, -0.08275005966424942, -0.05666271969676018, 0.020652184262871742, 0.0412571094930172, 0.008102000690996647, 0.11241036653518677, 0.007242773659527302, -0.07369647920131683, -0.032727357000112534, -0.11505412310361862, -0.017984218895435333, 0.017296994104981422, -0.05084672197699547, 0.0215557049959898, -0.0450865663588047, -0.05627160519361496, -0.035733502358198166, -0.04239622876048088, -0.09012692421674728, 0.004928783047944307, -0.05911898612976074, 0.029251443222165108, -0.08160188049077988, 0.021778913214802742, 0.04144284874200821, -0.010701669380068779, 0.09186094254255295, -0.04876717925071716, -0.008234839886426926, -0.14319872856140137, -1.3861056302744146e-08, -0.003086996264755726, 0.11216661334037781, -0.025332573801279068, -0.08606592565774918, -0.012375419959425926, 0.02470848336815834, 0.08744502812623978, -0.03808768838644028, -0.15261691808700562, 0.0323701910674572, 0.11994649469852448, -0.059137020260095596, 0.09676478058099747, -0.03178854659199715, 0.021266169846057892, -0.011807886883616447, -0.04668981581926346, -0.019501179456710815, -0.049485817551612854, 0.05348706245422363, -0.03292505070567131, 0.10105352848768234, -0.023853838443756104, 0.0440436489880085, 0.10893604904413223, 0.039163075387477875, 0.06078541651368141, 0.04556327313184738, 0.050687436014413834, -0.023446984589099884, -0.042320575565099716, -0.006845004856586456, 0.020595572888851166, -0.008455146104097366, -0.02445162646472454, -0.013558464124798775, 0.07469150424003601, 0.02600691467523575, -0.03384650498628616, -0.024873461574316025, 0.016474649310112, -0.0994865745306015, 0.04127684608101845, 0.03977108374238014, -0.03729350119829178, 0.008942312560975552, -0.02948015183210373, -0.04936297610402107, -0.06279100477695465, -0.0070610190741717815, -0.033512964844703674, -0.031225943937897682, -0.01788962259888649, 0.08223585039377213, 0.021526874974370003, 0.023469459265470505, -0.08171045780181885, 0.035551782697439194, -0.0478106364607811, -0.043798189610242844, -0.04078318178653717, 0.04554406926035881, -0.01773909665644169, 0.08446462452411652]}}, {'_index': 'my_documents', '_id': 'MhPcpZABMnum9rmLigy5', '_score': None, '_rank': 5, '_ignored': ['content.keyword'], '_source': {'name': 'Open main shoulder cultural occur.', 'url': 'http://www.howard-hines.com/', 'summary': 'Energy sea him magazine employee example.', 'content': 'Break push player chair before yes.\\nTen memory behind create increase. Low relate put sound wonder south site.\\nEvening break state arm including. Learn include list peace traditional get use.\\nRecently here especially dream police region. Call say hour yourself entire camera someone.\\nChoose green many language. Tonight improve address build record walk hold TV.\\nStart yes become main begin rock season itself. Medical sure firm election.\\nDebate drop today address let Mr institution pull. Somebody son window fish court.\\nSound team never two detail including. Because prove mouth home. Per focus top truth be. Free present art enter.\\nCentral ball into beautiful offer particularly modern. Want beat usually miss maybe. Mrs official money say.\\nGrowth care number student realize brother.\\nBoth chair discuss similar free manager. From American president try realize. Identify popular each teach thing.\\nAround more man wear turn.', 'created_on': '2024-03-06T07:34:02.262189', 'category': 'sharepoint', 'rolePermissions': ['guest'], 'embedding': [-0.017831288278102875, 0.10192093253135681, -0.011479153297841549, 0.053047001361846924, 0.008436629548668861, 0.023084145039319992, 0.07241160422563553, -0.024926641955971718, -0.07758407294750214, -0.026874585077166557, 0.002110207686200738, 0.031322114169597626, -0.030673600733280182, 0.01267076376825571, 0.04151970520615578, -0.004582701716572046, 0.07245878875255585, 0.06117023527622223, -0.027851974591612816, -0.04045770317316055, 0.05630055442452431, 0.012829123064875603, 0.056831005960702896, -0.039443813264369965, -0.006821103394031525, -0.02827320247888565, -0.015499277971684933, 0.0704430639743805, -0.00457658339291811, -0.046608831733465195, -0.006762650795280933, -0.04215517267584801, -0.019431937485933304, 0.007267269771546125, 0.0709637925028801, 0.10336420685052872, -0.0378306545317173, 0.12941309809684753, -0.009639658033847809, 0.03672560304403305, -0.054962918162345886, -0.08670802414417267, 0.04577404633164406, -0.022642342373728752, -0.03874070197343826, -0.09411352127790451, 0.0442718006670475, -0.030902793630957603, -0.01560396607965231, 0.02813233807682991, -0.06358788162469864, -0.10685592889785767, 0.045525871217250824, 0.035604652017354965, 0.06359700858592987, -0.01753656007349491, -0.00799083337187767, -0.012475572526454926, 0.004598643165081739, -0.00808338075876236, -0.000731362437363714, -0.11581994593143463, -0.10885781049728394, 0.019562777131795883, 0.12751147150993347, 0.025110842660069466, -0.014306335709989071, 0.09935110062360764, -0.07580386102199554, -0.07888602465391159, 0.05504219979047775, -0.07634776085615158, -0.057340994477272034, 0.04283773899078369, 0.0032829721458256245, -0.027521993964910507, 0.008834253065288067, 0.04154595360159874, 0.08242735266685486, -0.043957531452178955, -0.03460283204913139, -0.0773622989654541, -0.020907966420054436, 0.016263382509350777, 0.04258619248867035, 0.017744194716215134, 0.024371257051825523, -0.07227300107479095, 0.04353634640574455, 0.07708552479743958, -0.08437130600214005, -0.041683390736579895, 0.053525831550359726, -0.0410008579492569, -0.016488177701830864, -0.028853639960289, -0.07459156215190887, 0.00045289951958693564, -0.11772004514932632, 0.05956441909074783, 0.03023390844464302, 0.03453803062438965, 0.014504685066640377, 0.0019523109076544642, -0.004095406271517277, -0.14343971014022827, -0.023685108870267868, 0.06404058635234833, 0.004580971784889698, 0.0248013474047184, -0.0054982732981443405, 0.054617952555418015, -0.1028299406170845, -0.11568200588226318, 0.0645366981625557, -0.027235200628638268, -0.029803471639752388, -0.0159955695271492, -0.03321792930364609, -0.009313657879829407, 0.06411639600992203, 0.04345639422535896, -0.03651311621069908, 0.0639825090765953, -0.07986235618591309, -0.02951071970164776, 0.02296508103609085, -2.599933793235765e-33, 0.0033291333820670843, -0.009750531055033207, 0.061683908104896545, 0.06054402515292168, 0.04827621951699257, 0.050729475915431976, -0.1005687490105629, -0.02572096884250641, 0.06152215600013733, -0.027921972796320915, -0.021801555529236794, 0.14560405910015106, 0.03609954193234444, 0.05486482381820679, -0.08415988832712173, -0.033719293773174286, -0.0666150152683258, 0.09946616739034653, 0.010265369899570942, -0.0415099635720253, 0.059178926050662994, 0.06332563608884811, 0.009137525223195553, -0.005671432241797447, 0.08644255250692368, 0.01916508562862873, 0.006205954123288393, -0.06409585475921631, 0.007098502013832331, 0.09660150855779648, -0.027454914525151253, 0.06114711984992027, 0.002214542357251048, -0.0038823182694613934, -0.03167283535003662, -0.009015154093503952, -0.04470839723944664, -0.01381740067154169, 0.009938322007656097, -0.0048105353489518166, 0.002202983247116208, 0.030648047104477882, 0.05941908434033394, -0.02578279934823513, -0.03487385809421539, 0.03250962123274803, 0.09626565873622894, -0.015609435737133026, 0.0662943497300148, 0.05679839104413986, 0.028552832081913948, 0.005769886076450348, -0.0061758435331285, -0.04664681851863861, 0.0822785273194313, -0.01710824854671955, 0.06662195920944214, 0.009756451472640038, 0.06120377406477928, -0.008498321287333965, 0.021683480590581894, 0.17127159237861633, 0.04024092108011246, 0.04250040277838707, 0.019640089944005013, -0.0022396836429834366, -0.014103500172495842, -0.07401484996080399, -0.054495442658662796, 0.04776325076818466, 0.030141936615109444, 0.006128030829131603, -0.02300911769270897, -0.08044198900461197, -0.058473192155361176, 0.033230021595954895, -0.059840358793735504, 0.005539875943213701, -0.09747245162725449, -0.0015556495636701584, 0.009622500278055668, -0.05991607531905174, 0.07492811977863312, -0.015658846125006676, -0.06401952356100082, 0.02527952753007412, 0.04362925514578819, -0.049400631338357925, 0.04274571314454079, -0.00882956013083458, -0.005671813152730465, -0.0012214758899062872, 0.06290900707244873, 0.05524173378944397, -0.07345544546842575, 4.975791363568932e-34, -0.05958059802651405, -0.024517633020877838, -0.06813713163137436, 0.0221815574914217, 0.10037960112094879, 0.002998208859935403, 0.003205815562978387, -0.03420644998550415, 0.012678510509431362, 0.021799497306346893, -0.0632781982421875, -0.05102843791246414, -0.0387163981795311, -0.005682667717337608, 0.022414712235331535, -0.014371447265148163, 0.059193000197410583, -0.07260334491729736, -0.0746006965637207, 0.03858019784092903, 0.060635603964328766, 0.03459174931049347, -0.01188911683857441, 0.09907155483961105, 0.014273209497332573, 0.009408555924892426, 0.06556818634271622, -0.05717175081372261, -0.027249421924352646, -0.05505356937646866, -0.08857309073209763, 0.03878200799226761, -0.00525048328563571, 0.03741220757365227, -0.10492910444736481, 0.03661775961518288, -0.04211493954062462, 0.017516370862722397, 0.015070457011461258, 0.01423695683479309, 0.06818542629480362, -0.04493651166558266, 0.0013344724429771304, 0.026012690737843513, -0.0657765194773674, -0.04847189784049988, -0.008173099718987942, -0.06384316086769104, -0.04383932426571846, 0.010369107127189636, -0.05507011339068413, -0.037481773644685745, -0.0481434091925621, 0.01507148239761591, -0.07366574555635452, -0.023046204820275307, -0.028224270790815353, -0.033508025109767914, -0.016968613490462303, 0.014263995923101902, -0.0038147433660924435, -0.02984381839632988, 0.04735930263996124, 0.06898216903209686, -0.06353200972080231, -0.044589877128601074, -0.012583550065755844, -0.039457112550735474, -0.04786728695034981, -0.028401335701346397, 0.06804891675710678, -0.019944047555327415, -0.000536089763045311, -0.06768015027046204, -0.02842208556830883, 0.04779590293765068, 0.01587400585412979, -0.029000870883464813, -0.09185879677534103, -0.033468786627054214, -0.04587225988507271, 0.005902127828449011, -0.009613162837922573, 0.0040896181017160416, 0.07539544999599457, 0.01807226613163948, 0.0028709762264043093, -0.07920653373003006, -0.023345671594142914, 0.011753307655453682, 0.049444034695625305, -0.07204385846853256, -0.07338009029626846, 0.09020034223794937, 0.012873939238488674, -1.7490041415157975e-08, -0.005835122894495726, -0.01457956526428461, 0.10637205839157104, -0.013314572162926197, 0.05776771903038025, -0.011147103272378445, 0.030332544818520546, -0.0015766645083203912, 0.06246462091803551, 0.03646467253565788, 0.05235720053315163, 0.006704661063849926, 0.05345405265688896, 0.0021909838542342186, 0.07384000718593597, -0.04195839911699295, 0.01482358481734991, 0.02377752587199211, -0.09715980291366577, -0.037819743156433105, 0.02662937343120575, 0.051198821514844894, 0.028170326724648476, -0.02309301868081093, 0.00455066841095686, 0.08041452616453171, -0.033193085342645645, 0.012320139445364475, 0.09328809380531311, 0.03206520527601242, -0.029463455080986023, 0.05411594733595848, -0.04612484201788902, -0.07212847471237183, -0.003292928682640195, 0.021492954343557358, 0.02561335824429989, 0.021753182634711266, -0.04168619215488434, 0.039617277681827545, -0.03138638660311699, 0.005713155958801508, 0.02427189238369465, 0.06345617026090622, -0.02076547034084797, 0.04766421392560005, -0.07771610468626022, -0.014583689160645008, -0.00919592659920454, 0.06723280996084213, 0.017447300255298615, 0.012064672075212002, 0.05503226816654205, -0.019067782908678055, 0.012131039053201675, 0.024264322593808174, 0.02338176779448986, -0.07083716243505478, -0.11105126887559891, -0.04820403829216957, 0.08975108712911606, -0.06038575991988182, -0.021752478554844856, 0.08261869102716446]}}], 'total': 10, 'aggs': {'Year': {'2023': 5}}}\n"
     ]
    }
   ],
   "source": [
    "results = search.hybrid_search(\"tesla\")\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
