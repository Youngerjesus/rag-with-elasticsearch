{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search\n",
    "\n",
    "This section will introduce you to a different way of searching that leverages Machine Learning (ML) techniques to interpret meaning and context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Embeddings\n",
    "In Machine Learning, an embedding is a vector (an array of numbers) that represents real-world objects such as words, sentences, images or videos. The interesting property that these embeddings have is that two embeddings that represent similar or related real-world entities will share some similarities as well, so embeddings can be compared, and a distance between them can be calculated.\n",
    "\n",
    "When thinking specifically in terms of an application for searching, performing a search of embeddings in the vector space tends to find results that are more related to concepts, instead of to the exact keywords typed in the search prompt.\n",
    "\n",
    "In this section of the tutorial you are going to learn how to generate embeddings using freely available machine learning models, then you will use Elasticsearch's vector database support to store and search these embeddings. And towards the end, you will also combine vector and full-text search results and create a powerful hybrid search solution that offers the best of both approaches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings\n",
    "In this section you are going to learn about one of the most convenient options that are available to generate embeddings for text, which is based on the SentenceTransformers framework.\n",
    "\n",
    "Working with SentenceTransformers is the recommended path while you explore and become familiar with the use of embeddings, as the models that are available under this framework can be installed on your computer, perform reasonably well without a GPU, and are free to use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentenceTransformers\n",
    "\n",
    "SentenceTransformers는 문장이나 텍스트의 의미를 고차원 벡터 공간에 매핑하는 데 사용되는 Python 프레임워크입니다. 이 프레임워크는 주로 자연어 처리(NLP) 작업에 사용되며, 특히 Vector Search와 같은 의미 기반 검색에 매우 유용합니다.\n",
    "\n",
    "기본 개념:\n",
    "- 텍스트를 고정 길이의 dense vector로 변환합니다.\n",
    "- 이 벡터는 텍스트의 의미적 내용을 포착합니다.\n",
    "\n",
    "모델 기반:\n",
    "- 주로 BERT, RoBERTa, XLM-RoBERTa 등의 트랜스포머 모델을 기반으로 합니다.\n",
    "- 다양한 사전 훈련된 모델을 제공합니다.\n",
    "\n",
    "다국어 지원:\n",
    "- 100개 이상의 언어를 지원하는 모델을 포함합니다.\n",
    "\n",
    "사용 예시: \n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install SentenceTransformers\n",
    "The SentenceTransformers framework is installed as a Python package. Make sure that your Python virtual environment is activated, and then run the following command on your terminal to install this framework:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from sentence-transformers) (4.41.2)\n",
      "Requirement already satisfied: tqdm in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: numpy in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from sentence-transformers) (1.26.4)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.5.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (12 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from sentence-transformers) (0.23.4)\n",
      "Requirement already satisfied: Pillow in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Using cached sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "Downloading scikit_learn-1.5.1-cp39-cp39-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sentence-transformers\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.1 scipy-1.13.1 sentence-transformers-3.0.1 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a Model\n",
    "The next task is to decide on a machine learning model to use for embedding generation. There is a list of pretrained models in the documentation. Because SentenceTransformers is a very popular framework, there are also compatible models created by researchers not directly associated with the framework. To see a complete list of models that can be used, you can check the SentenceTransformers tag on HuggingFace.\n",
    "\n",
    "For the purposes of this tutorial there is no need to overthink the model selection, as any model will suffice. The SentenceTransformers documentation includes the following note with regards to their pretrained models:\n",
    "\n",
    "The all-* models where trained on all available training data (more than 1 billion training pairs) and are designed as general purpose models. The all-mpnet-base-v2 model provides the best quality, while all-MiniLM-L6-v2 is 5 times faster and still offers good quality.\n",
    "\n",
    "This seems to suggest that their all-MiniLM-L6-v2 model is a good choice that offers a good compromise between speed and quality, so let's use this model. Locate this model in the table, and click the \"info\" icon to see some information about it.\n",
    "\n",
    "An interesting detail that is good to be aware of about your chosen model is the length the generated embeddings have, or in other words, how many numbers or dimensions the resulting vectors will have. This is important because it directly affects the amount of storage you will need. In the case of all-MiniLM-L6-v2, the generated vectors have 384 dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model\n",
    "The following Python code demonstrates how the model is loaded. You can try this in a Python shell.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/jeongmin/PycharmProjects/python-skills-bootcamp/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time you do this, the model will be downloaded and installed in your virtual environment, so the call may take some time to return. Once the model is installed, instantiating it should not take long.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings\n",
    "With the model instantiated, you are now ready to generate an embedding. To do this, pass the source text to the model.encode() method:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.encode('The quick brown fox jumps over the lazy dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.54968011e-02,  6.12862706e-02,  5.26920557e-02,  7.07050413e-02,\n",
       "        3.31013948e-02, -3.06696109e-02,  6.62061898e-03, -6.11832850e-02,\n",
       "       -1.32602104e-03,  1.06456643e-02,  3.86499353e-02,  3.99531797e-02,\n",
       "       -3.83675545e-02, -1.66687984e-02, -5.61561156e-03, -2.43558995e-02,\n",
       "       -3.59968394e-02, -3.02429274e-02,  5.84700331e-02, -4.94961701e-02,\n",
       "       -7.72954598e-02, -5.23876920e-02,  2.45271791e-02,  2.93105822e-02,\n",
       "       -7.39091635e-02, -2.49592010e-02, -6.53142035e-02, -4.28864807e-02,\n",
       "        7.11656362e-02, -1.13819472e-01, -1.26593364e-02,  3.96260805e-02,\n",
       "       -2.10036244e-02,  1.78064071e-02, -3.18874270e-02, -9.11229402e-02,\n",
       "        5.91224581e-02, -7.30397413e-03,  3.31367701e-02,  2.99061127e-02,\n",
       "        4.21688668e-02, -1.69129800e-02, -4.50015739e-02,  2.96744388e-02,\n",
       "       -9.92584750e-02,  5.32891788e-02, -7.64784738e-02, -1.48679931e-02,\n",
       "        1.52494945e-02,  1.37893632e-02, -4.41923775e-02, -2.78393030e-02,\n",
       "        6.73079677e-03,  5.64969927e-02,  7.21781850e-02, -4.12064092e-03,\n",
       "       -3.77659616e-03, -3.55087370e-02,  4.90683950e-02, -1.03429845e-02,\n",
       "        2.36084294e-02,  3.63824107e-02,  1.80067439e-02, -9.42732149e-04,\n",
       "        3.87706645e-02,  2.31451318e-02, -2.71658991e-02, -8.00189674e-02,\n",
       "       -9.76722687e-02,  3.99069861e-03,  1.36213414e-02, -4.74255942e-02,\n",
       "       -1.67798586e-02, -9.50407144e-03,  4.89121722e-03, -2.80310176e-02,\n",
       "        5.52376173e-02, -5.92487194e-02,  6.14453927e-02,  3.54701141e-03,\n",
       "       -2.98315510e-02, -5.49717359e-02, -5.29682450e-02,  4.70336266e-02,\n",
       "        3.43414098e-02,  5.52391959e-03,  2.80624460e-02,  3.03138662e-02,\n",
       "       -1.43292733e-02, -3.52453962e-02, -2.86585912e-02, -6.23138398e-02,\n",
       "       -4.20150645e-02,  2.44777668e-02,  5.53563982e-03,  8.14049505e-03,\n",
       "        1.53732244e-02, -4.85228077e-02, -6.48295283e-02,  2.46882383e-02,\n",
       "        1.49867628e-02,  1.80066098e-02,  1.23578578e-01,  2.14027166e-02,\n",
       "       -1.67560428e-02, -4.69397977e-02,  5.99938957e-03,  8.19533411e-03,\n",
       "        9.56789255e-02,  2.58209594e-02, -1.20125134e-02, -5.72569715e-03,\n",
       "       -8.57408531e-03,  1.05052933e-01,  2.76331566e-02,  8.67517665e-03,\n",
       "       -6.76575303e-02, -2.67715380e-02, -4.06814069e-02, -1.03794850e-01,\n",
       "        7.66287893e-02,  1.26355812e-01, -8.59397203e-02,  1.20138340e-02,\n",
       "       -2.57108063e-02, -5.09863645e-02, -3.28317918e-02, -2.02578727e-33,\n",
       "        7.33251795e-02, -2.40866486e-02, -8.00556242e-02, -6.78968057e-02,\n",
       "       -5.16041443e-02, -7.83167183e-02, -1.33349467e-02, -2.67809909e-02,\n",
       "       -2.50325222e-02,  4.69421595e-02, -7.37413913e-02, -2.62129324e-04,\n",
       "        1.30887171e-02, -3.09572518e-02, -2.00150553e-02, -1.16042152e-01,\n",
       "        2.14620028e-03, -1.27644120e-02,  2.96524931e-02,  5.50505035e-02,\n",
       "        3.08322776e-02,  1.05993316e-01, -3.80328223e-02, -2.74119228e-02,\n",
       "        5.24591953e-02, -2.05130354e-02, -7.18794763e-02, -3.37742902e-02,\n",
       "       -1.51276672e-02,  4.96650226e-02, -4.12642807e-02, -4.23065946e-02,\n",
       "       -4.00017314e-02,  9.03002471e-02, -2.37208940e-02, -1.30580321e-01,\n",
       "        6.22314066e-02, -5.70305772e-02, -3.23460624e-02,  6.05499744e-02,\n",
       "       -6.04436081e-03,  1.40458988e-02,  3.24462727e-02,  2.66418327e-02,\n",
       "       -6.91086650e-02, -1.00011355e-03,  2.81587839e-02,  1.46810878e-02,\n",
       "       -1.64453013e-04,  2.58034449e-02, -2.66302042e-02,  1.57559905e-02,\n",
       "        5.38726561e-02, -5.33531122e-02, -5.54367937e-02,  9.00570676e-02,\n",
       "        7.70272985e-02, -2.44424567e-02, -3.47368196e-02,  9.82798636e-02,\n",
       "        3.04498225e-02, -2.00087689e-02,  4.53980733e-03, -4.74198498e-02,\n",
       "        1.42640695e-01, -6.86056241e-02, -8.13757703e-02,  1.05308800e-03,\n",
       "       -1.78350881e-02,  7.29835406e-02,  1.64816715e-02,  3.98400053e-02,\n",
       "        4.68362644e-02, -1.44534960e-01,  4.02424373e-02, -3.14182155e-02,\n",
       "        1.53545868e-02, -3.36852893e-02,  3.83163020e-02, -2.92718969e-02,\n",
       "        1.20120727e-01, -8.05278793e-02, -4.77892756e-02,  4.57337126e-02,\n",
       "       -2.07783915e-02,  6.10832982e-02,  7.39166327e-03,  1.99949238e-02,\n",
       "       -1.49614522e-02, -3.89547087e-02, -4.93791997e-02, -8.07103608e-03,\n",
       "        4.91255037e-02, -4.90615815e-02,  6.87639490e-02,  1.05193116e-33,\n",
       "        9.64529663e-02, -4.49997336e-02,  7.08592236e-02,  7.01550096e-02,\n",
       "       -3.07358764e-02,  5.32288365e-02, -7.13494280e-03,  4.52578254e-02,\n",
       "       -7.71549195e-02,  6.13044575e-02, -2.57572345e-02,  8.33091699e-03,\n",
       "       -1.65870227e-03, -4.16173687e-04,  1.13713190e-01, -2.59553781e-04,\n",
       "        6.54771924e-02, -6.39371714e-03,  2.79586669e-02,  1.51046263e-02,\n",
       "       -4.68892045e-02,  3.95999067e-02, -1.86009500e-02,  6.94512874e-02,\n",
       "        3.29815932e-02,  5.68617955e-02,  8.76662508e-02, -2.53197793e-02,\n",
       "       -4.36838344e-02, -1.03877187e-01, -5.24876602e-02, -5.71490116e-02,\n",
       "       -1.11060664e-02, -4.67860252e-02,  1.87631398e-02,  4.78794649e-02,\n",
       "       -4.17945981e-02, -6.59293821e-03, -2.18464565e-02, -8.24298114e-02,\n",
       "        3.08676343e-02, -1.24090922e-03,  2.34952662e-02,  7.13226423e-02,\n",
       "        2.72879377e-02,  3.08869872e-03, -5.66032231e-02,  4.98435088e-02,\n",
       "       -3.76918651e-02,  6.29746541e-02, -3.45266517e-03,  3.84236537e-02,\n",
       "        3.93796787e-02,  2.76155323e-02, -4.96771075e-02, -5.40543720e-02,\n",
       "        4.65717539e-03, -4.01742086e-02,  3.90536711e-02, -1.10563887e-02,\n",
       "        8.10949598e-03,  2.47772057e-02, -1.24726193e-02, -3.20834829e-03,\n",
       "       -6.75000669e-03, -8.95393342e-02, -7.46335313e-02, -5.39297946e-02,\n",
       "        7.71142319e-02, -7.48031959e-02, -5.91213070e-03,  3.00307330e-02,\n",
       "        9.53919627e-03, -7.08926022e-02,  9.31638945e-03,  7.84344673e-02,\n",
       "        1.10272020e-01,  4.93478356e-03,  7.26145059e-02, -3.91793624e-02,\n",
       "        1.15645919e-02, -1.69644225e-02, -1.54852110e-03,  1.13657378e-02,\n",
       "       -6.91898316e-02,  3.62798236e-02, -1.15797043e-01,  7.05056340e-02,\n",
       "        4.28795479e-02, -6.56523332e-02,  2.57525016e-02,  9.05412138e-02,\n",
       "        5.89157790e-02,  8.48691687e-02, -1.29126860e-02, -1.76108035e-08,\n",
       "       -5.10395542e-02,  1.34697203e-02, -9.77618247e-02,  4.43888083e-02,\n",
       "        8.00856501e-02,  2.05735713e-02, -3.20180431e-02,  1.20612327e-02,\n",
       "        8.37345198e-02, -3.04363370e-02,  3.55386697e-02,  2.50448193e-02,\n",
       "        5.86509667e-02,  4.10656035e-02, -2.28329021e-02,  1.78446341e-02,\n",
       "       -3.64020541e-02,  1.02113262e-02,  2.88050883e-02,  1.61286622e-01,\n",
       "       -4.23883880e-03, -5.56772612e-02, -1.09122684e-02, -2.70623937e-02,\n",
       "       -5.23658805e-02, -3.65737788e-02, -8.47721621e-02,  5.52399177e-03,\n",
       "       -3.13450210e-02,  1.30554419e-02, -5.08419797e-02,  9.68923792e-02,\n",
       "       -8.70660767e-02,  8.64324858e-04,  3.43630686e-02,  3.16394307e-02,\n",
       "        1.01861604e-01, -9.79756238e-04,  2.66416874e-02,  8.03417061e-03,\n",
       "        8.95350333e-03,  3.50238048e-02, -2.04781555e-02, -7.34798471e-03,\n",
       "       -7.61493742e-02, -6.33014878e-03, -3.11222579e-02, -1.02514505e-01,\n",
       "        7.49528930e-02, -5.15709221e-02, -4.73833345e-02, -4.23636027e-02,\n",
       "        4.27946597e-02,  6.56187385e-02, -4.99790646e-02,  1.02580467e-03,\n",
       "       -5.40309353e-03, -6.54073581e-02, -4.58586924e-02,  3.61347534e-02,\n",
       "        6.25733212e-02,  5.46831712e-02,  5.38233444e-02,  8.67674053e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an array with all the numbers that make up the embedding. As you recall, the embeddings generated by the chosen model have 384 dimensions, so this is the length of the embedding array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Embeddings in Elasticsearch\n",
    "Elasticsearch provides full support for storing and retrieving vectors, which makes it an ideal database for working with embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field Types\n",
    "In the Full-Text Search chapter of this tutorial you have learned how to create an index with several fields. At that time it was mentioned that Elasticsearch can, for the most part, automatically determine the best type to use for each field based on the data itself. Even though Elasticsearch 8.11 is able to automatically map some vector types, in this chapter you will define this type explicitly as an opportunity to learn more about type mappings in Elasticsearch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Type Mappings\n",
    "The types associated with each field in an index are determined in a process called mapping, which can be dynamic or explicit. The mappings that were created in the Full-Text Search portion of this tutorial were all dynamically generated by Elasticsearch.\n",
    "\n",
    "The Elasticsearch client offers a get_mapping method, which returns the type mappings that are in effect for a given index. If you want to explore these mappings on your own, start a Python shell and enter the following code:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "class Search:\n",
    "    def __init__(self):\n",
    "        self.es = Elasticsearch('http://localhost:9200')\n",
    "        client_info = self.es.info()\n",
    "        print('Connected to Elasticsearch!')\n",
    "        pprint(client_info.body)\n",
    "\n",
    "    def create_index(self):\n",
    "        self.es.indices.delete(index='my_documents', ignore_unavailable=True)\n",
    "        self.es.indices.create(index='my_documents', mappings={\n",
    "            'properties': {\n",
    "                'embedding': {\n",
    "                    'type': 'dense_vector',\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    def insert_documents(self, documents):\n",
    "        operations = []\n",
    "        for document in documents:\n",
    "            operations.append({'index': {'_index': 'my_documents'}})\n",
    "            operations.append(document)\n",
    "        return self.es.bulk(operations=operations)\n",
    "\n",
    "    def reindex(self):\n",
    "        self.create_index()\n",
    "        with open('data.json', 'rt') as f:\n",
    "            documents = json.loads(f.read())\n",
    "        return self.insert_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense vector field type\n",
    "\n",
    "정의:\n",
    "- dense_vector 필드 타입은 숫자 값의 밀집 벡터를 저장합니다.\n",
    "- 주로 k-최근접 이웃(k-nearest neighbor, kNN) 검색에 사용됩니다.\n",
    "\n",
    "\n",
    "제한사항:\n",
    "- 이 필드 타입은 집계(aggregations)나 정렬(sorting)을 지원하지 않습니다.\n",
    "\n",
    "\n",
    "구조:\n",
    "- 숫자 값의 배열로 저장됩니다.\n",
    "- 기본적으로 float 타입의 element_type을 사용합니다.\n",
    "\n",
    "매핑 예시:\n",
    "```json\n",
    "\"my_vector\": {\n",
    "  \"type\": \"dense_vector\",\n",
    "  \"dims\": 3\n",
    "}\n",
    "```\n",
    "\n",
    "- \"dims\": 3은 이 벡터가 3차원임을 나타냅니다.\n",
    "\n",
    "사용 예시:\n",
    "- 인덱스 생성 및 매핑 정의:\n",
    "\n",
    "```json\n",
    "PUT my-index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"my_vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 3\n",
    "      },\n",
    "      \"my_text\" : {\n",
    "        \"type\" : \"keyword\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "문서 추가:\n",
    "```json\n",
    "PUT my-index/_doc/1\n",
    "{\n",
    "  \"my_text\" : \"text1\",\n",
    "  \"my_vector\" : [0.5, 10, 6]\n",
    "}\n",
    "\n",
    "PUT my-index/_doc/2\n",
    "{\n",
    "  \"my_text\" : \"text2\",\n",
    "  \"my_vector\" : [-0.5, 10, 10]\n",
    "}\n",
    "```\n",
    "\n",
    "주의사항:\n",
    "- 벡터의 차원 수(dims)는 매핑 시 명시적으로 정의해야 하며, 문서마다 일관된 차원 수를 유지해야 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index vectors for kNN search\n",
    "\n",
    "kNN 검색 정의:\n",
    "- 쿼리 벡터와 가장 유사한 k개의 벡터를 찾는 검색 방법입니다.\n",
    "\n",
    "Dense vector 필드의 사용:\n",
    "- script_score 쿼리에서 문서 랭킹에 사용될 수 있습니다.\n",
    "- 모든 문서를 스캔하는 브루트 포스 kNN 검색이 가능합니다.\n",
    "\n",
    "효율적인 kNN 검색:\n",
    "- 브루트 포스 방식은 비효율적일 수 있어, 특수한 데이터 구조로 벡터를 인덱싱합니다.\n",
    "- search API의 knn 옵션을 통해 빠른 kNN 검색을 지원합니다.\n",
    "\n",
    "동적 매핑:\n",
    "- 128에서 4096 사이의 크기를 가진 float 배열은 자동으로 dense_vector로 매핑됩니다.\n",
    "- 기본 유사도 메트릭은 코사인 유사도입니다\n",
    "\n",
    "인덱싱 비용:\n",
    "- 벡터 인덱싱은 비용이 많이 드는 작업입니다.\n",
    "- 인덱싱이 활성화된 벡터 필드가 있는 문서의 색인 생성에 상당한 시간이 걸릴 수 있습니다.\n",
    "\n",
    "인덱싱 설정:\n",
    "- 기본적으로 int8_hnsw로 인덱싱됩니다.\n",
    "- 유사도 메트릭을 지정할 수 있습니다 (예: dot_product).\n",
    "\n",
    "```json \n",
    "PUT my-index-2\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"my_vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 3,\n",
    "        \"similarity\": \"dot_product\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "인덱싱 비활성화:\n",
    "- index 파라미터를 false로 설정하여 인덱싱을 비활성화할 수 있습니다.\n",
    "\n",
    "```json\n",
    "PUT my-index-2\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"my_vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 3,\n",
    "        \"index\": false\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "HNSW 알고리즘:\n",
    "- Elasticsearch는 효율적인 kNN 검색을 위해 HNSW(Hierarchical Navigable Small World) 알고리즘을 사용합니다.\n",
    "- 이는 근사 방법으로, 결과의 정확성을 일부 희생하여 검색 속도를 향상시킵니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically quantize vectors for kNN search\n",
    "\n",
    "자동 벡터 양자화의 목적:\n",
    "- float 벡터를 검색할 때 필요한 메모리 사용량을 줄이기 위해 사용됩니다.\n",
    "- 원래는 dense vector 에서 각 벡터당 float 이니까 4바이트 씀. 여기서는 int8_hnsw 를 써서 1바이트로 줄인거고\n",
    "\n",
    "\n",
    "\n",
    "지원되는 양자화 방법:\n",
    "- 현재는 int8 양자화만 지원됩니다.\n",
    "- 입력 벡터의 element_type은 반드시 float이어야 합니다.\n",
    "\n",
    "int8_hnsw 인덱스의 특징:\n",
    "- 각 float 벡터의 차원을 1바이트 정수로 양자화합니다.\n",
    "- 메모리 사용량을 최대 75%까지 줄일 수 있습니다.\n",
    "- 정확도가 약간 감소할 수 있습니다.\n",
    "- 디스크 사용량은 양자화된 벡터와 원본 벡터를 모두 저장하기 때문에 25% 정도 증가할 수 있습니다.\n",
    "\n",
    "```json\n",
    "PUT my-byte-quantized-index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"my_vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 3,\n",
    "        \"index\": true,\n",
    "        \"index_options\": {\n",
    "          \"type\": \"int8_hnsw\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Embeddings to Documents\n",
    "In the previous section you have learned how to generate embeddings using the SentenceTransformers framework and the all-MiniLM-L6-v2 model. Now it is time to integrate the model into the application.\n",
    "\n",
    "First of all, the model can be instantiated in the Search class constructor:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "class Search:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.es = Elasticsearch('http://localhost:9200')\n",
    "        client_info = self.es.info()\n",
    "        print('Connected to Elasticsearch!')\n",
    "        pprint(client_info.body)\n",
    "\n",
    "    def create_index(self):\n",
    "        self.es.indices.delete(index='my_documents', ignore_unavailable=True)\n",
    "        self.es.indices.create(index='my_documents', mappings={\n",
    "            'properties': {\n",
    "                'embedding': {\n",
    "                    'type': 'dense_vector',\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    def insert_documents(self, documents):\n",
    "        operations = []\n",
    "        for document in documents:\n",
    "            operations.append({'index': {'_index': 'my_documents'}})\n",
    "            operations.append(document)\n",
    "        return self.es.bulk(operations=operations)\n",
    "\n",
    "    def reindex(self):\n",
    "        self.create_index()\n",
    "        with open('data.json', 'rt') as f:\n",
    "            documents = json.loads(f.read())\n",
    "        return self.insert_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you recall from the full-text search portion of this tutorial, the Search class has insert_document() and insert_documents() methods, to insert single and multiple documents into the index respectively. These two methods now need to generate the corresponding embeddings that go with each document.\n",
    "\n",
    "The next code block shows new versions of these two methods, along with a new get_embedding() helper method that returns an embedding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "class Search:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.es = Elasticsearch('http://localhost:9200')\n",
    "        client_info = self.es.info()\n",
    "        print('Connected to Elasticsearch!')\n",
    "        pprint(client_info.body)\n",
    "\n",
    "    def create_index(self):\n",
    "        self.es.indices.delete(index='my_documents', ignore_unavailable=True)\n",
    "        self.es.indices.create(index='my_documents', mappings={\n",
    "            'properties': {\n",
    "                'embedding': {\n",
    "                    'type': 'dense_vector',\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    def insert_document(self, document):\n",
    "        return self.es.index(index='my_documents', document={\n",
    "            **document,\n",
    "            'embedding': self.get_embedding(document['summary']),\n",
    "        })\n",
    "    \n",
    "    def insert_documents(self, documents):\n",
    "        operations = []\n",
    "        for document in documents:\n",
    "            operations.append({'index': {'_index': 'my_documents'}})\n",
    "            operations.append({\n",
    "                **document,\n",
    "                'embedding': self.get_embedding(document['summary']),\n",
    "            })\n",
    "        return self.es.bulk(operations=operations)\n",
    "\n",
    "\n",
    "    def reindex(self):\n",
    "        self.create_index()\n",
    "        with open('data.json', 'rt') as f:\n",
    "            documents = json.loads(f.read())\n",
    "        return self.insert_documents(documents)\n",
    "    \n",
    "    def get_embedding(self, text):\n",
    "        return self.model.encode(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modified methods add the new embedding field to the document to be inserted. The embedding is generated from the summary field of each document. In general, embeddings are generated from sentences or short paragraphs, so in this case the summary is an ideal field to use. Other options would have been the name field, which contains the title of the document, or maybe the first few sentences from the document's body.\n",
    "\n",
    "With these changes in place the index can be rebuilt, so that it stores an embedding for each document. To rebuilt the index, use this command:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbor (kNN) Search\n",
    "\n",
    "The k-nearest neighbor (kNN) algorithm performs a similarity search on fields of dense_vector type. This type of search, which is more appropriately called \"approximate kNN\", accepts a vector or embedding as a search term, and finds entries in the index that are close.\n",
    "\n",
    "In this section you are going to learn how to run a kNN search using the document embeddings created in the previous section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The knn Query\n",
    "In the full-text search section of the tutorial you learned about the query option passed to the search() method of the Elasticsearch client. When searching vectors, the knn option is used instead.\n",
    "\n",
    "Below you can see a new version of the handle_search() function in app.py that runs a kNN search for the query entered by the user in the search form.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest neighbor (kNN) search\n",
    "\n",
    "kNN 검색 정의:\n",
    "- 쿼리 벡터와 가장 유사한 k개의 벡터를 찾는 검색 방법입니다.\n",
    "- 유사도 메트릭을 사용하여 측정합니다.\n",
    "\n",
    "\n",
    "kNN의 일반적인 사용 사례:\n",
    "- 자연어 처리(NLP) 알고리즘 기반의 관련성 랭킹\n",
    "- 제품 추천 및 추천 엔진\n",
    "- 이미지나 비디오의 유사도 검색\n",
    "\n",
    "\n",
    "전제 조건:\n",
    "- 데이터를 의미 있는 벡터 값으로 변환할 수 있어야 합니다.\n",
    "- 벡터는 Elasticsearch 내부의 NLP 모델을 사용하거나 외부에서 생성할 수 있습니다.\n",
    "- 문서에 dense_vector 필드 값으로 벡터를 추가합니다.\n",
    "- 쿼리도 동일한 차원의 벡터로 표현됩니다.\n",
    "\n",
    "\n",
    "벡터 설계 원칙:\n",
    "- 유사도 메트릭 기준으로 문서의 벡터가 쿼리 벡터에 가까울수록 더 좋은 매치가 되도록 설계해야 합니다.\n",
    "\n",
    "\n",
    "필요한 인덱스 권한:\n",
    "- create_index 또는 manage: dense_vector 필드를 가진 인덱스 생성\n",
    "- create, index, 또는 write: 생성한 인덱스에 데이터 추가\n",
    "- read: 인덱스 검색\n",
    "\n",
    "\n",
    "중요한 포인트:\n",
    "- kNN 검색은 벡터 공간에서의 유사도를 기반으로 합니다.\n",
    "- 효과적인 kNN 검색을 위해서는 데이터를 적절한 벡터 표현으로 변환하는 것이 중요합니다.\n",
    "- 벡터의 차원과 유사도 메트릭은 검색 성능과 정확도에 큰 영향을 미칩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벡터 유사도 메트릭(similarity metric)\n",
    "\n",
    "Dense Vector Field 에서 지정할 수 있는 유사도 검색 파라미터에 대해 알아보자. \n",
    "\n",
    "similarity 파라미터:\n",
    "- kNN 검색에서 사용할 벡터 유사도 메트릭을 지정합니다.\n",
    "- 옵션이지만, index가 true일 때만 지정할 수 있습니다.\n",
    "- 기본값은 cosine입니다\n",
    "\n",
    "l2_norm (L2 거리, 유클리드 거리):\n",
    "- 벡터 간의 절대적인 거리가 중요할 때\n",
    "- 데이터 포인트의 실제 위치나 크기가 의미가 있을 때 \n",
    "- 고차원 데이터에는 유용하지 않음. \n",
    "- 이상치 탐지나 클러스터링 작업에서 유용 (데이터 포인트간의 거리 차이로 이상치를 탐색함.)\n",
    "\n",
    "dot_product: \n",
    "- 모든 벡터가 이미 정규화되어 있을 때 (cosine 유사도의 최적화 버전)\n",
    "- 대규모 데이터셋에서는 계산 효율성이 중요할 수 있으며, 이 경우 dot_product가 유리할 수 있습니다.\n",
    "\n",
    "dot_product (내적): 사용을 피해야 할 경우:\n",
    "- 벡터가 정규화되어 있지 않을 때 (길이가 1이 아닌 경우)\n",
    "- 벡터의 크기 차이가 결과에 영향을 주면 안 될 때\n",
    "- 음수 값을 포함한 데이터에서 유사도의 해석이 중요할 때\n",
    "- 데이터의 스케일이 일정하지 않은 경우\n",
    "- 예: 대규모 텍스트 코퍼스에서의 빠른 유사도 검색, 임베딩 벡터를 사용한 단어 유사성 비교\n",
    "\n",
    "\n",
    "cosine (코사인 유사도):\n",
    "- 벡터의 방향이 중요하고 크기는 덜 중요할 때\n",
    "- 텍스트 문서의 유사성을 비교할 때\n",
    "- 고차원 데이터에서 작동이 잘 될 때\n",
    "- 예시: 문서 유사도 검색, 추천 시스템 자연어 처리 작업\n",
    "\n",
    "cosine (코사인 유사도): 사용을 피해야 할 경우:\n",
    "- 벡터의 크기(magnitude)가 중요한 의미를 가질 때\n",
    "- 음수 값을 포함한 데이터의 유사도를 비교할 때 (코사인 유사도는 양수 공간에서만 의미가 있음)\n",
    "- 벡터의 절대적 위치가 중요한 경우 (예: 지리적 위치 데이터)\n",
    "- 영벡터(모든 요소가 0인 벡터)를 포함한 데이터셋\n",
    "\n",
    "max_inner_product:\n",
    "- 벡터의 방향과 크기가 중요한 의미를 가질 때\n",
    "- 정규화되지 않은 벡터를 다룰 때\n",
    "- 양수와 음수 값을 모두 고려해야 할 때 \n",
    "- 예: 추천 시스템에서 사용자-아이템 상호작용 강도를 고려할 때, 특성의 중요도가 서로 다른 기계학습 모델의 결과를 비교할 때\n",
    "\n",
    "\n",
    "데이터의 특성: 데이터가 방향만 중요한지, 절대적 크기도 중요한지 고려합니다.\n",
    "정규화 여부: 데이터가 이미 정규화되어 있다면 dot_product나 cosine이 효율적일 수 있습니다.\n",
    "계산 효율성: 대규모 데이터셋에서는 계산 효율성이 중요할 수 있으며, 이 경우 dot_product가 유리할 수 있습니다.\n",
    "벡터 값 범위: 음수 지원하는지 \n",
    "\n",
    "dot_product vs cosine \n",
    "- 둘 다 벡터의 방향이 더 중요\n",
    "- cosine 은 -1 ~ 1 까지 지원, dot_product 는 양수만 지원. 그래서 cosine 이 더 넓은 유사도 범위를 할 수 있음. \n",
    "- dot_product 는 cosine 보다 계산이 간단해서 대규모 데이터에서 더 빠름. \n",
    "- dot_product 는 정규화 벡터에서만 동작. cosine 은 알아서 정규화해줌. \n",
    "\n",
    "\n",
    "max_inner_product vs l2_norm \n",
    "- max_inner_product 는 벡터의 방향과 크기도 고려하는 반면에, l2_norm 은 순수 거리만을 측정한다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Methods \n",
    "\n",
    "Elasticsearch의 두 가지 kNN 검색 방법: \n",
    "- Approximate kNN: \n",
    "  - 낮은 지연 시간을 제공하지만, 인덱싱이 느리고 완벽한 정확도를 보장하지 않습니다.\n",
    "\n",
    "- brute-force kNN:  \n",
    "  - 정확한 결과를 보장하지만, 대규모 데이터셋에서는 확장성이 떨어집니다.\n",
    "\n",
    "사용 시 고려사항:\n",
    "- 데이터셋의 크기\n",
    "- 요구되는 정확도 수준\n",
    "- 허용 가능한 검색 지연 시간\n",
    "- 인덱싱 속도의 중요성\n",
    "\n",
    "브루트 포스 kNN의 활용:\n",
    "- 데이터를 작은 부분집합으로 필터링할 수 있는 경우, 이 방법으로도 좋은 검색 성능을 얻을 수 있습니다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate kNN\n",
    "\n",
    "리소스 요구사항: \n",
    "- 근사 kNN 검색은 특별한 리소스 요구사항이 있습니다.\n",
    "- 모든 벡터 데이터가 노드의 페이지 캐시에 맞아야 효율적입니다.\n",
    "- 구성 및 크기 조정에 대한 튜닝 가이드를 참조하는 것이 중요합니다.\n",
    "\n",
    "kNN 쿼리 실행: \n",
    "- 'knn' 옵션을 사용하여 검색을 실행합니다.\n",
    "- 검색 파라미터로는 field, query_vector, k, num_candidates 등이 있습니다.\n",
    "- 문서의 _score는 쿼리 벡터와 문서 벡터 간의 유사도에 의해 결정됩니다.\n",
    "\n",
    "```json\n",
    "POST image-index/_search\n",
    "{\n",
    "  \"knn\": {\n",
    "    \"field\": \"image-vector\",\n",
    "    \"query_vector\": [-5, 9, -12],\n",
    "    \"k\": 10,\n",
    "    \"num_candidates\": 100\n",
    "  },\n",
    "  \"fields\": [ \"title\", \"file-type\" ]\n",
    "}\n",
    "\n",
    "``` \n",
    "\n",
    "매핑 요구사항:\n",
    "- dense_vector 필드를 명시적으로 매핑해야 합니다.\n",
    "- similarity 값을 지정해야 합니다 (기본값은 cosine).\n",
    "- 예시에서는 'l2_norm' 유사도 메트릭을 사용합니다.\n",
    "\n",
    "```json\n",
    "PUT image-index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"image-vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 3,\n",
    "        \"similarity\": \"l2_norm\"\n",
    "      },\n",
    "      \"title-vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 5,\n",
    "        \"similarity\": \"l2_norm\"\n",
    "      },\n",
    "      \"title\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"file-type\": {\n",
    "        \"type\": \"keyword\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "버전 호환성:\n",
    "- 근사 kNN 검색 지원은 Elasticsearch 8.0 버전에서 추가되었습니다.\n",
    "- 8.0 이전 버전에서 생성된 인덱스의 경우, 근사 kNN 검색을 지원하려면 index: true 옵션으로 데이터를 재인덱싱해야 합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune approximate kNN for speed or accuracy\n",
    "\n",
    "kNN 검색 과정:\n",
    "- 각 샤드에서 'num_candidates' 수만큼의 근사 최근접 이웃 후보를 찾습니다.\n",
    "- 이 후보 벡터들과 쿼리 벡터 간의 유사도를 계산합니다.\n",
    "- 각 샤드에서 가장 유사한 k개의 결과를 선택합니다.\n",
    "- 모든 샤드의 결과를 병합하여 전체 상위 k개의 최근접 이웃을 반환합니다.\n",
    "\n",
    "num_candidates 파라미터의 역할:\n",
    "- 이 파라미터는 정확도와 검색 속도 사이의 균형을 조절합니다.\n",
    "\n",
    "\n",
    "num_candidates 값 증가:\n",
    "- 장점: 더 정확한 결과를 얻을 수 있습니다.\n",
    "- 단점: 검색 속도가 느려집니다.\n",
    "- 이유: 각 샤드에서 더 많은 후보를 고려하므로, 진정한 상위 k개 최근접 이웃을 찾을 확률이 높아집니다.\n",
    "\n",
    "\n",
    "num_candidates 값 감소:\n",
    "- 장점: 검색 속도가 빨라집니다.\n",
    "- 단점: 결과의 정확도가 떨어질 수 있습니다.\n",
    "- 이유: 각 샤드에서 고려하는 후보 수가 줄어들어 처리 시간이 단축됩니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate kNN using byte vectors\n",
    "\n",
    "바이트 벡터 지원:\n",
    "- 근사 kNN 검색 API는 float 값 벡터 외에도 바이트 값 벡터를 지원합니다.\n",
    "- dense_vector 필드에 element_type을 'byte'로 설정하고 인덱싱을 활성화해야 합니다.\n",
    "\n",
    "```json \n",
    "PUT byte-image-index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"byte-image-vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"element_type\": \"byte\",\n",
    "        \"dims\": 2\n",
    "      },\n",
    "      \"title\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "데이터 인덱싱:\n",
    "- 모든 벡터 값은 -128에서 127 사이의 정수여야 합니다.\n",
    "\n",
    "주요 포인트:\n",
    "- 바이트 벡터는 float 벡터에 비해 메모리 사용량을 줄일 수 있습니다.\n",
    "- 값의 범위가 제한되어 있으므로(-128에서 127), 이 범위 내에서 데이터를 적절히 정규화해야 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte quantized kNN search \n",
    "\n",
    "바이트 양자화의 목적:\n",
    "- float 벡터를 제공하면서도 바이트 벡터의 메모리 절약 효과를 얻을 수 있습니다.\n",
    "- float 벡터를 내부적으로 바이트 벡터로 인덱싱하지만, 원본 float 벡터도 인덱스에 유지합니다.\n",
    "\n",
    "기본 인덱스 타입:\n",
    "- dense_vector의 기본 인덱스 타입은 'int8_hnsw'입니다.\n",
    "\n",
    "인덱스 매핑:\n",
    "- dense_vector 필드에 'int8_hnsw' 인덱스 타입을 지정합니다.\n",
    "- element_type은 'float'으로 설정합니다.\n",
    "\n",
    "```json \n",
    "PUT quantized-image-index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"image-vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"element_type\": \"float\",\n",
    "        \"dims\": 2,\n",
    "        \"index\": true,\n",
    "        \"index_options\": {\n",
    "          \"type\": \"int8_hnsw\"\n",
    "        }\n",
    "      },\n",
    "      \"title\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "'knn' 옵션을 사용하여 검색을 실행합니다.\n",
    "- 검색 시 float 벡터가 자동으로 바이트 벡터로 양자화됩니다.\n",
    "\n",
    "\n",
    "```json\n",
    "POST quantized-image-index/_search\n",
    "{\n",
    "  \"knn\": {\n",
    "    \"field\": \"image-vector\",\n",
    "    \"query_vector\": [0.1, -2],\n",
    "    \"k\": 10,\n",
    "    \"num_candidates\": 100\n",
    "  },\n",
    "  \"fields\": [ \"title\" ]\n",
    "}\n",
    "```\n",
    "\n",
    "재점수화(Rescoring):\n",
    "- 원본 float 벡터를 사용하여 상위 결과의 점수를 재계산할 수 있습니다.\n",
    "- 상위 k개 결과에 대해서만 원본 float 벡터를 사용하여 재점수화합니다.\n",
    "- 이를 통해 빠른 검색과 정확한 점수 계산의 장점을 모두 얻을 수 있습니다.\n",
    "\n",
    "\n",
    "```json\n",
    "POST quantized-image-index/_search\n",
    "{\n",
    "  \"knn\": {\n",
    "    \"field\": \"image-vector\",\n",
    "    \"query_vector\": [0.1, -2],\n",
    "    \"k\": 15,\n",
    "    \"num_candidates\": 100\n",
    "  },\n",
    "  \"fields\": [ \"title\" ],\n",
    "  \"rescore\": {\n",
    "    \"window_size\": 10,\n",
    "    \"query\": {\n",
    "      \"rescore_query\": {\n",
    "        \"script_score\": {\n",
    "          \"query\": {\n",
    "            \"match_all\": {}\n",
    "          },\n",
    "          \"script\": {\n",
    "            \"source\": \"cosineSimilarity(params.query_vector, 'image-vector') + 1.0\",\n",
    "            \"params\": {\n",
    "              \"query_vector\": [0.1, -2]\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered kNN search\n",
    "\n",
    "필터링된 kNN 검색:\n",
    "- kNN 검색 API는 필터를 사용하여 검색 범위를 제한할 수 있습니다.\n",
    "- 검색은 필터 쿼리와 일치하는 문서 중에서 상위 k개의 문서를 반환합니다.\n",
    "\n",
    "검색 요청 예시:\n",
    "- 'image-vector' 필드에 대해 근사 kNN 검색을 수행합니다.\n",
    "- 'file-type' 필드를 기준으로 필터링합니다 (이 예에서는 'png' 파일만 검색).\n",
    "\n",
    "```json\n",
    "POST image-index/_search\n",
    "{\n",
    "  \"knn\": {\n",
    "    \"field\": \"image-vector\",\n",
    "    \"query_vector\": [54, 10, -2],\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 50,\n",
    "    \"filter\": {\n",
    "      \"term\": {\n",
    "        \"file-type\": \"png\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"fields\": [\"title\"],\n",
    "  \"_source\": false\n",
    "}\n",
    "```\n",
    "\n",
    "필터 적용 방식:\n",
    "- 필터는 근사 kNN 검색 중에 적용됩니다.\n",
    "- 이는 k개의 일치하는 문서를 확실히 반환하기 위함입니다.\n",
    "\n",
    "후처리 필터링과의 차이:\n",
    "- 후처리 필터링은 kNN 검색 완료 후 필터를 적용합니다.\n",
    "- 후처리 필터링의 단점: 충분한 일치 문서가 있어도 k개 미만의 결과를 반환할 수 있습니다.\n",
    "\n",
    "주의사항:\n",
    "- 필터가 너무 제한적이면 k개의 결과를 찾지 못할 수 있습니다.\n",
    "- num_candidates 값을 적절히 설정하여 검색의 정확성과 속도를 조절해야 합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate kNN search and filtering\n",
    "\n",
    "일반적인 쿼리와 필터링 쿼리와의 차이:\n",
    "- ES 에서는 일반적인 쿼리에서는 더 제한적인 필터가 보통 더 빠른 쿼리 실행을 의미합니다.\n",
    "- 그러나 HNSW 인덱스를 사용한 근사 kNN 검색에서는 필터 적용이 오히려 성능을 저하시킬 수 있습니다.\n",
    "\n",
    "\n",
    "성능 저하의 이유:\n",
    "- HNSW 그래프 검색 시 필터 기준을 만족하는 num_candidates를 얻기 위해 추가적인 탐색이 필요합니다.\n",
    "- 추가적인 오버헤드가 있는거지. 일반적인 필터는 탐색 범위를 줄이고 가는 반면에 여기서는 HNSW 탐색 하면서 필터를 적용하다보니까. \n",
    "\n",
    "\n",
    "\n",
    "Lucene의 성능 최적화 전략:\n",
    "- Lucene은 세그먼트 별로 다음 전략을 구현하여 성능 저하를 방지합니다:\n",
    "  - 필터링된 문서 수가 num_candidates 이하인 경우:\n",
    "    - HNSW 그래프 검색을 우회해서, 필터링된 문서에 대해 브루트 포스 검색을 수행합니다.\n",
    "  - HNSW 그래프 탐색 중 특정 조건 만족 시: \n",
    "    - 탐색된 노드 수가 필터를 만족하는 문서 수를 초과하면, 그래프 탐색을 중단하고 필터링된 문서에 대해 브루트 포스 검색으로 전환합니다.\n",
    "\n",
    "필터링 + HNSW 그래프 검색 매커니즘: \n",
    "- 필터링으로 통과한 문서의 집합을 구함. \n",
    "- num_candidated 수만큼을 구하기 위해서 HNSW 인덱스를 타서 검색을 함. 여기서 마지막 노드가 필터링을 통과하지 못한다면 추가 탐색이 계속적으로 발생할 수 있음. 그래서 필터링이 성능 저하를 일으킴. \n",
    "\n",
    "\n",
    "Lucene 성능 최적화 해석: \n",
    "- num_candidated 만큼 최소 HNSW 탐색이 이뤄지기 때문에, 필터링 된 문서가 num_candidated 보다 이하라면 브루트 포스 접근을 하는것. \n",
    "- 필터링된 문서의 수만큼 브루트 포스를 하게 되면 최대 시간 복잡도는 계산할 수 있음. 근데 num_candidate 가 필터링 문서보다 작다면 HNSW 탐색으로 더 빠르게 찾아낼 수도 있는 가능성이 있음. 근데 이게 필터링된 문서 수보다 많이 탐색을 해야한다면 그냥 브루트 포스 접근이 나은 것이었으니 이 방식으로 하는 것. \n",
    "\n",
    "사용 시 고려사항:\n",
    "- num_candidates 값을 적절히 설정하는 것이 중요합니다.\n",
    "- 필터의 선택성을 고려하여 쿼리를 설계해야 합니다.\n",
    "- 대규모 데이터셋에서는 필터링과 kNN 검색의 균형을 잘 맞추어야 합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine approximate kNN with other features\n",
    "\n",
    "하이브리드 검색:\n",
    "- kNN 옵션과 일반 쿼리를 함께 사용하여 하이브리드 검색을 수행할 수 있습니다.\n",
    "\n",
    "예시 쿼리 \n",
    "- 텍스트 매치 쿼리(\"mountain lake\")와 벡터 kNN 검색을 결합합니다.\n",
    "- 각 부분에 boost 값을 적용하여 가중치를 조절합니다.\n",
    "\n",
    "```json \n",
    "POST image-index/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"title\": {\n",
    "        \"query\": \"mountain lake\",\n",
    "        \"boost\": 0.9\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"knn\": {\n",
    "    \"field\": \"image-vector\",\n",
    "    \"query_vector\": [54, 10, -2],\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 50,\n",
    "    \"boost\": 0.1\n",
    "  },\n",
    "  \"size\": 10\n",
    "}\n",
    "```\n",
    "\n",
    "결과 결합 방식:\n",
    "- kNN 결과와 쿼리 결과는 disjunction(OR) 방식으로 결합됩니다.\n",
    "\n",
    "점수 계산:\n",
    "- 각 히트의 점수는 kNN 점수와 쿼리 점수의 가중 합으로 계산됩니다.\n",
    "- 예: score = 0.9 * match_score + 0.1 * knn_score\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform semantic search\n",
    "\n",
    "의미론적 검색의 개념:\n",
    "- 검색어의 문자 그대로의 일치가 아닌, 검색 쿼리의 의도와 문맥적 의미에 기반하여 결과를 검색합니다.\n",
    "\n",
    "\n",
    "작동 원리:\n",
    "- 사전에 배포된 텍스트 임베딩 모델을 사용합니다.\n",
    "- 입력 쿼리 문자열을 밀집 벡터(dense vector)로 변환합니다.\n",
    "- 이 벡터를 동일한 모델로 생성된 밀집 벡터가 저장된 인덱스에 대해 검색합니다.\n",
    "\n",
    "\n",
    "의미론적 검색 수행을 위한 요구사항:\n",
    "- 검색 대상 데이터의 밀집 벡터 표현이 포함된 인덱스가 필요합니다.\n",
    "- 검색에 사용하는 텍스트 임베딩 모델은 입력 데이터의 벡터 생성에 사용한 모델과 동일해야 합니다.\n",
    "- 텍스트 임베딩 NLP 모델 배포가 시작되어 있어야 합니다.\n",
    "\n",
    "쿼리 구조:\n",
    "- query_vector_builder 객체를 사용하여 배포된 텍스트 임베딩 모델을 참조합니다.\n",
    "- model_text 파라미터에 검색 쿼리를 제공합니다.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"knn\": {\n",
    "    \"field\": \"dense-vector-field\",\n",
    "    \"k\": 10,\n",
    "    \"num_candidates\": 100,\n",
    "    \"query_vector_builder\": {\n",
    "      \"text_embedding\": { \n",
    "        \"model_id\": \"my-text-embedding-model\", \n",
    "        \"model_text\": \"The opposite of blue\" \n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "추가 정보:\n",
    "- 훈련된 모델을 배포하고 텍스트 임베딩을 생성하는 방법에 대한 자세한 정보는 별도의 예제를 참조\n",
    "- https://www.elastic.co/guide/en/machine-learning/8.14/ml-nlp-text-emb-vector-search-example.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search multiple kNN fields\n",
    "\n",
    "여러 kNN(k-Nearest Neighbors) 벡터 필드를 동시에 검색하는 방법 \n",
    "\n",
    "다중 kNN 필드 검색:\n",
    "- 하나 이상의 kNN 벡터 필드를 동시에 검색할 수 있습니다.\n",
    "- 이는 하이브리드 검색의 확장된 형태입니다.\n",
    "\n",
    "예시 쿼리 구조:\n",
    "- 텍스트 매치 쿼리(\"mountain lake\")\n",
    "- 두 개의 kNN 검색 (image-vector와 title-vector)\n",
    "- 각 부분에 대한 boost 값 지정\n",
    "\n",
    "\n",
    "결과 결합 방식:\n",
    "- 여러 kNN 엔트리와 쿼리 매치는 disjunction(OR) 방식으로 결합됩니다.\n",
    "- 각 벡터 필드의 상위 k 결과는 모든 인덱스 샤드에 걸친 전역 최근접 이웃을 나타냅니다.\n",
    "\n",
    "\n",
    "점수 계산:\n",
    "- 각 문서의 점수는 텍스트 매치 점수와 각 kNN 검색의 점수를 가중 합산하여 계산됩니다.\n",
    "- 예시: score = 0.9 * match_score + 0.1 * knn_score_image-vector + 0.5 * knn_score_title-vector\n",
    "\n",
    "\n",
    "```json \n",
    "POST image-index/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"title\": {\n",
    "        \"query\": \"mountain lake\",\n",
    "        \"boost\": 0.9\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"knn\": [ {\n",
    "    \"field\": \"image-vector\",\n",
    "    \"query_vector\": [54, 10, -2],\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 50,\n",
    "    \"boost\": 0.1\n",
    "  },\n",
    "  {\n",
    "    \"field\": \"title-vector\",\n",
    "    \"query_vector\": [1, 20, -52, 23, 10],\n",
    "    \"k\": 10,\n",
    "    \"num_candidates\": 10,\n",
    "    \"boost\": 0.5\n",
    "  }],\n",
    "  \"size\": 10\n",
    "}\n",
    "```\n",
    "\n",
    "주요 이점:\n",
    "- 다양한 유형의 벡터 데이터를 동시에 고려할 수 있습니다.\n",
    "- 텍스트 기반 검색과 여러 벡터 기반 검색을 결합하여 더 풍부한 검색 결과를 제공합니다.\n",
    "- 각 검색 구성요소에 대한 가중치(boost)를 조정하여 검색 결과의 우선순위를 세밀하게 제어할 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search kNN with expected similarity\n",
    "\n",
    "Elasticsearch의 kNN(k-Nearest Neighbors) 검색에서 기대 유사도(expected similarity)를 사용하는 방법에 대해 설명\n",
    "\n",
    "kNN의 한계점:\n",
    "- kNN은 항상 k개의 최근접 이웃을 반환하려고 합니다.\n",
    "- 필터와 함께 사용할 때, 관련 없는 문서만 남을 수 있습니다.\n",
    "\n",
    "\n",
    "similarity 파라미터:\n",
    "- kNN 절에서 사용 가능한 새로운 파라미터입니다.\n",
    "- 벡터가 매치로 간주되기 위한 최소 유사도를 지정합니다.\n",
    "\n",
    "similarity를 사용한 kNN 검색 흐름:\n",
    "- 사용자가 제공한 필터 쿼리 적용\n",
    "- 벡터 공간에서 k개의 벡터 탐색\n",
    "- 구성된 similarity보다 멀리 있는 벡터는 반환하지 않음\n",
    "\n",
    "similarity와 _score의 관계:\n",
    "- similarity는 _score로 변환되기 전의 실제 유사도입니다.\n",
    "- 각 유사도 메트릭에 대한 _score 변환 공식이 제공됩니다.\n",
    "  - l2_norm: sqrt((1 / _score) - 1)\n",
    "  - cosine: (2 * _score) - 1\n",
    "  - dot_product: (2 * _score) - 1\n",
    "  - max_inner_product:\n",
    "    - _score < 1: 1 - (1 / _score)\n",
    "    - _score >= 1: _score - 1\n",
    "\n",
    "```json\n",
    "\n",
    "POST image-index/_search\n",
    "{\n",
    "  \"knn\": {\n",
    "    \"field\": \"image-vector\",\n",
    "    \"query_vector\": [1, 5, -20],\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 50,\n",
    "    \"similarity\": 36,\n",
    "    \"filter\": {\n",
    "      \"term\": {\n",
    "        \"file-type\": \"png\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"fields\": [\"title\"],\n",
    "  \"_source\": false\n",
    "}\n",
    "```\n",
    "\n",
    "주요 이점:\n",
    "- 관련성 없는 결과를 효과적으로 필터링할 수 있습니다.\n",
    "- 유사도에 기반한 더 정확한 검색 결과를 얻을 수 있습니다.\n",
    "\n",
    "\n",
    "사용 시 고려사항:\n",
    "- similarity 값을 적절히 설정하는 것이 중요합니다. (유사도 메트릭도 추가로 고려해야함.)\n",
    "- 데이터의 특성과 벡터 공간의 분포를 이해해야 합니다.\n",
    "- 필터와 similarity를 함께 사용할 때 검색 결과가 없을 수 있음을 인지해야 합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
